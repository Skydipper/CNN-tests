{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning pipeline mockup\n",
    "\n",
    "[Graphical representation of the pipeline](https://www.draw.io/#G1U6XDddvcjas2vglyKeVz0ouFOElzOMCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup software libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.202'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and initialize the Earth Engine library.\n",
    "import ee\n",
    "ee.Initialize()\n",
    "ee.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.3\n"
     ]
    }
   ],
   "source": [
    "# Folium setup.\n",
    "import folium\n",
    "print(folium.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.7\n"
     ]
    }
   ],
   "source": [
    "# Skydipper library.\n",
    "import Skydipper\n",
    "print(Skydipper.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "from pprint import pprint\n",
    "import env\n",
    "import time\n",
    "import sqlalchemy\n",
    "from sqlalchemy import Column, Integer, BigInteger, Float, Text, String, Boolean, DateTime\n",
    "from sqlalchemy.dialects.postgresql import JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Database\n",
    "\n",
    "We will create a Database to save all the attributes that we will generate all through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_table(table_path, columns, dtypes):\n",
    "    if not os.path.exists(table_path):\n",
    "        dictionary = dict(zip(columns, dtypes))\n",
    "        dtypes = np.dtype([(k, v) for k, v in dictionary.items()]) \n",
    "    \n",
    "        data = np.empty(0, dtype=dtypes)\n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "        df.to_csv(table_path)\n",
    "    else:\n",
    "        df = pd.read_csv(table_path, index_col=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Database'):\n",
    "    os.makedirs('Database')\n",
    "    \n",
    "datasets = create_db_table('Database/dataset.csv', \n",
    "                          columns = ['slug', 'name', 'bands', 'rgb_bands', 'provider'], \n",
    "                          dtypes = [str, str, list, list, str]\n",
    "                         )\n",
    "\n",
    "images = create_db_table('Database/image.csv', \n",
    "                          columns = ['dataset_id', 'bands_selections', 'scale', 'init_date',\n",
    "                                     'end_date', 'composite_method', 'bands_min_max'], \n",
    "                          dtypes = [int, list, float, str, str, str, str]\n",
    "                         )\n",
    "\n",
    "models = create_db_table('Database/model.csv', \n",
    "                          columns = ['model_name', 'model_type', 'model_description', 'output_image_id'], \n",
    "                          dtypes = [str, str, str, int]\n",
    "                        )\n",
    "                         \n",
    "versions = create_db_table('Database/model_versions.csv', \n",
    "                           columns = ['model_id', 'model_architecture', 'input_image_id', 'output_image_id', 'geostore_id', 'sample_size', \n",
    "                                      'training_params', 'version', 'data_status', 'training_status', 'eeified', 'deployed'], \n",
    "                           dtypes = [int, str, int, int, str, int, str, int, str, str, bool, bool]   \n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Pandas to a Database with SQLAlchemy ([tutorial](https://hackersandslackers.com/connecting-pandas-to-a-sql-database-with-sqlalchemy/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an engine\n",
    "\n",
    "An `engine` is an object used to connect to databases using the information in our URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('postgresql://postgres:postgres@0.0.0.0:5432/geomodels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create SQL tables from a DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_db(df, table_name):\n",
    "    if table_name == \"dataset\":\n",
    "        df.to_sql(\"dataset\",\n",
    "                       engine,\n",
    "                       if_exists='replace',\n",
    "                       schema='public',\n",
    "                       index=True,\n",
    "                       index_label='id',\n",
    "                       chunksize=500,\n",
    "                       dtype={\"slug\": Text,\n",
    "                              \"name\": Text,\n",
    "                              \"bands\": Text,\n",
    "                              \"bands\": Text,\n",
    "                              \"provider\": Text})\n",
    "    if table_name == \"image\":\n",
    "        df.to_sql(\"image\",\n",
    "                       engine,\n",
    "                       if_exists='replace',\n",
    "                       schema='public',\n",
    "                       index=True,\n",
    "                       index_label='id',\n",
    "                       chunksize=500,\n",
    "                       dtype={\"dataset_id \": Integer,\n",
    "                              \"bands_selections\": Text,\n",
    "                              \"scale\": Float,\n",
    "                              \"init_date\": Text,\n",
    "                              \"end_date\": Text,\n",
    "                              \"bands_min_max\": JSON})\n",
    "\n",
    "    if table_name == \"model\":\n",
    "        df.to_sql(\"model\",\n",
    "                       engine,\n",
    "                       if_exists='replace',\n",
    "                       schema='public',\n",
    "                       index=True,\n",
    "                       index_label='id',\n",
    "                       chunksize=500,\n",
    "                       dtype={\"model_name\": Text,\n",
    "                              \"model_type\": Text,\n",
    "                              \"model_description\": Text,\n",
    "                              \"output_image_id\": Integer})\n",
    "\n",
    "    if table_name == \"model_versions\":\n",
    "        df.to_sql(\"model_versions\",\n",
    "                       engine,\n",
    "                       if_exists='replace',\n",
    "                       schema='public',\n",
    "                       index=True,\n",
    "                       index_label='id',\n",
    "                       chunksize=500,\n",
    "                       dtype={\"model_id\": Integer,\n",
    "                              \"model_architecture\": Text,\n",
    "                              \"input_image_id\": Integer,\n",
    "                              \"output_image_id\": Integer,\n",
    "                              \"geostore_id\": Text,\n",
    "                              \"sample_size\": BigInteger,\n",
    "                              \"training_params\": JSON,\n",
    "                              \"version\": BigInteger,\n",
    "                              \"data_status\": Text,\n",
    "                              \"training_status\": Text,\n",
    "                              \"eeified\": Boolean,\n",
    "                              \"deployed\": Boolean})                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not engine.dialect.has_table(engine, \"dataset\"):\n",
    "    datasets = pd.read_csv('Database/dataset.csv', index_col=0)\n",
    "if not engine.dialect.has_table(engine, \"image\"):\n",
    "    images = pd.read_csv('Database/image.csv', index_col=0)\n",
    "if not engine.dialect.has_table(engine, \"model\"):\n",
    "    models = pd.read_csv('Database/model.csv', index_col=0)\n",
    "if not engine.dialect.has_table(engine, \"model_versions\"):\n",
    "    versions = pd.read_csv('Database/model_versions.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save SQL tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not engine.dialect.has_table(engine, \"dataset\"):\n",
    "    df_to_db(datasets, \"dataset\")\n",
    "if not engine.dialect.has_table(engine, \"image\"):\n",
    "    df_to_db(images, \"image\")\n",
    "if not engine.dialect.has_table(engine, \"model\"):\n",
    "    df_to_db(models, \"model\")\n",
    "if not engine.dialect.has_table(engine, \"model_versions\"):\n",
    "    df_to_db(versions, \"model_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read DataFrames from query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_sql(\"SELECT * FROM dataset\", con=engine).drop(columns='id')\n",
    "images = pd.read_sql(\"SELECT * FROM image\", con=engine).drop(columns='id')\n",
    "models = pd.read_sql(\"SELECT * FROM model\", con=engine).drop(columns='id')\n",
    "versions = pd.read_sql(\"SELECT * FROM model_versions\", con=engine).drop(columns='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.to_csv('Database/dataset.csv',sep=';', quotechar='\\'',index=True, index_label='id')\n",
    "images.to_csv('Database/image.csv',sep=';', quotechar='\\'',index=True, index_label='id')\n",
    "models.to_csv('Database/model.csv',sep=';', quotechar='\\'',index=True, index_label='id')\n",
    "versions.to_csv('Database/model_versions.csv',sep=';', quotechar='\\'',index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>name</th>\n",
       "      <th>bands</th>\n",
       "      <th>rgb_bands</th>\n",
       "      <th>provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentinel-2-Top-of-Atmosphere-Reflectance</td>\n",
       "      <td>Sentinel 2 Top-of-Atmosphere Reflectance</td>\n",
       "      <td>['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8...</td>\n",
       "      <td>['B4', 'B3', 'B2']</td>\n",
       "      <td>gee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Landsat-7-Surface-Reflectance</td>\n",
       "      <td>Landsat 7 Surface Reflectance</td>\n",
       "      <td>['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'nd...</td>\n",
       "      <td>['B3', 'B2', 'B1']</td>\n",
       "      <td>gee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Landsat-8-Surface-Reflectance</td>\n",
       "      <td>Landsat 8 Surface Reflectance</td>\n",
       "      <td>['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B1...</td>\n",
       "      <td>['B4', 'B3', 'B2']</td>\n",
       "      <td>gee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USDA-NASS-Cropland-Data-Layers</td>\n",
       "      <td>USDA NASS Cropland Data Layers</td>\n",
       "      <td>['landcover', 'cropland', 'land', 'water', 'ur...</td>\n",
       "      <td>['landcover']</td>\n",
       "      <td>gee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USGS-National-Land-Cover-Database</td>\n",
       "      <td>USGS National Land Cover Database</td>\n",
       "      <td>['impervious']</td>\n",
       "      <td>['impervious']</td>\n",
       "      <td>gee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       slug  \\\n",
       "0  Sentinel-2-Top-of-Atmosphere-Reflectance   \n",
       "1             Landsat-7-Surface-Reflectance   \n",
       "2             Landsat-8-Surface-Reflectance   \n",
       "3            USDA-NASS-Cropland-Data-Layers   \n",
       "4         USGS-National-Land-Cover-Database   \n",
       "\n",
       "                                       name  \\\n",
       "0  Sentinel 2 Top-of-Atmosphere Reflectance   \n",
       "1             Landsat 7 Surface Reflectance   \n",
       "2             Landsat 8 Surface Reflectance   \n",
       "3            USDA NASS Cropland Data Layers   \n",
       "4         USGS National Land Cover Database   \n",
       "\n",
       "                                               bands           rgb_bands  \\\n",
       "0  ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8...  ['B4', 'B3', 'B2']   \n",
       "1  ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'nd...  ['B3', 'B2', 'B1']   \n",
       "2  ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B1...  ['B4', 'B3', 'B2']   \n",
       "3  ['landcover', 'cropland', 'land', 'water', 'ur...       ['landcover']   \n",
       "4                                     ['impervious']      ['impervious']   \n",
       "\n",
       "  provider  \n",
       "0      gee  \n",
       "1      gee  \n",
       "2      gee  \n",
       "3      gee  \n",
       "4      gee  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>bands_selections</th>\n",
       "      <th>scale</th>\n",
       "      <th>init_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>bands_min_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'nd...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>{\"B10_max\": 2995.0, \"B10_min\": 2509.0, \"B11_ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>['cropland', 'land', 'water', 'urban']</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>['impervious']</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_id                                   bands_selections  scale  \\\n",
       "0           2  ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'nd...   30.0   \n",
       "1           3             ['cropland', 'land', 'water', 'urban']   30.0   \n",
       "2           4                                     ['impervious']   30.0   \n",
       "\n",
       "    init_date    end_date                                      bands_min_max  \n",
       "0  2016-01-01  2016-12-31  {\"B10_max\": 2995.0, \"B10_min\": 2509.0, \"B11_ma...  \n",
       "1  2016-01-01  2016-12-31                                                 {}  \n",
       "2  2016-01-01  2016-12-31                                                 {}  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>model_description</th>\n",
       "      <th>output_image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>segmentation_0_1</td>\n",
       "      <td>segmentation</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regression_2</td>\n",
       "      <td>regression</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model_name    model_type model_description  output_image_id\n",
       "0  segmentation_0_1  segmentation              None                1\n",
       "1      regression_2    regression              None                2"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>model_architecture</th>\n",
       "      <th>input_image_id</th>\n",
       "      <th>output_image_id</th>\n",
       "      <th>geostore_id</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>training_params</th>\n",
       "      <th>version</th>\n",
       "      <th>data_status</th>\n",
       "      <th>training_status</th>\n",
       "      <th>eeified</th>\n",
       "      <th>deployed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>segnet</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>f75559fb87f5c22deb56eb2a73aa4e12</td>\n",
       "      <td>1000</td>\n",
       "      <td>{\"bucket\": \"geo-ai\", \"job_dir\": \"gs://geo-ai/M...</td>\n",
       "      <td>1579684545</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>deepvel</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27d4ea6fd635f3a2dc20789e8058f34d</td>\n",
       "      <td>1000</td>\n",
       "      <td>{\"bucket\": \"geo-ai\", \"job_dir\": \"gs://geo-ai/M...</td>\n",
       "      <td>1579780131</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>unet</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27d4ea6fd635f3a2dc20789e8058f34d</td>\n",
       "      <td>1000</td>\n",
       "      <td>{\"bucket\": \"geo-ai\", \"job_dir\": \"gs://geo-ai/M...</td>\n",
       "      <td>1579795237</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>deepvel</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>f75559fb87f5c22deb56eb2a73aa4e12</td>\n",
       "      <td>1000</td>\n",
       "      <td>{\"bucket\": \"geo-ai\", \"base_names\": [\"training_...</td>\n",
       "      <td>1579867953</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>SUCCEEDED</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id model_architecture  input_image_id  output_image_id  \\\n",
       "0         0             segnet               0                1   \n",
       "1         1            deepvel               0                2   \n",
       "2         1               unet               0                2   \n",
       "3         0            deepvel               0                1   \n",
       "\n",
       "                        geostore_id  sample_size  \\\n",
       "0  f75559fb87f5c22deb56eb2a73aa4e12         1000   \n",
       "1  27d4ea6fd635f3a2dc20789e8058f34d         1000   \n",
       "2  27d4ea6fd635f3a2dc20789e8058f34d         1000   \n",
       "3  f75559fb87f5c22deb56eb2a73aa4e12         1000   \n",
       "\n",
       "                                     training_params     version data_status  \\\n",
       "0  {\"bucket\": \"geo-ai\", \"job_dir\": \"gs://geo-ai/M...  1579684545   COMPLETED   \n",
       "1  {\"bucket\": \"geo-ai\", \"job_dir\": \"gs://geo-ai/M...  1579780131   COMPLETED   \n",
       "2  {\"bucket\": \"geo-ai\", \"job_dir\": \"gs://geo-ai/M...  1579795237   COMPLETED   \n",
       "3  {\"bucket\": \"geo-ai\", \"base_names\": [\"training_...  1579867953   COMPLETED   \n",
       "\n",
       "  training_status  eeified  deployed  \n",
       "0       SUCCEEDED     True      True  \n",
       "1       SUCCEEDED     True      True  \n",
       "2       SUCCEEDED     True      True  \n",
       "3       SUCCEEDED     True      True  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Skydipper datasets for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slugs_list = [\"Sentinel-2-Top-of-Atmosphere-Reflectance\",\n",
    "              \"Landsat-7-Surface-Reflectance\",\n",
    "              \"Landsat-8-Surface-Reflectance\",\n",
    "              \"USDA-NASS-Cropland-Data-Layers\",\n",
    "              \"USGS-National-Land-Cover-Database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Skydipper.Collection(search=' '.join(slugs_list), object_type=['dataset'], app=['skydipper'], limit=10)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earth Engine ImageCollection attributes\n",
    "\n",
    "We define the different attributes that we will need for each Earth Engine ImageCollection all through the notebook. \n",
    "\n",
    "We include them in the `ee_collection_specifics.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ee_collection_specifics.py\n",
    "\n",
    "\"\"\"\n",
    "Information on Earth Engine collections stored here (e.g. bands, collection ids, etc.)\n",
    "\"\"\"\n",
    "\n",
    "import ee\n",
    "\n",
    "def ee_collections(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine image collection names\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': 'COPERNICUS/S2',\n",
    "        'Landsat-7-Surface-Reflectance': 'LANDSAT/LE07/C01/T1_SR',\n",
    "        'Landsat-8-Surface-Reflectance': 'LANDSAT/LC08/C01/T1_SR',\n",
    "        'USDA-NASS-Cropland-Data-Layers': 'USDA/NASS/CDL',\n",
    "        'USGS-National-Land-Cover-Database': 'USGS/NLCD',\n",
    "        'Skydipper-Water-Quality': 'projects/vizzuality/skydipper-water-quality/LWQ-100m'\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine band names\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': ['B1','B2','B3','B4','B5','B6','B7','B8A','B8','B11','B12','ndvi','ndwi'],\n",
    "        'Landsat-7-Surface-Reflectance': ['B1','B2','B3','B4','B5','B6','B7','ndvi','ndwi'],\n",
    "        'Landsat-8-Surface-Reflectance': ['B1','B2','B3','B4','B5','B6','B7','B10','B11','ndvi','ndwi'],\n",
    "        'USDA-NASS-Cropland-Data-Layers': ['landcover', 'cropland', 'land', 'water', 'urban'],\n",
    "        'USGS-National-Land-Cover-Database': ['impervious'],\n",
    "        'Skydipper-Water-Quality': ['turbidity_blended_mean']\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands_rgb(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine rgb band names\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': ['B4','B3','B2'],\n",
    "        'Landsat-7-Surface-Reflectance': ['B3','B2','B1'],\n",
    "        'Landsat-8-Surface-Reflectance': ['B4', 'B3', 'B2'],\n",
    "        'USDA-NASS-Cropland-Data-Layers': ['landcover'],\n",
    "        'USGS-National-Land-Cover-Database': ['impervious'],\n",
    "        'Skydipper-Water-Quality': ['turbidity_blended_mean']\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands_normThreshold(collection):\n",
    "    \"\"\"\n",
    "    Normalization threshold percentage\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': {'B1': 75,'B2': 75,'B3': 75,'B4': 75,'B5': 80,'B6': 80,'B7': 80,'B8A': 80,'B8': 80,'B11': 100,'B12': 100},\n",
    "        'Landsat-7-Surface-Reflectance': {'B1': 95,'B2': 95,'B3': 95,'B4': 100,'B5': 100,'B6': 100,'B7': 100},\n",
    "        'Landsat-8-Surface-Reflectance': {'B1': 90,'B2': 95,'B3': 95,'B4': 95,'B5': 100,'B6': 100,'B7': 100,'B10': 100,'B11': 100},\n",
    "        'USDA-NASS-Cropland-Data-Layers': {'landcover': 100, 'cropland': 100, 'land': 100, 'water': 100, 'urban': 100},\n",
    "        'USGS-National-Land-Cover-Database': {'impervious': 100},\n",
    "        'Skydipper-Water-Quality': {'turbidity_blended_mean': 100}\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def normalize(collection):\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': True,\n",
    "        'Landsat-7-Surface-Reflectance': True,\n",
    "        'Landsat-8-Surface-Reflectance': True,\n",
    "        'USDA-NASS-Cropland-Data-Layers': False,\n",
    "        'USGS-National-Land-Cover-Database': False,\n",
    "        'Skydipper-Water-Quality': False\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def vizz_params_rgb(collection):\n",
    "    \"\"\"\n",
    "    Visualization parameters\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': {'min':0,'max':3000, 'bands':['B4','B3','B2']},\n",
    "        'Landsat-7-Surface-Reflectance': {'min':0,'max':3000, 'gamma':1.4, 'bands':['B3','B2','B1']},\n",
    "        'Landsat-8-Surface-Reflectance': {'min':0,'max':3000, 'gamma':1.4, 'bands':['B4','B3','B2']},\n",
    "        'USDA-NASS-Cropland-Data-Layers': {'min':0,'max':3, 'bands':['landcover']},\n",
    "        'USGS-National-Land-Cover-Database': {'min': 0, 'max': 1, 'bands':['impervious']},\n",
    "        'Skydipper-Water-Quality': {'min': 0, 'max': 1, 'bands':['turbidity_blended_mean']}\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def vizz_params(collection):\n",
    "    \"\"\"\n",
    "    Visualization parameters\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': [{'min':0,'max':1, 'bands':['B4','B3','B2']}, \n",
    "                      {'min':0,'max':1, 'bands':['B1']},\n",
    "                      {'min':0,'max':1, 'bands':['B5']},\n",
    "                      {'min':0,'max':1, 'bands':['B6']},\n",
    "                      {'min':0,'max':1, 'bands':['B7']},\n",
    "                      {'min':0,'max':1, 'bands':['B8A']},\n",
    "                      {'min':0,'max':1, 'bands':['B8']},\n",
    "                      {'min':0,'max':1, 'bands':['B11']},\n",
    "                      {'min':0,'max':1, 'bands':['B12']},\n",
    "                      {'min':0,'max':1, 'gamma':1.4, 'bands':['ndvi']},\n",
    "                      {'min':0,'max':1, 'gamma':1.4, 'bands':['ndwi']}],\n",
    "        'Landsat-7-Surface-Reflectance': [{'min':0,'max':1, 'gamma':1.4, 'bands':['B3','B2','B1']}, \n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B4']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B5']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B7']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B6']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['ndvi']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['ndwi']}],\n",
    "        'Landsat-8-Surface-Reflectance': [{'min':0,'max':1, 'gamma':1.4, 'bands':['B4','B3','B2']}, \n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B1']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B5']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B6']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B7']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B10']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B11']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['ndvi']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['ndwi']}],\n",
    "        'USDA-NASS-Cropland-Data-Layers': [{'min':0,'max':3, 'bands':['landcover']},\n",
    "                               {'min':0,'max':1, 'bands':['cropland']},\n",
    "                               {'min':0,'max':1, 'bands':['land']},\n",
    "                               {'min':0,'max':1, 'bands':['water']},\n",
    "                               {'min':0,'max':1, 'bands':['urban']}],\n",
    "        'USGS-National-Land-Cover-Database': [{'min': 0, 'max': 1, 'bands':['impervious']}],\n",
    "        'Skydipper-Water-Quality': [{'min': 0, 'max': 1, 'bands':['turbidity_blended_mean']}],\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "## ------------------------- Filter datasets ------------------------- ##\n",
    "## Lansat 7 Cloud Free Composite\n",
    "def CloudMaskL7sr(image):\n",
    "    qa = image.select('pixel_qa')\n",
    "    #If the cloud bit (5) is set and the cloud confidence (7) is high\n",
    "    #or the cloud shadow bit is set (3), then it's a bad pixel.\n",
    "    cloud = qa.bitwiseAnd(1 << 5).And(qa.bitwiseAnd(1 << 7)).Or(qa.bitwiseAnd(1 << 3))\n",
    "    #Remove edge pixels that don't occur in all bands\n",
    "    mask2 = image.mask().reduce(ee.Reducer.min())\n",
    "    return image.updateMask(cloud.Not()).updateMask(mask2)\n",
    "\n",
    "def CloudFreeCompositeL7(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate).map(CloudMaskL7sr)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    ## normDiff bands\n",
    "    normDiff_band_names = ['ndvi', 'ndwi']\n",
    "    for nB, normDiff_band in enumerate([['B4','B3'], ['B4','B2']]):\n",
    "        image_nd = composite.normalizedDifference(normDiff_band).rename(normDiff_band_names[nB])\n",
    "        composite = ee.Image.cat([composite, image_nd])\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Lansat 8 Cloud Free Composite\n",
    "def CloudMaskL8sr(image):\n",
    "    opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "    thermalBands = ['B10', 'B11']\n",
    "\n",
    "    cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
    "    cloudsBitMask = ee.Number(2).pow(5).int()\n",
    "    qa = image.select('pixel_qa')\n",
    "    mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
    "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
    "    mask2 = image.mask().reduce('min')\n",
    "    mask3 = image.select(opticalBands).gt(0).And(\n",
    "            image.select(opticalBands).lt(10000)).reduce('min')\n",
    "    mask = mask1.And(mask2).And(mask3)\n",
    "    \n",
    "    return image.updateMask(mask)\n",
    "\n",
    "def CloudFreeCompositeL8(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate).map(CloudMaskL8sr)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    ## normDiff bands\n",
    "    normDiff_band_names = ['ndvi', 'ndwi']\n",
    "    for nB, normDiff_band in enumerate([['B5','B4'], ['B5','B3']]):\n",
    "        image_nd = composite.normalizedDifference(normDiff_band).rename(normDiff_band_names[nB])\n",
    "        composite = ee.Image.cat([composite, image_nd])\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Sentinel 2 Cloud Free Composite\n",
    "def CloudMaskS2(image):\n",
    "    \"\"\"\n",
    "    European Space Agency (ESA) clouds from 'QA60', i.e. Quality Assessment band at 60m\n",
    "    parsed by Nick Clinton\n",
    "    \"\"\"\n",
    "    AerosolsBands = ['B1']\n",
    "    VIBands = ['B2', 'B3', 'B4']\n",
    "    RedBands = ['B5', 'B6', 'B7', 'B8A']\n",
    "    NIRBands = ['B8']\n",
    "    SWIRBands = ['B11', 'B12']\n",
    "\n",
    "    qa = image.select('QA60')\n",
    "\n",
    "    # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "    cloudBitMask = int(2**10)\n",
    "    cirrusBitMask = int(2**11)\n",
    "\n",
    "    # Both flags set to zero indicates clear conditions.\n",
    "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(\\\n",
    "            qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "\n",
    "    return image.updateMask(mask)\n",
    "\n",
    "def CloudFreeCompositeS2(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('COPERNICUS/S2')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\\\n",
    "            .map(CloudMaskS2)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    ## normDiff bands\n",
    "    normDiff_band_names = ['ndvi', 'ndwi']\n",
    "    for nB, normDiff_band in enumerate([['B8','B4'], ['B8','B3']]):\n",
    "        image_nd = composite.normalizedDifference(normDiff_band).rename(normDiff_band_names[nB])\n",
    "        composite = ee.Image.cat([composite, image_nd])\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Cropland Data Layers\n",
    "def CroplandData(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('USDA/NASS/CDL')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Change classes\n",
    "    land = ['65', '131', '141', '142', '143', '152', '176', '87', '190', '195']\n",
    "    water = ['83', '92', '111']\n",
    "    urban = ['82', '121', '122', '123', '124']\n",
    "    \n",
    "    classes = []\n",
    "    for n, i in enumerate([land,water,urban]):\n",
    "        a = ''\n",
    "        for m, j in enumerate(i):\n",
    "            if m < len(i)-1:\n",
    "                a = a + 'crop == '+ j + ' || '\n",
    "            else: \n",
    "                a = a + 'crop == '+ j\n",
    "        classes.append('('+a+') * '+str(n+1))\n",
    "    classes = ' + '.join(classes)\n",
    "    \n",
    "    image = image.expression(classes, {'crop': image.select(['cropland'])})\n",
    "    \n",
    "    image =image.rename('landcover')\n",
    "    \n",
    "    # Split image into 1 band per class\n",
    "    names = ['cropland', 'land', 'water', 'urban']\n",
    "    mask = image\n",
    "    for i, name in enumerate(names):\n",
    "        image = ee.Image.cat([image, mask.eq(i).rename(name)])\n",
    "     \n",
    "    return image\n",
    "\n",
    "## National Land Cover Database\n",
    "def ImperviousData(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('USGS/NLCD')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Select impervious band\n",
    "    image = image.select('impervious')\n",
    "    \n",
    "    ## Normalize to 1\n",
    "    image = image.divide(100).float()\n",
    "    \n",
    "    return image\n",
    "\n",
    "def WaterQuality(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('projects/vizzuality/skydipper-water-quality/LWQ-100m')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Select impervious band\n",
    "    image = image.select('turbidity_blended_mean')\n",
    "    \n",
    "    return image\n",
    "\n",
    "## ------------------------------------------------------------------- ##\n",
    "\n",
    "def Composite(collection):\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': CloudFreeCompositeS2,\n",
    "        'Landsat-7-Surface-Reflectance': CloudFreeCompositeL7,\n",
    "        'Landsat-8-Surface-Reflectance': CloudFreeCompositeL8,\n",
    "        'USDA-NASS-Cropland-Data-Layers': CroplandData,\n",
    "        'USGS-National-Land-Cover-Database': ImperviousData,\n",
    "        'Skydipper-Water-Quality': WaterQuality\n",
    "    }\n",
    "    \n",
    "    return dic[collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee_collection_specifics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate `dataset` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for collection in slugs_list:\n",
    "\n",
    "    ds = Skydipper.Dataset(id_hash=collection)\n",
    "    name = ds.attributes.get('name')\n",
    "    provider = ds.attributes.get('provider')\n",
    "\n",
    "    bands = [ee_collection_specifics.ee_bands(collection)]\n",
    "    rgb_bands = [ee_collection_specifics.ee_bands_rgb(collection)]\n",
    "\n",
    "\n",
    "    dictionary = dict(zip(list(datasets.keys()), [collection, name, bands, rgb_bands, provider]))\n",
    "    \n",
    "    if (datasets['slug'] == collection).any():\n",
    "        datasets = datasets\n",
    "    else:\n",
    "        datasets = datasets.append(pd.DataFrame(dictionary), ignore_index = True)\n",
    "        datasets.to_csv('Database/dataset.csv')\n",
    "    \n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data pre-processing\n",
    "\n",
    "We normalize the composite images to have values from 0 to 1.\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = 'Landsat-8-Surface-Reflectance'\n",
    "output_dataset = 'USDA-NASS-Cropland-Data-Layers'\n",
    "init_date = '2016-01-01'\n",
    "end_date = '2016-12-31'\n",
    "scale = 30 #scale in meters\n",
    "collections = [input_dataset, output_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize images function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_values(image, collection, scale):\n",
    "    \n",
    "    normThreshold = ee_collection_specifics.ee_bands_normThreshold(collection)\n",
    "    \n",
    "    num = 2\n",
    "    lon = np.linspace(-180, 180, num)\n",
    "    lat = np.linspace(-90, 90, num)\n",
    "    \n",
    "    features = []\n",
    "    for i in range(len(lon)-1):\n",
    "        for j in range(len(lat)-1):\n",
    "            features.append(ee.Feature(ee.Geometry.Rectangle(lon[i], lat[j], lon[i+1], lat[j+1])))\n",
    "    \n",
    "    regReducer = {\n",
    "        'geometry': ee.FeatureCollection(features),\n",
    "        'reducer': ee.Reducer.minMax(),\n",
    "        'maxPixels': 1e10,\n",
    "        'bestEffort': True,\n",
    "        'scale':scale\n",
    "        \n",
    "    }\n",
    "    \n",
    "    values = image.reduceRegion(**regReducer).getInfo()\n",
    "    \n",
    "    # Avoid outliers by taking into account only the normThreshold% of the data points.\n",
    "    regReducer = {\n",
    "        'geometry': ee.FeatureCollection(features),\n",
    "        'reducer': ee.Reducer.histogram(),\n",
    "        'maxPixels': 1e10,\n",
    "        'bestEffort': True,\n",
    "        'scale':scale\n",
    "        \n",
    "    }\n",
    "    \n",
    "    hist = image.reduceRegion(**regReducer).getInfo()\n",
    "\n",
    "    for band in list(normThreshold.keys()):\n",
    "        if normThreshold[band] != 100:\n",
    "            count = np.array(hist.get(band).get('histogram'))\n",
    "            x = np.array(hist.get(band).get('bucketMeans'))\n",
    "        \n",
    "            cumulative_per = np.cumsum(count/count.sum()*100)\n",
    "        \n",
    "            values[band+'_max'] = x[np.where(cumulative_per < normThreshold[band])][-1]\n",
    "        \n",
    "    return values\n",
    "\n",
    "def normalize_ee_images(image, collection, values):\n",
    "    \n",
    "    Bands = ee_collection_specifics.ee_bands(collection)\n",
    "       \n",
    "    # Normalize [0, 1] ee images\n",
    "    for i, band in enumerate(Bands):\n",
    "        if i == 0:\n",
    "            image_new = image.select(band).clamp(values[band+'_min'], values[band+'_max'])\\\n",
    "                                .subtract(values[band+'_min'])\\\n",
    "                                .divide(values[band+'_max']-values[band+'_min'])\n",
    "        else:\n",
    "            image_new = image_new.addBands(image.select(band).clamp(values[band+'_min'], values[band+'_max'])\\\n",
    "                                    .subtract(values[band+'_min'])\\\n",
    "                                    .divide(values[band+'_max']-values[band+'_min']))\n",
    "            \n",
    "    return image_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate `image` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and image tables\n",
    "datasets = pd.read_csv('Database/dataset.csv', index_col=0)\n",
    "images = pd.read_csv('Database/image.csv', index_col=0)\n",
    "\n",
    "for collection in collections:\n",
    "    dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "\n",
    "    # Populate image table\n",
    "    if images[['dataset_id', 'scale', 'init_date', 'end_date']].isin([dataset_id, scale, init_date, end_date]).all(axis=1).any():\n",
    "        images = images\n",
    "    else:\n",
    "        # Create composite\n",
    "        image = ee_collection_specifics.Composite(collection)(init_date, end_date)\n",
    "    \n",
    "        bands = ee_collection_specifics.ee_bands(collection)\n",
    "        image = image.select(bands)\n",
    "        \n",
    "        if ee_collection_specifics.normalize(collection):\n",
    "            # Get min/man values for each band\n",
    "            values = min_max_values(image, collection, scale)\n",
    "        else:\n",
    "            values = {}\n",
    "    \n",
    "        # Append values to table\n",
    "        dictionary = dict(zip(list(images.keys()), [[dataset_id], [''], [scale], [init_date], [end_date], [''], [''], [json.dumps(values)]]))\n",
    "        images = images.append(pd.DataFrame(dictionary), ignore_index = True)\n",
    "        \n",
    "    # Save table\n",
    "    images.to_csv('Database/image.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "Map = folium.Map(location=[38.012, -121.2747], zoom_start=11)\n",
    "\n",
    "# Read dataset and image tables\n",
    "datasets = pd.read_csv('Database/dataset.csv', index_col=0)\n",
    "images = pd.read_csv('Database/image.csv', index_col=0)\n",
    "\n",
    "for collection in collections:\n",
    "\n",
    "    dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "    \n",
    "    df = images[(images['dataset_id'] == dataset_id) & \n",
    "                (images['scale'] == scale) & \n",
    "                (images['init_date'] == init_date) & \n",
    "                (images['end_date'] == end_date)\n",
    "               ].copy()\n",
    "    \n",
    "    values = json.loads(df['bands_min_max'].iloc[0])\n",
    "    \n",
    "    # Create composite\n",
    "    image = ee_collection_specifics.Composite(collection)(init_date, end_date)\n",
    "    \n",
    "    # Normalize images\n",
    "    if bool(values): \n",
    "        image = normalize_ee_images(image, collection, values)\n",
    "    \n",
    "        \n",
    "    for params in ee_collection_specifics.vizz_params(collection):\n",
    "        mapid = image.getMapId(params)\n",
    "        folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name=str(params['bands']),\n",
    "      ).add_to(Map)\n",
    "\n",
    "Map.add_child(folium.LayerControl())\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select input/output bands\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bands = ['B1','B2','B3','B4','B5','B6','B7','ndvi','ndwi']\n",
    "output_bands = ['cropland', 'land', 'water', 'urban']\n",
    "\n",
    "bands = [input_bands, output_bands]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Populate `image` table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and image tables\n",
    "datasets = pd.read_csv('Database/dataset.csv', index_col=0)\n",
    "images = pd.read_csv('Database/image.csv', index_col=0)\n",
    "images = images.astype({\"bands_selections\": str})\n",
    "\n",
    "for n, collection in enumerate(collections):\n",
    "\n",
    "    dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "    \n",
    "    df = images[(images['dataset_id'] == dataset_id) & \n",
    "                (images['scale'] == scale) & \n",
    "                (images['init_date'] == init_date) & \n",
    "                (images['end_date'] == end_date)\n",
    "               ].copy()\n",
    "    \n",
    "    # Take rows where bands_selections column is NaN\n",
    "    df1 = df[df['bands_selections'] == 'nan'].copy()\n",
    "    \n",
    "    if df1.any().any():\n",
    "        # Take first index\n",
    "        index = df1.index[0]\n",
    "        images.at[index, 'bands_selections'] = str(bands[n])\n",
    "    else:\n",
    "        if images[['dataset_id', 'bands_selections', 'scale', 'init_date', 'end_date']].isin([dataset_id, str(bands[n]), scale, init_date, end_date]).all(axis=1).any():\n",
    "            images = images\n",
    "        else:\n",
    "            df2 = df.iloc[0:1].copy()\n",
    "            df2.at[df2.index[0], 'bands_selections'] = str(bands[n])\n",
    "            images = images.append(df2, ignore_index = True)\n",
    "                   \n",
    "# Save table\n",
    "images.to_csv('Database/image.csv')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create TFRecords for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**geoStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygons_to_geoStoreMultiPoligon(Polygons):\n",
    "    MultiPoligon = {}\n",
    "    properties = [\"training\", \"validation\"]\n",
    "    features = []\n",
    "    for n, polygons in enumerate(Polygons):\n",
    "        multipoligon = []\n",
    "        for polygon in polygons.get('features'):\n",
    "            multipoligon.append(polygon.get('geometry').get('coordinates'))\n",
    "            \n",
    "        features.append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\"name\": properties[n]},\n",
    "            \"geometry\": {\n",
    "                \"type\": \"MultiPolygon\",\n",
    "                \"coordinates\":  multipoligon\n",
    "            }\n",
    "        }\n",
    "        ) \n",
    "        \n",
    "    MultiPoligon = {\n",
    "        \"geojson\": {\n",
    "            \"type\": \"FeatureCollection\", \n",
    "            \"features\": features\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return MultiPoligon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if collections[1] == 'USGS-National-Land-Cover-Database':\n",
    "    trainPolygons = {\"type\":\"FeatureCollection\",\"features\":[{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-123.22265625000001,45.213003555993964],[-122.03613281249999,45.213003555993964],[-122.03613281249999,46.164614496897094],[-123.22265625000001,46.164614496897094],[-123.22265625000001,45.213003555993964]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-122.1240234375,38.16911413556086],[-120.76171875,38.16911413556086],[-120.76171875,39.13006024213511],[-122.1240234375,39.13006024213511],[-122.1240234375,38.16911413556086]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-119.70703125,34.77771580360469],[-118.3447265625,34.77771580360469],[-118.3447265625,35.92464453144099],[-119.70703125,35.92464453144099],[-119.70703125,34.77771580360469]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-115.97167968750001,35.496456056584165],[-114.521484375,35.496456056584165],[-114.521484375,36.73888412439431],[-115.97167968750001,36.73888412439431],[-115.97167968750001,35.496456056584165]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-118.21289062499999,33.797408767572485],[-116.23535156249999,33.797408767572485],[-116.23535156249999,34.379712580462204],[-118.21289062499999,34.379712580462204],[-118.21289062499999,33.797408767572485]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-112.6318359375,33.02708758002874],[-111.4013671875,33.02708758002874],[-111.4013671875,34.016241889667015],[-112.6318359375,34.016241889667015],[-112.6318359375,33.02708758002874]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-105.6005859375,39.40224434029275],[-104.5458984375,39.40224434029275],[-104.5458984375,40.44694705960048],[-105.6005859375,40.44694705960048],[-105.6005859375,39.40224434029275]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-112.67578124999999,40.27952566881291],[-111.4453125,40.27952566881291],[-111.4453125,41.21172151054787],[-112.67578124999999,41.21172151054787],[-112.67578124999999,40.27952566881291]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-97.734375,32.21280106801518],[-95.9326171875,32.21280106801518],[-95.9326171875,33.32134852669881],[-97.734375,33.32134852669881],[-97.734375,32.21280106801518]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-99.36035156249999,29.036960648558267],[-97.822265625,29.036960648558267],[-97.822265625,30.031055426540206],[-99.36035156249999,30.031055426540206],[-99.36035156249999,29.036960648558267]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-95.185546875,38.61687046392973],[-93.9990234375,38.61687046392973],[-93.9990234375,39.639537564366684],[-95.185546875,39.639537564366684],[-95.185546875,38.61687046392973]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-91.2744140625,38.30718056188316],[-89.6484375,38.30718056188316],[-89.6484375,39.16414104768742],[-91.2744140625,39.16414104768742],[-91.2744140625,38.30718056188316]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-88.330078125,41.343824581185686],[-86.8798828125,41.343824581185686],[-86.8798828125,42.391008609205045],[-88.330078125,42.391008609205045],[-88.330078125,41.343824581185686]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-93.91113281249999,44.49650533109348],[-92.5048828125,44.49650533109348],[-92.5048828125,45.583289756006316],[-93.91113281249999,45.583289756006316],[-93.91113281249999,44.49650533109348]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-81.38671875,34.813803317113155],[-80.2880859375,34.813803317113155],[-80.2880859375,35.782170703266075],[-81.38671875,35.782170703266075],[-81.38671875,34.813803317113155]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-85.0341796875,33.17434155100208],[-83.7158203125,33.17434155100208],[-83.7158203125,34.27083595165],[-85.0341796875,34.27083595165],[-85.0341796875,33.17434155100208]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-87.2314453125,35.60371874069731],[-86.17675781249999,35.60371874069731],[-86.17675781249999,36.63316209558658],[-87.2314453125,36.63316209558658],[-87.2314453125,35.60371874069731]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-87.14355468749999,32.91648534731439],[-86.2646484375,32.91648534731439],[-86.2646484375,33.97980872872457],[-87.14355468749999,33.97980872872457],[-87.14355468749999,32.91648534731439]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-81.9140625,27.566721430409707],[-81.03515625,27.566721430409707],[-81.03515625,28.844673680771795],[-81.9140625,28.844673680771795],[-81.9140625,27.566721430409707]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-84.7705078125,38.92522904714054],[-83.75976562499999,38.92522904714054],[-83.75976562499999,40.17887331434696],[-84.7705078125,40.17887331434696],[-84.7705078125,38.92522904714054]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-80.947265625,40.27952566881291],[-79.98046875,40.27952566881291],[-79.98046875,41.178653972331674],[-80.947265625,41.178653972331674],[-80.947265625,40.27952566881291]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-75.2783203125,40.613952441166596],[-73.8720703125,40.613952441166596],[-73.8720703125,41.21172151054787],[-75.2783203125,41.21172151054787],[-75.2783203125,40.613952441166596]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-78.0908203125,38.44498466889473],[-76.728515625,38.44498466889473],[-76.728515625,39.33429742980725],[-78.0908203125,39.33429742980725],[-78.0908203125,38.44498466889473]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-112.6318359375,46.164614496897094],[-111.4453125,46.164614496897094],[-111.4453125,46.86019101567027],[-112.6318359375,46.86019101567027],[-112.6318359375,46.164614496897094]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-117.1142578125,43.229195113965005],[-115.57617187499999,43.229195113965005],[-115.57617187499999,44.08758502824516],[-117.1142578125,44.08758502824516],[-117.1142578125,43.229195113965005]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-96.328125,35.746512259918504],[-95.2734375,35.746512259918504],[-95.2734375,36.4566360115962],[-96.328125,36.4566360115962],[-96.328125,35.746512259918504]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-98.173828125,35.02999636902566],[-96.9873046875,35.02999636902566],[-96.9873046875,35.817813158696616],[-98.173828125,35.817813158696616],[-98.173828125,35.02999636902566]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-92.6806640625,34.379712580462204],[-91.7578125,34.379712580462204],[-91.7578125,35.10193405724606],[-92.6806640625,35.10193405724606],[-92.6806640625,34.379712580462204]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-90.7470703125,34.63320791137959],[-89.3408203125,34.63320791137959],[-89.3408203125,35.71083783530009],[-90.7470703125,35.71083783530009],[-90.7470703125,34.63320791137959]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-107.314453125,34.74161249883172],[-106.12792968749999,34.74161249883172],[-106.12792968749999,35.60371874069731],[-107.314453125,35.60371874069731],[-107.314453125,34.74161249883172]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-94.3505859375,41.1455697310095],[-92.94433593749999,41.1455697310095],[-92.94433593749999,42.19596877629178],[-94.3505859375,42.19596877629178],[-94.3505859375,41.1455697310095]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-85.869140625,40.68063802521456],[-84.5947265625,40.68063802521456],[-84.5947265625,41.64007838467894],[-85.869140625,41.64007838467894],[-85.869140625,40.68063802521456]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-87.099609375,39.30029918615029],[-85.6494140625,39.30029918615029],[-85.6494140625,40.245991504199026],[-87.099609375,40.245991504199026],[-87.099609375,39.30029918615029]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-117.7734375,47.30903424774781],[-116.103515625,47.30903424774781],[-116.103515625,48.1367666796927],[-117.7734375,48.1367666796927],[-117.7734375,47.30903424774781]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-97.91015624999999,37.3002752813443],[-96.8115234375,37.3002752813443],[-96.8115234375,38.09998264736481],[-97.91015624999999,38.09998264736481],[-97.91015624999999,37.3002752813443]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-94.06494140625,32.25926542645933],[-93.4716796875,32.25926542645933],[-93.4716796875,32.7872745269555],[-94.06494140625,32.7872745269555],[-94.06494140625,32.25926542645933]]]}}]}  \n",
    "    evalPolygons = {\"type\":\"FeatureCollection\",\"features\":[{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-95.888671875,29.38217507514529],[-95.06469726562499,29.38217507514529],[-95.06469726562499,30.12612436422458],[-95.888671875,30.12612436422458],[-95.888671875,29.38217507514529]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-83.84765625,42.374778361114195],[-82.94677734375,42.374778361114195],[-82.94677734375,42.78733853171998],[-83.84765625,42.78733853171998],[-83.84765625,42.374778361114195]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-96.88568115234375,40.69521661351714],[-95.77606201171875,40.69521661351714],[-95.77606201171875,41.393294288784865],[-96.88568115234375,41.393294288784865],[-96.88568115234375,40.69521661351714]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-105.05126953124999,38.57393751557591],[-104.490966796875,38.57393751557591],[-104.490966796875,39.0831721934762],[-105.05126953124999,39.0831721934762],[-105.05126953124999,38.57393751557591]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-122.62390136718749,46.95776134668866],[-121.84936523437499,46.95776134668866],[-121.84936523437499,48.04136507445029],[-122.62390136718749,48.04136507445029],[-122.62390136718749,46.95776134668866]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-120.157470703125,36.465471886798134],[-119.24560546875001,36.465471886798134],[-119.24560546875001,37.03763967977139],[-120.157470703125,37.03763967977139],[-120.157470703125,36.465471886798134]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-120.02563476562501,39.33854604847979],[-119.55871582031251,39.33854604847979],[-119.55871582031251,39.7240885773337],[-120.02563476562501,39.7240885773337],[-120.02563476562501,39.33854604847979]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-86.30859375,37.61423141542417],[-84.9462890625,37.61423141542417],[-84.9462890625,38.65119833229951],[-86.30859375,38.65119833229951],[-86.30859375,37.61423141542417]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-78.31054687499999,36.914764288955936],[-76.86035156249999,36.914764288955936],[-76.86035156249999,38.03078569382294],[-78.31054687499999,38.03078569382294],[-78.31054687499999,36.914764288955936]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-102.87597656249999,31.541089879585808],[-101.4697265625,31.541089879585808],[-101.4697265625,32.24997445586331],[-102.87597656249999,32.24997445586331],[-102.87597656249999,31.541089879585808]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-83.5400390625,39.50404070558415],[-82.177734375,39.50404070558415],[-82.177734375,40.54720023441049],[-83.5400390625,40.54720023441049],[-83.5400390625,39.50404070558415]]]}}]}\n",
    "    \n",
    "    geostore = polygons_to_geoStoreMultiPoligon([trainPolygons, evalPolygons])\n",
    "    \n",
    "if collections[1] == 'USDA-NASS-Cropland-Data-Layers':\n",
    "    trainPolygons = {\"type\": \"FeatureCollection\", \"features\": [{\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-122.882080078125, 40.50126945841645], [-122.1240234375, 40.50126945841645], [-122.1240234375, 41.008920735004885], [-122.882080078125, 41.008920735004885], [-122.882080078125, 40.50126945841645]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-122.2283935546875, 39.00637903337455], [-121.607666015625, 39.00637903337455], [-121.607666015625, 39.46588451142044], [-122.2283935546875, 39.46588451142044], [-122.2283935546875, 39.00637903337455]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.355224609375, 38.77978137804918], [-119.608154296875, 38.77978137804918], [-119.608154296875, 39.342794408952365], [-120.355224609375, 39.342794408952365], [-120.355224609375, 38.77978137804918]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-121.90979003906249, 37.70555348721583], [-120.9814453125, 37.70555348721583], [-120.9814453125, 38.39764411353178], [-121.90979003906249, 38.39764411353178], [-121.90979003906249, 37.70555348721583]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.03662109374999, 37.45741810262938], [-119.1851806640625, 37.45741810262938], [-119.1851806640625, 38.08268954483802], [-120.03662109374999, 38.08268954483802], [-120.03662109374999, 37.45741810262938]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.03662109374999, 37.45741810262938], [-119.1851806640625, 37.45741810262938], [-119.1851806640625, 38.08268954483802], [-120.03662109374999, 38.08268954483802], [-120.03662109374999, 37.45741810262938]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.03662109374999, 37.45741810262938], [-119.1851806640625, 37.45741810262938], [-119.1851806640625, 38.08268954483802], [-120.03662109374999, 38.08268954483802], [-120.03662109374999, 37.45741810262938]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-112.554931640625, 33.0178760185549], [-111.588134765625, 33.0178760185549], [-111.588134765625, 33.78827853625996], [-112.554931640625, 33.78827853625996], [-112.554931640625, 33.0178760185549]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-112.87353515625, 40.51379915504413], [-111.829833984375, 40.51379915504413], [-111.829833984375, 41.28606238749825], [-112.87353515625, 41.28606238749825], [-112.87353515625, 40.51379915504413]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-108.19335937499999, 39.095962936305476], [-107.1826171875, 39.095962936305476], [-107.1826171875, 39.85915479295669], [-108.19335937499999, 39.85915479295669], [-108.19335937499999, 39.095962936305476]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-124.25537109375, 30.86451022625836], [-124.25537109375, 30.86451022625836], [-124.25537109375, 30.86451022625836], [-124.25537109375, 30.86451022625836]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-106.875, 37.142803443716836], [-105.49072265625, 37.142803443716836], [-105.49072265625, 38.18638677411551], [-106.875, 38.18638677411551], [-106.875, 37.142803443716836]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-117.31201171875001, 43.27720532212024], [-116.01562499999999, 43.27720532212024], [-116.01562499999999, 44.134913443750726], [-117.31201171875001, 44.134913443750726], [-117.31201171875001, 43.27720532212024]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-115.7080078125, 44.69989765840318], [-114.7412109375, 44.69989765840318], [-114.7412109375, 45.36758436884978], [-115.7080078125, 45.36758436884978], [-115.7080078125, 44.69989765840318]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.65185546875, 47.517200697839414], [-119.33349609375, 47.517200697839414], [-119.33349609375, 48.32703913063476], [-120.65185546875, 48.32703913063476], [-120.65185546875, 47.517200697839414]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-119.83886718750001, 45.69083283645816], [-118.38867187500001, 45.69083283645816], [-118.38867187500001, 46.694667307773116], [-119.83886718750001, 46.694667307773116], [-119.83886718750001, 45.69083283645816]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-107.09472656249999, 47.45780853075031], [-105.84228515625, 47.45780853075031], [-105.84228515625, 48.31242790407178], [-107.09472656249999, 48.31242790407178], [-107.09472656249999, 47.45780853075031]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-101.57958984375, 46.93526088057719], [-100.107421875, 46.93526088057719], [-100.107421875, 47.945786463687185], [-101.57958984375, 47.945786463687185], [-101.57958984375, 46.93526088057719]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-101.162109375, 44.32384807250689], [-99.7119140625, 44.32384807250689], [-99.7119140625, 45.22848059584359], [-101.162109375, 45.22848059584359], [-101.162109375, 44.32384807250689]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-100.5908203125, 41.261291493919884], [-99.25048828124999, 41.261291493919884], [-99.25048828124999, 42.114523952464246], [-100.5908203125, 42.114523952464246], [-100.5908203125, 41.261291493919884]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-97.9541015625, 37.142803443716836], [-96.65771484375, 37.142803443716836], [-96.65771484375, 38.13455657705411], [-97.9541015625, 38.13455657705411], [-97.9541015625, 37.142803443716836]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-112.78564453124999, 32.91648534731439], [-111.357421875, 32.91648534731439], [-111.357421875, 33.925129700072], [-112.78564453124999, 33.925129700072], [-112.78564453124999, 32.91648534731439]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-106.435546875, 35.15584570226544], [-105.22705078125, 35.15584570226544], [-105.22705078125, 36.13787471840729], [-106.435546875, 36.13787471840729], [-106.435546875, 35.15584570226544]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-97.3828125, 32.45415593941475], [-96.2841796875, 32.45415593941475], [-96.2841796875, 33.22949814144951], [-97.3828125, 33.22949814144951], [-97.3828125, 32.45415593941475]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-97.97607421875, 35.04798673426734], [-97.00927734375, 35.04798673426734], [-97.00927734375, 35.764343479667176], [-97.97607421875, 35.764343479667176], [-97.97607421875, 35.04798673426734]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-97.97607421875, 35.04798673426734], [-97.00927734375, 35.04798673426734], [-97.00927734375, 35.764343479667176], [-97.97607421875, 35.764343479667176], [-97.97607421875, 35.04798673426734]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-95.4052734375, 47.62097541515849], [-94.24072265625, 47.62097541515849], [-94.24072265625, 48.28319289548349], [-95.4052734375, 48.28319289548349], [-95.4052734375, 47.62097541515849]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-94.19677734375, 41.27780646738183], [-93.09814453125, 41.27780646738183], [-93.09814453125, 42.13082130188811], [-94.19677734375, 42.13082130188811], [-94.19677734375, 41.27780646738183]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-93.71337890625, 37.75334401310656], [-92.6806640625, 37.75334401310656], [-92.6806640625, 38.51378825951165], [-93.71337890625, 38.51378825951165], [-93.71337890625, 37.75334401310656]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-90.63720703125, 34.615126683462194], [-89.47265625, 34.615126683462194], [-89.47265625, 35.69299463209881], [-90.63720703125, 35.69299463209881], [-90.63720703125, 34.615126683462194]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-93.05419921875, 30.44867367928756], [-91.77978515625, 30.44867367928756], [-91.77978515625, 31.57853542647338], [-93.05419921875, 31.57853542647338], [-93.05419921875, 30.44867367928756]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-90.02197265625, 44.276671273775186], [-88.59374999999999, 44.276671273775186], [-88.59374999999999, 44.98034238084973], [-90.02197265625, 44.98034238084973], [-90.02197265625, 44.276671273775186]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-90.63720703125, 38.41055825094609], [-89.49462890625, 38.41055825094609], [-89.49462890625, 39.18117526158749], [-90.63720703125, 39.18117526158749], [-90.63720703125, 38.41055825094609]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-87.56103515625, 35.62158189955968], [-86.28662109375, 35.62158189955968], [-86.28662109375, 36.4566360115962], [-87.56103515625, 36.4566360115962], [-87.56103515625, 35.62158189955968]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-90.63720703125, 31.93351676190369], [-89.49462890625, 31.93351676190369], [-89.49462890625, 32.731840896865684], [-90.63720703125, 32.731840896865684], [-90.63720703125, 31.93351676190369]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-69.54345703125, 44.68427737181225], [-68.5107421875, 44.68427737181225], [-68.5107421875, 45.336701909968134], [-69.54345703125, 45.336701909968134], [-69.54345703125, 44.68427737181225]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-73.212890625, 41.49212083968776], [-72.35595703125, 41.49212083968776], [-72.35595703125, 42.032974332441405], [-73.212890625, 42.032974332441405], [-73.212890625, 41.49212083968776]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-77.93701171875, 38.70265930723801], [-76.97021484375, 38.70265930723801], [-76.97021484375, 39.26628442213066], [-77.93701171875, 39.26628442213066], [-77.93701171875, 38.70265930723801]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-79.25537109375, 35.44277092585766], [-78.15673828125, 35.44277092585766], [-78.15673828125, 36.13787471840729], [-79.25537109375, 36.13787471840729], [-79.25537109375, 35.44277092585766]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-81.4306640625, 33.55970664841198], [-80.44189453125, 33.55970664841198], [-80.44189453125, 34.288991865037524], [-81.4306640625, 34.288991865037524], [-81.4306640625, 33.55970664841198]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-84.90234375, 33.394759218577995], [-83.91357421875, 33.394759218577995], [-83.91357421875, 34.19817309627726], [-84.90234375, 34.19817309627726], [-84.90234375, 33.394759218577995]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-82.28759765625, 28.246327971048842], [-81.2548828125, 28.246327971048842], [-81.2548828125, 29.209713225868185], [-82.28759765625, 29.209713225868185], [-82.28759765625, 28.246327971048842]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-109.88525390624999, 42.65012181368022], [-108.56689453125, 42.65012181368022], [-108.56689453125, 43.50075243569041], [-109.88525390624999, 43.50075243569041], [-109.88525390624999, 42.65012181368022]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-117.61962890624999, 39.04478604850143], [-116.65283203124999, 39.04478604850143], [-116.65283203124999, 39.740986355883564], [-117.61962890624999, 39.740986355883564], [-117.61962890624999, 39.04478604850143]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-102.67822265625, 31.42866311735861], [-101.71142578125, 31.42866311735861], [-101.71142578125, 32.26855544621476], [-102.67822265625, 32.26855544621476], [-102.67822265625, 31.42866311735861]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-119.47631835937499, 36.03133177633187], [-118.58642578124999, 36.03133177633187], [-118.58642578124999, 36.55377524336089], [-119.47631835937499, 36.55377524336089], [-119.47631835937499, 36.03133177633187]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-116.224365234375, 33.091541548655215], [-115.56518554687499, 33.091541548655215], [-115.56518554687499, 33.568861182555565], [-116.224365234375, 33.568861182555565], [-116.224365234375, 33.091541548655215]]]}}]}\n",
    "    evalPolygons = {\"type\": \"FeatureCollection\", \"features\": [{\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-122.13208008, 41.25126946], [-121.37402344, 41.25126946], [-121.37402344, 41.75892074], [-122.13208008, 41.75892074], [-122.13208008, 41.25126946]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-121.15979004, 38.45555349], [-120.23144531, 38.45555349], [-120.23144531, 39.14764411], [-121.15979004, 39.14764411], [-121.15979004, 38.45555349]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-111.80493164, 33.76787602], [-110.83813477, 33.76787602], [-110.83813477, 34.53827854], [-111.80493164, 34.53827854], [-111.80493164, 33.76787602]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-106.125, 37.89280344], [-104.74072266, 37.89280344], [-104.74072266, 38.93638677], [-106.125, 38.93638677], [-106.125, 37.89280344]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-119.08886719, 46.44083284], [-117.63867188, 46.44083284], [-117.63867188, 47.44466731], [-119.08886719, 47.44466731], [-119.08886719, 46.44083284]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-99.84082031, 42.01129149], [-98.50048828, 42.01129149], [-98.50048828, 42.86452395], [-99.84082031, 42.86452395], [-99.84082031, 42.01129149]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-96.6328125, 33.20415594], [-95.53417969, 33.20415594], [-95.53417969, 33.97949814], [-96.6328125, 33.97949814], [-96.6328125, 33.20415594]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-93.44677734, 42.02780647], [-92.34814453, 42.02780647], [-92.34814453, 42.8808213], [-93.44677734, 42.8808213], [-93.44677734, 42.02780647]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-89.27197266, 45.02667127], [-87.84375, 45.02667127], [-87.84375, 45.73034238], [-89.27197266, 45.73034238], [-89.27197266, 45.02667127]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-68.79345703, 45.43427737], [-67.76074219, 45.43427737], [-67.76074219, 46.08670191], [-68.79345703, 46.08670191], [-68.79345703, 45.43427737]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-80.68066406, 34.30970665], [-79.69189453, 34.30970665], [-79.69189453, 35.03899187], [-80.68066406, 35.03899187], [-80.68066406, 34.30970665]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-116.86962891, 39.79478605], [-115.90283203, 39.79478605], [-115.90283203, 40.49098636], [-116.86962891, 40.49098636], [-116.86962891, 39.79478605]]]}}]}\n",
    "    \n",
    "    geostore = polygons_to_geoStoreMultiPoligon([trainPolygons, evalPolygons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = len(geostore.get('geojson').get('features')[0].get('geometry').get('coordinates'))\n",
    "nEval = len(geostore.get('geojson').get('features')[1].get('geometry').get('coordinates'))\n",
    "print('Number of training polygons:',  nTrain)\n",
    "print('Number of training polygons:',  nEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipolygon = Skydipper.Geometry(attributes=geostore)\n",
    "multipolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipolygon.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Polygons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "Map = folium.Map(location=[38., -100.], zoom_start=5)\n",
    "for params in ee_collection_specifics.vizz_params(output_dataset):\n",
    "    mapid = image.getMapId(params)\n",
    "    folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name=str(params['bands']),\n",
    " ).add_to(Map)\n",
    " \n",
    "\n",
    "#  Convert the GeoJSONs to feature collections\n",
    "trainFeatures = ee.FeatureCollection([geostore.get('geojson').get('features')[0]])\n",
    "evalFeatures = ee.FeatureCollection([geostore.get('geojson').get('features')[1]])\n",
    "\n",
    "polyImage = ee.Image(0).byte().paint(trainFeatures, 1).paint(evalFeatures, 2)\n",
    "polyImage = polyImage.updateMask(polyImage)\n",
    "\n",
    "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
    "folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='training polygons',\n",
    "  ).add_to(Map)\n",
    "Map.add_child(folium.LayerControl())\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An array of images\n",
    "\n",
    "We have to stack the 2D images (input and output images of the Neural Network) to create a single image from which samples can be taken. Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band. This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [neighborhoodToArray()](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_into_array(collections, bands, init_date, end_date, scale):\n",
    "    kernelSize = 256\n",
    "    \n",
    "    # Read dataset and image tables\n",
    "    datasets = pd.read_csv('Database/dataset.csv', index_col=0)\n",
    "    images = pd.read_csv('Database/image.csv', index_col=0)\n",
    "    \n",
    "    for n, collection in enumerate(collections):\n",
    "        \n",
    "        dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "        \n",
    "        df = images[(images['dataset_id'] == dataset_id) & \n",
    "                (images['bands_selections'] == str(bands[n])) & \n",
    "                (images['scale'] == scale) & \n",
    "                (images['init_date'] == init_date) & \n",
    "                (images['end_date'] == end_date)\n",
    "               ].copy()\n",
    "        \n",
    "        values = json.loads(df['bands_min_max'].iloc[0])\n",
    "    \n",
    "        # Create composite\n",
    "        composite = ee_collection_specifics.Composite(collection)(init_date, end_date)\n",
    "        \n",
    "        print(composite.getInfo())\n",
    "    \n",
    "        # Normalize images\n",
    "        if bool(values): \n",
    "            composite = normalize_ee_images(composite, collection, values)\n",
    "        \n",
    "        if n == 0:\n",
    "            image = composite.select(bands[n])\n",
    "        else:\n",
    "            featureStack = ee.Image.cat([image,composite.select(bands[n])]).float()\n",
    "            \n",
    "    list = ee.List.repeat(1, kernelSize)\n",
    "    lists = ee.List.repeat(list, kernelSize)\n",
    "    kernel = ee.Kernel.fixed(kernelSize, kernelSize, lists)\n",
    "    \n",
    "    arrays = featureStack.neighborhoodToArray(kernel)\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export TFRecords\n",
    "\n",
    "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export. The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point. It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record. You do NOT need to export each training/testing patch to a different image. Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the computed value too large error. Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_ids(collections, bands, scale, init_date, end_date):\n",
    "    # Read dataset and image tables\n",
    "    datasets = pd.read_csv('Database/dataset.csv', index_col=0)\n",
    "    images = pd.read_csv('Database/image.csv', index_col=0)\n",
    "    \n",
    "    image_ids = []\n",
    "    for n, collection in enumerate(collections):\n",
    "        \n",
    "        dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "        \n",
    "        df = images[(images['dataset_id'] == dataset_id) & \n",
    "                (images['bands_selections'] == str(bands[n])) & \n",
    "                (images['scale'] == scale) & \n",
    "                (images['init_date'] == init_date) & \n",
    "                (images['end_date'] == end_date)\n",
    "               ].copy()\n",
    "        \n",
    "        image_ids.append(df.index[0])\n",
    "        \n",
    "    return image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeoJSONs_to_FeatureCollections(geostore):\n",
    "    feature_collections = []\n",
    "    for n in range(len(geostore.get('geojson').get('features'))):\n",
    "        # Make a list of Features\n",
    "        features = []\n",
    "        for i in range(len(geostore.get('geojson').get('features')[n].get('geometry').get('coordinates'))):\n",
    "            features.append(\n",
    "                ee.Feature(\n",
    "                    ee.Geometry.Polygon(\n",
    "                        geostore.get('geojson').get('features')[n].get('geometry').get('coordinates')[i]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        # Create a FeatureCollection from the list.\n",
    "        feature_collections.append(ee.FeatureCollection(features))\n",
    "    return feature_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_TFRecords(arrays, scale, bands, image_ids, geostore_id, sample_size, feature_collections, feature_lists):\n",
    "    # Export all the training/evaluation data (in many pieces), with one task per geometry.\n",
    "    \n",
    "    # These numbers determined experimentally.\n",
    "    nShards  = int(sample_size/20) # Number of shards in each polygon.\n",
    "    \n",
    "    base_names = ['training_patches', 'eval_patches']\n",
    "    bucket = 'geo-ai'\n",
    "    folder = 'Data/'+str(image_ids[0])+'_'+ str(image_ids[1])+'/'+str(geostore_id)+'/'+str(sample_size)\n",
    "\n",
    "    file_paths = []\n",
    "    for i, feature in enumerate(feature_collections):\n",
    "        for g in range(feature.size().getInfo()):\n",
    "            geomSample = ee.FeatureCollection([])\n",
    "            for j in range(nShards):\n",
    "                sample = arrays.sample(\n",
    "                    region = ee.Feature(feature_lists[i].get(g)).geometry(), \n",
    "                    scale = scale, \n",
    "                    numPixels = sample_size / nShards, # Size of the shard.\n",
    "                    seed = j,\n",
    "                    tileScale = 8\n",
    "                )\n",
    "                geomSample = geomSample.merge(sample)\n",
    "                \n",
    "            desc = base_names[i] + '_g' + str(g)\n",
    "            \n",
    "            file_paths.append(bucket+ '/' + folder + '/' + desc)\n",
    "            \n",
    "            task = ee.batch.Export.table.toCloudStorage(\n",
    "                collection = geomSample,\n",
    "                description = desc, \n",
    "                bucket = bucket, \n",
    "                fileNamePrefix = folder + '/' + desc,\n",
    "                fileFormat = 'TFRecord',\n",
    "                selectors = bands[0] + bands[1]\n",
    "            )\n",
    "            task.start()\n",
    "            \n",
    "    return task, file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = get_image_ids(collections, bands, scale, init_date, end_date)\n",
    "geostore_id = multipolygon.id\n",
    "sample_size = 1000 # Total sample size in each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the GeoJSON to feature collections\n",
    "feature_collections = GeoJSONs_to_FeatureCollections(geostore)\n",
    "\n",
    "# Convert the feature collections to lists for iteration.\n",
    "feature_lists = list(map(lambda x: x.toList(x.size()), feature_collections))\n",
    "\n",
    "# Export all the training/evaluation data (in many pieces), with one task per geometry.\n",
    "versions = pd.read_csv('Database/model_versions.csv', index_col=0)\n",
    "versions.replace(np.nan, '', regex=True, inplace = True)\n",
    "df = versions[['input_image_id', 'output_image_id', 'geostore_id', 'sample_size']].isin([image_ids[0], image_ids[1], geostore_id, sample_size]).copy()\n",
    "if not df.all(axis=1).any() and not (versions[df.all(axis=1)]['data_status'] == 'COMPLETED').all():\n",
    "    task, file_paths= export_TFRecords(arrays, scale, bands, image_ids, geostore_id, sample_size, feature_collections, feature_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate `model_versions` tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (versions.empty) or not df.all(axis=1).any():\n",
    "    dictionary = dict(zip(list(versions.keys()), [[''], [''], [image_ids[0]], [image_ids[1]], [geostore_id], [sample_size], [''], [''], [''], [''], [''], ['']]))\n",
    "    versions = versions.append(pd.DataFrame(dictionary), ignore_index = True, sort=False)\n",
    "    \n",
    "# Save table\n",
    "versions.to_csv('Database/model_versions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def check_status_data(task, file_paths):\n",
    "    status_list = list(map(lambda x: str(x), task.list()[:len(file_paths)])) \n",
    "    status_list = list(map(lambda x: x[x.find(\"(\")+1:x.find(\")\")], status_list))\n",
    "    \n",
    "    return status_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (versions[df.all(axis=1)]['data_status'] == 'COMPLETED').all():\n",
    "    status_list = check_status_data(task, file_paths)\n",
    "    index = versions.index[-1]\n",
    "    while not status_list == ['COMPLETED'] * len(file_paths):\n",
    "        status_list = check_status_data(task, file_paths)\n",
    "        \n",
    "        #Save temporal status in table\n",
    "        versions.at[index, 'data_status'] = json.dumps(dict(zip(file_paths, status_list)))\n",
    "        versions.to_csv('Database/model_versions.csv')\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    #Save final status in table\n",
    "    versions.at[index, 'data_status'] = \"COMPLETED\"\n",
    "    versions.to_csv('Database/model_versions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Training the model in AI Platform\n",
    "### Training code package setup\n",
    "\n",
    "It's necessary to create a Python package to hold the training code.  Here we're going to get started with that by creating a folder for the package and adding an empty `__init__.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = 'AI_Platform/cnn_trainer'\n",
    "PACKAGE_FOLDER = '/trainer'\n",
    "\n",
    "!rm -r {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH+PACKAGE_FOLDER}\n",
    "!touch {ROOT_PATH+PACKAGE_FOLDER}/__init__.py\n",
    "!ls -l {ROOT_PATH+PACKAGE_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for training in AI platform\n",
    "bucket = 'geo-ai'\n",
    "project_id = env.project_id\n",
    "region = \"us-central1\"\n",
    "\n",
    "trainer_package_path = 'AI_Platform/cnn_trainer/trainer/'\n",
    "main_trainer_module = 'trainer.task'\n",
    "\n",
    "model_type = 'segmentation'\n",
    "model_architecture = 'deepvel'\n",
    "\n",
    "# Training parameters\n",
    "training_params = {\n",
    "    \"bucket\": bucket,\n",
    "    \"base_names\": ['training_patches', 'eval_patches'],\n",
    "    \"data_dir\": 'gs://' + bucket + '/Data/' + str(image_ids[0])+'_'+ str(image_ids[1])+'/'+str(geostore_id)+'/'+str(sample_size),\n",
    "    \"in_bands\": bands[0],\n",
    "    \"out_bands\": bands[1],\n",
    "    \"kernel_size\": 256,\n",
    "    \"train_size\": sample_size*nTrain,\n",
    "    \"eval_size\": sample_size*nEval,\n",
    "    \"model_type\": model_type,\n",
    "    \"model_architecture\": model_architecture,\n",
    "    \"output_activation\": '',\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 1,\n",
    "    \"shuffle_size\": 2000,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"loss\": \"mse\",\n",
    "    \"metrics\": ['RootMeanSquaredError']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Populate `model` table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.read_csv('Database/model.csv', index_col=0)\n",
    "models.replace(np.nan, '', regex=True, inplace = True)\n",
    "\n",
    "df = models[['model_type', 'output_image_id']].isin([model_type, image_ids[1]]).copy()\n",
    "if not df.all(axis=1).any():\n",
    "    dictionary = dict(zip(list(models.keys()), [[''], [model_type], [''], [image_ids[1]]]))\n",
    "    models = models.append(pd.DataFrame(dictionary), ignore_index = True, sort=False)\n",
    "\n",
    "# Save table\n",
    "models.to_csv('Database/model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = models[(models['model_type'] == model_type) & (models['output_image_id'] == image_ids[1])].index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Populate `model_versions` table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removekey(dictionary, key):\n",
    "    del dictionary[key]\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = pd.read_csv('Database/model_versions.csv', index_col=0, dtype={'version': str})\n",
    "versions.replace(np.nan, '', regex=True, inplace = True)\n",
    "df = versions.copy()\n",
    "\n",
    "# Check if the version already exists\n",
    "if (df['training_params'].apply(lambda x : removekey(json.loads(x),'job_dir')) == training_params).any():\n",
    "    # Get version id\n",
    "    version_id = df[df['training_params'].apply(lambda x : removekey(json.loads(x),'job_dir')) == training_params].index[0]\n",
    "    \n",
    "    # Check status\n",
    "    status = df.iloc[version_id]['training_status']\n",
    "    print('Version already exists with training status equal to:', status)\n",
    "    \n",
    "    if status == 'SUCCEEDED':\n",
    "        print('The training job successfully completed.')\n",
    "    if status == 'FAILED':\n",
    "        print('This version got a error while training.')\n",
    "        print('Change training parameters and try again.')\n",
    "    if status == 'CANCELLED':\n",
    "        print('The training job was cancelled.')\n",
    "        print('Start training again.')\n",
    "        # Get training version\n",
    "        training_version = df.iloc[version_id]['version']\n",
    "        \n",
    "        # Update job name\n",
    "        job_name = 'job_v' + str(int(time.time()))\n",
    "            \n",
    "        # Add job directory\n",
    "        training_params = json.loads(df.iloc[version_id]['training_params'])\n",
    "        training_params['job_dir'] = 'gs://' + bucket + '/Models/' + str(model_id) + '/' + training_version + '/'\n",
    "        \n",
    "        # Save training version and clear status\n",
    "        versions.at[version_id, 'training_params'] =  json.dumps(training_params)\n",
    "        versions.at[version_id, 'training_status'] = ''\n",
    "        \n",
    "# Create new version  \n",
    "else:\n",
    "    print('Create new version')\n",
    "    # New training version and job name\n",
    "    training_version = str(int(time.time()))\n",
    "    job_name = 'job_v' + training_version\n",
    "    \n",
    "    # Add job directory\n",
    "    training_params['job_dir'] = 'gs://' + bucket + '/Models/' + str(model_id) + '/' + training_version + '/'\n",
    "    \n",
    "    df = versions[['input_image_id', 'output_image_id', 'geostore_id', 'sample_size', 'version', 'data_status']].isin(\n",
    "        [image_ids[0], image_ids[1], geostore_id, sample_size, '', 'COMPLETED']).copy()\n",
    "    \n",
    "    # Check if untrained version already exists\n",
    "    if df.all(axis=1).any():\n",
    "        version_id = versions[(versions['input_image_id'] == image_ids[0]) & (versions['output_image_id'] == image_ids[1]) & \n",
    "                              (versions['geostore_id'] == geostore_id) & (versions['sample_size'] == sample_size)].index[0]\n",
    "    \n",
    "        versions.at[version_id, 'model_id'] = model_id\n",
    "        versions.at[version_id, 'model_architecture'] = model_architecture\n",
    "        versions.at[version_id, 'training_params'] = json.dumps(training_params)\n",
    "        versions.at[version_id, 'version'] = training_version\n",
    "        \n",
    "    else:\n",
    "        dictionary = dict(zip(list(versions.keys()), [[''], [''], [image_ids[0]], [image_ids[1]], [geostore_id], [sample_size], [''], [''], ['COMPLETED'], [''], [''], ['']]))\n",
    "        versions = versions.append(pd.DataFrame(dictionary), ignore_index = True, sort=False)\n",
    "        version_id = versions.index[-1]\n",
    "        \n",
    "        versions.at[version_id, 'model_id'] = model_id\n",
    "        versions.at[version_id, 'model_architecture'] = model_architecture\n",
    "        versions.at[version_id, 'training_params'] = json.dumps(training_params)\n",
    "        versions.at[version_id, 'version'] = training_version\n",
    "    \n",
    "versions.to_csv('Database/model_versions.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save training parameters**\n",
    "\n",
    "These training parameters need to be stored in a place where other code can access them.  There are a variety of ways of accomplishing that, but here we'll save it into a json file called `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ROOT_PATH+PACKAGE_FOLDER+'/training_params.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json.dumps(training_params), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ROOT_PATH+PACKAGE_FOLDER+'/training_params.json') as json_file:\n",
    "    config = json.loads(json.load(json_file))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/config.py\n",
    "\n",
    "config = {'bucket': 'geo-ai',\n",
    " 'base_names': ['training_patches', 'eval_patches'],\n",
    " 'data_dir': 'gs://geo-ai/Data/0_1/f75559fb87f5c22deb56eb2a73aa4e12/1000',\n",
    " 'in_bands': ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'ndvi', 'ndwi'],\n",
    " 'out_bands': ['cropland', 'land', 'water', 'urban'],\n",
    " 'kernel_size': 256,\n",
    " 'train_size': 47000,\n",
    " 'eval_size': 12000,\n",
    " 'model_type': 'segmentation',\n",
    " 'model_architecture': 'deepvel',\n",
    " 'output_activation': '',\n",
    " 'batch_size': 16,\n",
    " 'epochs': 1,\n",
    " 'shuffle_size': 2000,\n",
    " 'learning_rate': 0.001,\n",
    " 'loss': 'mse',\n",
    " 'metrics': ['RootMeanSquaredError'],\n",
    " 'job_dir': 'gs://geo-ai/Models/0/1579867953/'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training/evaluation data**\n",
    "\n",
    "The following is code to load training/evaluation data.  Write this into `util.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/util.py\n",
    "\"\"\"Utilities to download and preprocess the data.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from . import config\n",
    "\n",
    "#with open('training_params.json') as json_file:\n",
    "#    config = json.loads(json.load(json_file))\n",
    "\n",
    "def parse_function(proto):\n",
    "    \"\"\"The parsing function.\n",
    "    Read a serialized example into the structure defined by features_dict.\n",
    "    Args:\n",
    "      example_proto: a serialized Example.\n",
    "    Returns: \n",
    "      A dictionary of tensors, keyed by feature name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define your tfrecord \n",
    "    features = config.config.get('in_bands') + config.config.get('out_bands')\n",
    "    \n",
    "    # Specify the size and shape of patches expected by the model.\n",
    "    kernel_shape = [config.config.get('kernel_size'), config.config.get('kernel_size')]\n",
    "    columns = [\n",
    "      tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) for k in features\n",
    "    ]\n",
    "    features_dict = dict(zip(features, columns))\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.io.parse_single_example(proto, features_dict)\n",
    "\n",
    "    # Convert a dictionary of tensors to a tuple of (inputs, outputs)\n",
    "    inputs_list = [parsed_features.get(key) for key in features]\n",
    "    stacked = tf.stack(inputs_list, axis=0)\n",
    "    \n",
    "    # Convert the tensors into a stack in HWC shape\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    \n",
    "    return stacked[:,:,:len(config.config.get('in_bands'))], stacked[:,:,len(config.config.get('in_bands')):]\n",
    "\n",
    "def get_dataset(glob):\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "    Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "    \"\"\"\n",
    "    glob = tf.compat.v1.io.gfile.glob(glob)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=5)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_training_dataset():\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "    Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "    \"\"\"\n",
    "    glob = config.config.get('data_dir') + '/' + config.config.get('base_names')[0] + '*'\n",
    "    dataset = get_dataset(glob)\n",
    "    dataset = dataset.shuffle(config.config.get('shuffle_size')).batch(config.config.get('batch_size')).repeat()\n",
    "    return dataset\n",
    "\n",
    "def get_evaluation_dataset():\n",
    "    \"\"\"Get the preprocessed evaluation dataset\n",
    "    Returns: \n",
    "      A tf.data.Dataset of evaluation data.\n",
    "    \"\"\"\n",
    "    glob = config.config.get('data_dir') + '/' + config.config.get('base_names')[1] + '*'\n",
    "    dataset = get_dataset(glob)\n",
    "    dataset = dataset.batch(1).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that `util.py` is functioning as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_Platform.cnn_trainer.trainer import util\n",
    "\n",
    "training_dataset = util.get_training_dataset()\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "We rewrite the desired model (previously specified in `config.py`) into `model.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../models/{model_type}/{model_architecture+'.py'} {ROOT_PATH+PACKAGE_FOLDER}/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that `model.py` is functioning as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_Platform.cnn_trainer.trainer import model\n",
    "\n",
    "model = model.create_keras_model(inputShape = (None, None, len(training_params.get('in_bands'))), nClasses = len(training_params.get('out_bands')))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training task**\n",
    "\n",
    "The following will create `task.py`, which will get the training and evaluation data, train the model and save it when it's done in a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/task.py\n",
    "\"\"\"Trains a Keras model\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import config\n",
    "from . import util\n",
    "from . import model\n",
    "\n",
    "#with open('training_params.json') as json_file:\n",
    "#    config = json.loads(json.load(json_file))\n",
    "          \n",
    "def train_and_evaluate():\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the Keras Model\n",
    "    if not config.config.get('output_activation'):\n",
    "        keras_model = model.create_keras_model(inputShape = (None, None, len(config.config.get('in_bands'))), nClasses = len(config.config.get('out_bands')))\n",
    "    else:\n",
    "        keras_model = model.create_keras_model(inputShape = (None, None, len(config.config.get('in_bands'))), nClasses = len(config.config.get('out_bands')), output_activation = config.config.get('output_activation'))\n",
    "\n",
    "    # Compile Keras model\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=config.config.get('learning_rate'))\n",
    "    keras_model.compile(loss=config.config.get('loss'), optimizer=optimizer, metrics=config.config.get('metrics'))\n",
    "\n",
    "\n",
    "    # Pass a tfrecord\n",
    "    training_dataset = util.get_training_dataset()\n",
    "    evaluation_dataset = util.get_evaluation_dataset()\n",
    "\n",
    "    # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(os.path.join(config.config.get('job_dir'), 'logs'))\n",
    "\n",
    "    # Train model\n",
    "    keras_model.fit(\n",
    "        x=training_dataset,\n",
    "        steps_per_epoch=int(config.config.get('train_size') / config.config.get('batch_size')),\n",
    "        epochs=config.config.get('epochs'),\n",
    "        validation_data=evaluation_dataset,\n",
    "        validation_steps=int(config.config.get('eval_size') / config.config.get('batch_size')),\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard_cb])\n",
    "\n",
    "    tf.contrib.saved_model.save_keras_model(keras_model, os.path.join(config.config.get('job_dir'), 'model'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity('INFO')\n",
    "    train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using GPUs**\n",
    "\n",
    "AI Platform lets you run any TensorFlow training application on a GPU-enabled machine. Learn more about [using GPUs for training models in the cloud](https://cloud.google.com/ml-engine/docs/tensorflow/using-gpus#submit-job).\n",
    "We define a `config.yaml` file that describes the GPU options we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH}/config.yaml\n",
    "\n",
    "trainingInput:\n",
    "    scaleTier: CUSTOM\n",
    "    # A single NVIDIA Tesla V100 GPU\n",
    "    masterType: large_model_v100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the package to AI Platform for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up your GCP project**\n",
    "\n",
    "Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authenticate your GCP account**\n",
    "\n",
    "Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS 'privatekey.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit a training job to AI Platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training {job_name} \\\n",
    "    --job-dir {training_params.get('job_dir')} \\\n",
    "    --package-path {trainer_package_path} \\\n",
    "    --module-name {main_trainer_module} \\\n",
    "    --region {region} \\\n",
    "    --config {ROOT_PATH}/config.yaml \\\n",
    "    --runtime-version 1.14 \\\n",
    "    --python-version 3.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save training status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status_training(job_name, project_id):\n",
    "    desc = !gcloud ai-platform jobs describe {job_name} --project {project_id}\n",
    "    return desc.grep('state:')[0].split(':')[1].strip()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = pd.read_csv('Database/model_versions.csv', index_col=0, dtype={'version': str})\n",
    "\n",
    "status = check_status_training(job_name, project_id)\n",
    "while not status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "    status = check_status_training(job_name, project_id)\n",
    "    \n",
    "    #Save temporal status in table\n",
    "    versions.at[version_id, 'training_status'] = status\n",
    "    versions.to_csv('Database/model_versions.csv')\n",
    "    \n",
    "    time.sleep(60)\n",
    "\n",
    "#Save final status in table\n",
    "versions.at[version_id, 'training_status'] = status\n",
    "versions.to_csv('Database/model_versions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Prepare the model for making predictions in Earth Engine\n",
    "\n",
    "Before we can use the model in Earth Engine, it needs to be hosted by AI Platform.  But before we can host the model on AI Platform we need to *EEify* (a new word!) it.  The EEification process merely appends some extra operations to the input and outputs of the model in order to accomdate the interchange format between pixels from Earth Engine (float32) and inputs to AI Platform (base64).  (See [this doc](https://cloud.google.com/ml-engine/docs/online-predict#binary_data_in_prediction_input) for details.)  \n",
    "\n",
    "**`earthengine model prepare`**\n",
    "\n",
    "The EEification process is handled for you using the Earth Engine command `earthengine model prepare`.  To use that command, we need to specify the input and output model directories and the name of the input and output nodes in the TensorFlow computation graph.  We can do all that programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "model_path = training_params.get('job_dir') + 'model/'\n",
    "\n",
    "meta_graph_def = saved_model_utils.get_meta_graph_def(model_path, 'serve')\n",
    "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# model only has a single input and a single output.\n",
    "input_name = None\n",
    "for k,v in inputs.items():\n",
    "    input_name = v.name\n",
    "    break\n",
    "\n",
    "output_name = None\n",
    "for k,v in outputs.items():\n",
    "    output_name = v.name\n",
    "    break\n",
    "\n",
    "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
    "# AI Platform inputs and outputs, respectively.\n",
    "import json\n",
    "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "output_dict = \"'\" + json.dumps({output_name: \"prediction\"}) + \"'\"\n",
    "\n",
    "# Put the EEified model next to the trained model directory.\n",
    "EEified_path = training_params.get('job_dir') + 'eeified/' \n",
    "\n",
    "# You need to set the project before using the model prepare command.\n",
    "!earthengine set_project {project_id}\n",
    "!earthengine model prepare --source_dir {model_path} --dest_dir {EEified_path} --input {input_dict} --output {output_dict}\n",
    "\n",
    "# Populate models table\n",
    "versions.at[version_id, 'eeified'] = True\n",
    "versions.to_csv('Database/model_versions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployed the model to AI Platform**\n",
    "\n",
    "Before it's possible to get predictions from the trained and EEified model, it needs to be deployed on AI Platform.  The first step is to create the model.  The second step is to create a version.  See [this guide](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models) for details.  Note that models and versions can be monitored from the [AI Platform models page](http://console.cloud.google.com/ai-platform/models) of the Cloud Console. \n",
    "\n",
    "To ensure that the model is ready for predictions without having to warm up nodes, you can use a configuration yaml file to set the scaling type of this version to autoScaling, and, set a minimum number of nodes for the version. This will ensure there are always nodes on stand-by, however, you will be charged as long as they are running. For this example, we'll set the minNodes to 10. That means that at a minimum, 10 nodes are always up and running and waiting for predictions. The number of nodes will also scale up automatically if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "autoScaling:\n",
    "    minNodes: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-central1\"\n",
    "version_name = 'v' + training_version\n",
    "\n",
    "if not models.iloc[model_id]['model_name']:\n",
    "    model_name = models.iloc[model_id]['model_type']+'_'+str(versions.iloc[version_id]['output_image_id'])\n",
    "    models.at[model_id,'model_name'] = model_name\n",
    "    \n",
    "    models.to_csv('Database/model.csv')\n",
    "else:\n",
    "    model_name = models.iloc[model_id]['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating version: ' + version_name)\n",
    "\n",
    "!gcloud ai-platform models create {model_name} \n",
    "!gcloud ai-platform versions create {version_name} \\\n",
    "  --model {model_name} \\\n",
    "  --origin {EEified_path} \\\n",
    "  --runtime-version=1.14 \\\n",
    "  --framework \"TENSORFLOW\" \\\n",
    "  --python-version=3.5\n",
    "\n",
    "# Populate models table\n",
    "versions.at[version_id, 'deployed'] = True\n",
    "versions.to_csv('Database/model_versions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Predict in Earth Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_csv('Database/dataset.csv', index_col=0)\n",
    "images = pd.read_csv('Database/image.csv', index_col=0)\n",
    "models = pd.read_csv('Database/model.csv', index_col=0)\n",
    "versions = pd.read_csv('Database/model_versions.csv', index_col=0, dtype = {'model_id': pd.Int64Dtype(), 'version': pd.Int64Dtype()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select pre-trained models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(models['model_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'segmentation_0_1'\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select versions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = models[models['model_name'] == model_name].index[0]\n",
    "model_type = models.iloc[model_id]['model_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_names = list(map(lambda x: int(x), list(versions[versions['model_id'] == model_id]['version'])))\n",
    "print(version_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = version_names[1]\n",
    "version_id = versions[versions['version'] == version].index[0]\n",
    "version_name = 'v'+ str(version)\n",
    "print(version_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id = versions[versions['version'] == version].index[0]\n",
    "training_params =json.loads(versions[versions['version'] == version]['training_params'][version_id])\n",
    "image_ids = list(versions.iloc[version_id][['input_image_id', 'output_image_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = list(datasets.iloc[list(images.iloc[image_ids]['dataset_id'])]['slug'])\n",
    "bands = [training_params.get('in_bands'), training_params.get('out_bands')]\n",
    "scale, init_date, end_date = list(images.iloc[image_ids[0]][['scale', 'init_date', 'end_date']])\n",
    "scale = float(scale)\n",
    "project_id = env.project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Datasets: ', collections)\n",
    "print('Bands: ', bands)\n",
    "print('scale: ', scale)\n",
    "print('init_date: ', init_date)\n",
    "print('end_date: ', end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select new date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_date_new = '2018-01-01'\n",
    "end_date_new = '2018-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Polygon object from Geojson**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts={'geojson': {'type': 'FeatureCollection',\n",
    "  'features': [{'type': 'Feature',\n",
    "    'properties': {},\n",
    "    'geometry': {'type': 'Polygon',\n",
    "     'coordinates': [[[-3.9990234375,40.17887331434696],\n",
    "                      [ -3.3343505859375,40.17887331434696],\n",
    "                      [-3.3343505859375,40.57849862511043],\n",
    "                      [-3.9990234375,40.57849862511043],\n",
    "                      [-3.9990234375,40.17887331434696]]]}}]}}\n",
    "\n",
    "\n",
    "geometry = Skydipper.Geometry(attributes=atts)\n",
    "geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ee.Model.fromAiPlatformPredictor`**\n",
    "\n",
    "There is now a trained model, prepared for serving to Earth Engine, hosted and versioned on AI Platform.  \n",
    "We can now connect Earth Engine directly to the trained model for inference.  You do that with the `ee.Model.fromAiPlatformPredictor` command.\n",
    "For this command to work, we need to know a lot about the model.  To connect to the model, you need to know the name and version.\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "You need to be able to recreate the imagery on which it was trained in order to perform inference.  Specifically, you need to create an array-valued input from the scaled data and use that for input.  (Recall that the new input node is named `array`, which is convenient because the array image has one band, named `array` by default.)  The inputs will be provided as 144x144 patches (`inputTileSize`), at 30-meter resolution (`proj`), but 8 pixels will be thrown out (`inputOverlapSize`) to minimize boundary effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_id = versions.iloc[version_id]['input_image_id']\n",
    "    \n",
    "values = json.loads(images.iloc[input_image_id]['bands_min_max'])\n",
    "# Create composite\n",
    "image = ee_collection_specifics.Composite(collections[0])(init_date_new, end_date_new)\n",
    "\n",
    "# Normalize images\n",
    "if bool(values): \n",
    "    image = normalize_ee_images(image, collections[0], values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select bands and convert them into float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.select(bands[0]).float()\n",
    "image.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outputs**\n",
    "\n",
    "The output (which you also need to know)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and use it for prediction.\n",
    "model = ee.Model.fromAiPlatformPredictor(\n",
    "    projectName = project_id,\n",
    "    modelName = model_name,\n",
    "    version = version_name,\n",
    "    inputTileSize = [144, 144],\n",
    "    inputOverlapSize = [8, 8],\n",
    "    proj = ee.Projection('EPSG:4326').atScale(scale),\n",
    "    fixInputProj = True,\n",
    "    outputBands = {'prediction': {\n",
    "        'type': ee.PixelType.float(),\n",
    "        'dimensions': 1,\n",
    "      }                  \n",
    "    }\n",
    ")\n",
    "predictions = model.predictImage(image.toArray()).arrayFlatten([bands[1]])\n",
    "predictions.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the prediction area with the polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip the prediction area with the polygon\n",
    "polygon = ee.Geometry.Polygon(geometry.attributes.get('geojson').get('features')[0].get('geometry').get('coordinates'))\n",
    "predictions = predictions.clip(polygon)\n",
    "\n",
    "# Get centroid\n",
    "centroid = polygon.centroid().getInfo().get('coordinates')[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentate image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'segmentation':\n",
    "    maxValues = predictions.reduce(ee.Reducer.max())\n",
    "\n",
    "    predictions = predictions.addBands(maxValues)\n",
    "\n",
    "    expression = \"\"\n",
    "    for n, band in enumerate(bands[1]):\n",
    "        expression = expression + f\"(b('{band}') == b('max')) ? {str(n+1)} : \"\n",
    "\n",
    "    expression = expression + f\"0\"\n",
    "\n",
    "    segmentation = predictions.expression(expression)\n",
    "    predictions = predictions.addBands(segmentation.mask(segmentation).select(['constant'], ['categories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display**\n",
    "\n",
    "Use folium to visualize the input imagery and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 1})\n",
    "Map = folium.Map(location=centroid, zoom_start=11)\n",
    "folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(Map)\n",
    "\n",
    "for band in bands[1]:\n",
    "    mapid = predictions.getMapId({'bands': [band], 'min': 0, 'max': 1})\n",
    "    \n",
    "    folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name=band,\n",
    "      ).add_to(Map)\n",
    "\n",
    "\n",
    "if model_type == 'segmentation':\n",
    "    mapid = predictions.getMapId({'bands': ['categories'], 'min': 1, 'max': len(bands[1])})\n",
    "    \n",
    "    folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name='categories',\n",
    "      ).add_to(Map)\n",
    "    \n",
    "Map.add_child(folium.LayerControl())\n",
    "Map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
