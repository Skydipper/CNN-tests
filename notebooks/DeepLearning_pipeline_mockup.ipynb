{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning pipeline mockup\n",
    "\n",
    "[Graphical representation of the pipeline](https://www.draw.io/#G1U6XDddvcjas2vglyKeVz0ouFOElzOMCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup software libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize the Earth Engine library.\n",
    "import ee\n",
    "ee.Initialize()\n",
    "ee.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folium setup.\n",
    "import folium\n",
    "print(folium.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skydipper library.\n",
    "import Skydipper\n",
    "print(Skydipper.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import tarfile\n",
    "from pprint import pprint\n",
    "import env\n",
    "import time\n",
    "import sqlalchemy\n",
    "from sqlalchemy import Column, Integer, BigInteger, Float, Text, String, Boolean, DateTime\n",
    "from sqlalchemy.dialects.postgresql import JSON\n",
    "from shapely.geometry import shape\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Database\n",
    "\n",
    "We will create a Database to save all the attributes that we will generate all through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_table(table_path, columns, dtypes):\n",
    "    if not os.path.exists(table_path):\n",
    "        dictionary = dict(zip(columns, dtypes))\n",
    "        dtypes = np.dtype([(k, v) for k, v in dictionary.items()]) \n",
    "    \n",
    "        data = np.empty(0, dtype=dtypes)\n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "        df.to_csv(table_path, sep=';', quotechar='\\'',index=True, index_label='id')\n",
    "    else:\n",
    "        df = pd.read_csv(table_path, sep=';', quotechar='\\'')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Database'):\n",
    "    os.makedirs('Database')\n",
    "    \n",
    "datasets = create_db_table('Database/dataset.csv', \n",
    "                          columns = ['slug', 'name', 'bands', 'rgb_bands', 'provider'], \n",
    "                          dtypes = [str, str, list, list, str]\n",
    "                         )\n",
    "\n",
    "images = create_db_table('Database/image.csv', \n",
    "                          columns = ['dataset_id', 'bands_selections', 'scale', 'init_date',\n",
    "                                     'end_date', 'composite_method', 'bands_min_max', 'norm_type'], \n",
    "                          dtypes = [int, list, float, str, str, str, str, str]\n",
    "                         )\n",
    "\n",
    "models = create_db_table('Database/model.csv', \n",
    "                          columns = ['model_name', 'model_type', 'model_output', 'model_description', 'output_image_id'], \n",
    "                          dtypes = [str, str, str, str, int]\n",
    "                        )\n",
    "                         \n",
    "versions = create_db_table('Database/model_versions.csv', \n",
    "                           columns = ['model_id', 'model_architecture', 'input_image_id', 'output_image_id', 'geostore_id', 'kernel_size', 'sample_size', \n",
    "                                      'training_params', 'version', 'data_status', 'training_status', 'eeified', 'deployed'], \n",
    "                           dtypes = [int, str, int, int, str, int, int, str, int, str, str, bool, bool]   \n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Pandas to a Database with SQLAlchemy ([tutorial](https://hackersandslackers.com/connecting-pandas-to-a-sql-database-with-sqlalchemy/))\n",
    "\n",
    "#### Create an engine\n",
    "\n",
    "An `engine` is an object used to connect to databases using the information in our URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('postgresql://postgres:postgres@0.0.0.0:5432/geomodels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create SQL tables from DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_db(df, table_name):\n",
    "    if table_name == \"dataset\":\n",
    "        df.to_sql(\"dataset\",\n",
    "                       engine,\n",
    "                       if_exists='replace',\n",
    "                       schema='public',\n",
    "                       index=True,\n",
    "                       index_label='id',\n",
    "                       chunksize=500,\n",
    "                       dtype={\"slug\": Text,\n",
    "                              \"name\": Text,\n",
    "                              \"bands\": Text,\n",
    "                              \"bands\": Text,\n",
    "                              \"provider\": Text})\n",
    "    if table_name == \"image\":\n",
    "        df.to_sql(\"image\",\n",
    "                       engine,\n",
    "                       if_exists='replace',\n",
    "                       schema='public',\n",
    "                       index=True,\n",
    "                       index_label='id',\n",
    "                       chunksize=500,\n",
    "                       dtype={\"dataset_id \": Integer,\n",
    "                              \"bands_selections\": Text,\n",
    "                              \"scale\": Float,\n",
    "                              \"init_date\": Text,\n",
    "                              \"end_date\": Text,\n",
    "                              \"bands_min_max\": JSON,\n",
    "                              \"norm_type\": Text})\n",
    "    \n",
    "    if table_name == \"model\":\n",
    "        df.to_sql(\"model\",\n",
    "                       engine,\n",
    "                       if_exists='replace',\n",
    "                       schema='public',\n",
    "                       index=True,\n",
    "                       index_label='id',\n",
    "                       chunksize=500,\n",
    "                       dtype={\"model_name\": Text,\n",
    "                              \"model_type\": Text,\n",
    "                              \"model_output\": Text,\n",
    "                              \"model_description\": Text,\n",
    "                              \"output_image_id\": Integer})\n",
    "    \n",
    "    if table_name == \"model_versions\":\n",
    "        df.to_sql(\"model_versions\",\n",
    "                       engine,\n",
    "                       if_exists='replace',\n",
    "                       schema='public',\n",
    "                       index=True,\n",
    "                       index_label='id',\n",
    "                       chunksize=500,\n",
    "                       dtype={\"model_id\": Integer,\n",
    "                              \"model_architecture\": Text,\n",
    "                              \"input_image_id\": Integer,\n",
    "                              \"output_image_id\": Integer,\n",
    "                              \"geostore_id\": Text,\n",
    "                              \"kernel_size\": BigInteger,\n",
    "                              \"sample_size\": BigInteger,\n",
    "                              \"training_params\": JSON,\n",
    "                              \"version\": BigInteger,\n",
    "                              \"data_status\": Text,\n",
    "                              \"training_status\": Text,\n",
    "                              \"eeified\": Boolean,\n",
    "                              \"deployed\": Boolean})   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not engine.dialect.has_table(engine, \"dataset\"):\n",
    "    datasets = pd.read_csv('Database/dataset.csv', sep=';', quotechar='\\'')\n",
    "if not engine.dialect.has_table(engine, \"image\"):\n",
    "    images = pd.read_csv('Database/image.csv', sep=';', quotechar='\\'')\n",
    "if not engine.dialect.has_table(engine, \"model\"):\n",
    "    models = pd.read_csv('Database/model.csv', sep=';', quotechar='\\'')\n",
    "if not engine.dialect.has_table(engine, \"model_versions\"):\n",
    "    versions = pd.read_csv('Database/model_versions.csv', sep=';', quotechar='\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save SQL tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not engine.dialect.has_table(engine, \"dataset\"):\n",
    "    df_to_db(datasets, \"dataset\")\n",
    "if not engine.dialect.has_table(engine, \"image\"):\n",
    "    df_to_db(images, \"image\")\n",
    "if not engine.dialect.has_table(engine, \"model\"):\n",
    "    df_to_db(models, \"model\")\n",
    "if not engine.dialect.has_table(engine, \"model_versions\"):\n",
    "    df_to_db(versions, \"model_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read DataFrames from query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_query(table_name):\n",
    "    queries = {\n",
    "        \"dataset\": \"SELECT * FROM dataset\",\n",
    "        \"image\": \"SELECT * FROM image\",\n",
    "        \"model\": \"SELECT * FROM model\",\n",
    "        \"model_versions\": \"SELECT * FROM model_versions\",\n",
    "    } \n",
    "    \n",
    "    try:\n",
    "        if table_name in queries.keys():\n",
    "            df = pd.read_sql(queries.get(table_name), con=engine).drop(columns='id')\n",
    "            \n",
    "        return df\n",
    "    except:\n",
    "        print(\"Table doesn't exist in database!\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = df_from_query('dataset')\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df_from_query('image')\n",
    "images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = df_from_query('model')\n",
    "models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = df_from_query('model_versions')\n",
    "versions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_csv(df, table_name):\n",
    "    table_paths = {\n",
    "        \"dataset\": 'Database/dataset.csv',\n",
    "        \"image\": 'Database/image.csv',\n",
    "        \"model\": 'Database/model.csv',\n",
    "        \"model_versions\": 'Database/model_versions.csv',\n",
    "    } \n",
    "    \n",
    "    try:\n",
    "        if table_name in table_paths.keys():\n",
    "            df.to_csv(table_paths.get(table_name),sep=';', quotechar='\\'',index=True, index_label='id')\n",
    "    except:\n",
    "        print(\"Incorrect table name!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_csv(datasets, 'dataset')\n",
    "df_to_csv(images, 'image')\n",
    "df_to_csv(models, 'model')\n",
    "df_to_csv(versions, 'model_versions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Skydipper datasets for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slugs_list = [\"Sentinel-2-Top-of-Atmosphere-Reflectance\",\n",
    "              \"Landsat-7-Surface-Reflectance\",\n",
    "              \"Landsat-8-Surface-Reflectance\",\n",
    "              \"USDA-NASS-Cropland-Data-Layers\",\n",
    "              \"USGS-National-Land-Cover-Database\",\n",
    "              \"Lake-Water-Quality-100m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Skydipper.Collection(search=' '.join(slugs_list), object_type=['dataset'], app=['skydipper'], limit=10)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earth Engine ImageCollection attributes\n",
    "\n",
    "We define the different attributes that we will need for each Earth Engine ImageCollection all through the notebook. \n",
    "\n",
    "We include them in the `ee_collection_specifics.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ee_collection_specifics.py\n",
    "\n",
    "\"\"\"\n",
    "Information on Earth Engine collections stored here (e.g. bands, collection ids, etc.)\n",
    "\"\"\"\n",
    "\n",
    "import ee\n",
    "\n",
    "def ee_collections(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine image collection names\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': 'COPERNICUS/S2',\n",
    "        'Landsat-7-Surface-Reflectance': 'LANDSAT/LE07/C01/T1_SR',\n",
    "        'Landsat-8-Surface-Reflectance': 'LANDSAT/LC08/C01/T1_SR',\n",
    "        'USDA-NASS-Cropland-Data-Layers': 'USDA/NASS/CDL',\n",
    "        'USGS-National-Land-Cover-Database': 'USGS/NLCD',\n",
    "        'Lake-Water-Quality-100m': 'projects/vizzuality/skydipper-water-quality/LWQ-100m'\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine band names\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': ['B1','B2','B3','B4','B5','B6','B7','B8A','B8','B11','B12','ndvi','ndwi'],\n",
    "        'Landsat-7-Surface-Reflectance': ['B1','B2','B3','B4','B5','B6','B7','ndvi','ndwi'],\n",
    "        'Landsat-8-Surface-Reflectance': ['B1','B2','B3','B4','B5','B6','B7','B10','B11','ndvi','ndwi'],\n",
    "        'USDA-NASS-Cropland-Data-Layers': ['landcover', 'cropland', 'land', 'water', 'urban'],\n",
    "        'USGS-National-Land-Cover-Database': ['impervious'],\n",
    "        'Lake-Water-Quality-100m': ['turbidity_blended_mean']\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands_rgb(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine rgb band names\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': ['B4','B3','B2'],\n",
    "        'Landsat-7-Surface-Reflectance': ['B3','B2','B1'],\n",
    "        'Landsat-8-Surface-Reflectance': ['B4', 'B3', 'B2'],\n",
    "        'USDA-NASS-Cropland-Data-Layers': ['landcover'],\n",
    "        'USGS-National-Land-Cover-Database': ['impervious'],\n",
    "        'Lake-Water-Quality-100m': ['turbidity_blended_mean']\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands_normThreshold(collection):\n",
    "    \"\"\"\n",
    "    Normalization threshold percentage\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': {'B1': 75,'B2': 75,'B3': 75,'B4': 75,'B5': 80,'B6': 80,'B7': 80,'B8A': 80,'B8': 80,'B11': 100,'B12': 100},\n",
    "        'Landsat-7-Surface-Reflectance': {'B1': 95,'B2': 95,'B3': 95,'B4': 100,'B5': 100,'B6': 100,'B7': 100},\n",
    "        'Landsat-8-Surface-Reflectance': {'B1': 90,'B2': 95,'B3': 95,'B4': 95,'B5': 100,'B6': 100,'B7': 100,'B10': 100,'B11': 100},\n",
    "        'USDA-NASS-Cropland-Data-Layers': {'landcover': 100, 'cropland': 100, 'land': 100, 'water': 100, 'urban': 100},\n",
    "        'USGS-National-Land-Cover-Database': {'impervious': 100},\n",
    "        'Lake-Water-Quality-100m': {'turbidity_blended_mean': 100}\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def normalize(collection):\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': True,\n",
    "        'Landsat-7-Surface-Reflectance': True,\n",
    "        'Landsat-8-Surface-Reflectance': True,\n",
    "        'USDA-NASS-Cropland-Data-Layers': False,\n",
    "        'USGS-National-Land-Cover-Database': False,\n",
    "        'Lake-Water-Quality-100m': False\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def vizz_params_rgb(collection):\n",
    "    \"\"\"\n",
    "    Visualization parameters\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': {'min':0,'max':3000, 'bands':['B4','B3','B2']},\n",
    "        'Landsat-7-Surface-Reflectance': {'min':0,'max':3000, 'gamma':1.4, 'bands':['B3','B2','B1']},\n",
    "        'Landsat-8-Surface-Reflectance': {'min':0,'max':3000, 'gamma':1.4, 'bands':['B4','B3','B2']},\n",
    "        'USDA-NASS-Cropland-Data-Layers': {'min':0,'max':3, 'bands':['landcover']},\n",
    "        'USGS-National-Land-Cover-Database': {'min': 0, 'max': 1, 'bands':['impervious']},\n",
    "        'Lake-Water-Quality-100m': {'min': 0, 'max': 1, 'bands':['turbidity_blended_mean']}\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def vizz_params(collection):\n",
    "    \"\"\"\n",
    "    Visualization parameters\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': [{'min':0,'max':1, 'bands':['B4','B3','B2']}, \n",
    "                      {'min':0,'max':1, 'bands':['B1']},\n",
    "                      {'min':0,'max':1, 'bands':['B5']},\n",
    "                      {'min':0,'max':1, 'bands':['B6']},\n",
    "                      {'min':0,'max':1, 'bands':['B7']},\n",
    "                      {'min':0,'max':1, 'bands':['B8A']},\n",
    "                      {'min':0,'max':1, 'bands':['B8']},\n",
    "                      {'min':0,'max':1, 'bands':['B11']},\n",
    "                      {'min':0,'max':1, 'bands':['B12']},\n",
    "                      {'min':0,'max':1, 'gamma':1.4, 'bands':['ndvi']},\n",
    "                      {'min':0,'max':1, 'gamma':1.4, 'bands':['ndwi']}],\n",
    "        'Landsat-7-Surface-Reflectance': [{'min':0,'max':1, 'gamma':1.4, 'bands':['B3','B2','B1']}, \n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B4']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B5']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B7']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B6']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['ndvi']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['ndwi']}],\n",
    "        'Landsat-8-Surface-Reflectance': [{'min':0,'max':1, 'gamma':1.4, 'bands':['B4','B3','B2']}, \n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B1']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B5']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B6']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B7']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B10']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B11']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['ndvi']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['ndwi']}],\n",
    "        'USDA-NASS-Cropland-Data-Layers': [{'min':0,'max':3, 'bands':['landcover']},\n",
    "                               {'min':0,'max':1, 'bands':['cropland']},\n",
    "                               {'min':0,'max':1, 'bands':['land']},\n",
    "                               {'min':0,'max':1, 'bands':['water']},\n",
    "                               {'min':0,'max':1, 'bands':['urban']}],\n",
    "        'USGS-National-Land-Cover-Database': [{'min': 0, 'max': 1, 'bands':['impervious']}],\n",
    "        'Lake-Water-Quality-100m': [{'min': 0, 'max': 1, 'bands':['turbidity_blended_mean']}],\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "## ------------------------- Filter datasets ------------------------- ##\n",
    "## Lansat 7 Cloud Free Composite\n",
    "def CloudMaskL7sr(image):\n",
    "    qa = image.select('pixel_qa')\n",
    "    #If the cloud bit (5) is set and the cloud confidence (7) is high\n",
    "    #or the cloud shadow bit is set (3), then it's a bad pixel.\n",
    "    cloud = qa.bitwiseAnd(1 << 5).And(qa.bitwiseAnd(1 << 7)).Or(qa.bitwiseAnd(1 << 3))\n",
    "    #Remove edge pixels that don't occur in all bands\n",
    "    mask2 = image.mask().reduce(ee.Reducer.min())\n",
    "    return image.updateMask(cloud.Not()).updateMask(mask2)\n",
    "\n",
    "def CloudFreeCompositeL7(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate).map(CloudMaskL7sr)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    ## normDiff bands\n",
    "    normDiff_band_names = ['ndvi', 'ndwi']\n",
    "    for nB, normDiff_band in enumerate([['B4','B3'], ['B4','B2']]):\n",
    "        image_nd = composite.normalizedDifference(normDiff_band).rename(normDiff_band_names[nB])\n",
    "        composite = ee.Image.cat([composite, image_nd])\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Lansat 8 Cloud Free Composite\n",
    "def CloudMaskL8sr(image):\n",
    "    opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "    thermalBands = ['B10', 'B11']\n",
    "\n",
    "    cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
    "    cloudsBitMask = ee.Number(2).pow(5).int()\n",
    "    qa = image.select('pixel_qa')\n",
    "    mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
    "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
    "    mask2 = image.mask().reduce('min')\n",
    "    mask3 = image.select(opticalBands).gt(0).And(\n",
    "            image.select(opticalBands).lt(10000)).reduce('min')\n",
    "    mask = mask1.And(mask2).And(mask3)\n",
    "    \n",
    "    return image.updateMask(mask)\n",
    "\n",
    "def CloudFreeCompositeL8(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate).map(CloudMaskL8sr)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    ## normDiff bands\n",
    "    normDiff_band_names = ['ndvi', 'ndwi']\n",
    "    for nB, normDiff_band in enumerate([['B5','B4'], ['B5','B3']]):\n",
    "        image_nd = composite.normalizedDifference(normDiff_band).rename(normDiff_band_names[nB])\n",
    "        composite = ee.Image.cat([composite, image_nd])\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Sentinel 2 Cloud Free Composite\n",
    "def CloudMaskS2(image):\n",
    "    \"\"\"\n",
    "    European Space Agency (ESA) clouds from 'QA60', i.e. Quality Assessment band at 60m\n",
    "    parsed by Nick Clinton\n",
    "    \"\"\"\n",
    "    AerosolsBands = ['B1']\n",
    "    VIBands = ['B2', 'B3', 'B4']\n",
    "    RedBands = ['B5', 'B6', 'B7', 'B8A']\n",
    "    NIRBands = ['B8']\n",
    "    SWIRBands = ['B11', 'B12']\n",
    "\n",
    "    qa = image.select('QA60')\n",
    "\n",
    "    # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "    cloudBitMask = int(2**10)\n",
    "    cirrusBitMask = int(2**11)\n",
    "\n",
    "    # Both flags set to zero indicates clear conditions.\n",
    "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(\\\n",
    "            qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "\n",
    "    return image.updateMask(mask)\n",
    "\n",
    "def CloudFreeCompositeS2(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('COPERNICUS/S2')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\\\n",
    "            .map(CloudMaskS2)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    ## normDiff bands\n",
    "    normDiff_band_names = ['ndvi', 'ndwi']\n",
    "    for nB, normDiff_band in enumerate([['B8','B4'], ['B8','B3']]):\n",
    "        image_nd = composite.normalizedDifference(normDiff_band).rename(normDiff_band_names[nB])\n",
    "        composite = ee.Image.cat([composite, image_nd])\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Cropland Data Layers\n",
    "def CroplandData(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('USDA/NASS/CDL')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Change classes\n",
    "    land = ['65', '131', '141', '142', '143', '152', '176', '87', '190', '195']\n",
    "    water = ['83', '92', '111']\n",
    "    urban = ['82', '121', '122', '123', '124']\n",
    "    \n",
    "    classes = []\n",
    "    for n, i in enumerate([land,water,urban]):\n",
    "        a = ''\n",
    "        for m, j in enumerate(i):\n",
    "            if m < len(i)-1:\n",
    "                a = a + 'crop == '+ j + ' || '\n",
    "            else: \n",
    "                a = a + 'crop == '+ j\n",
    "        classes.append('('+a+') * '+str(n+1))\n",
    "    classes = ' + '.join(classes)\n",
    "    \n",
    "    image = image.expression(classes, {'crop': image.select(['cropland'])})\n",
    "    \n",
    "    image =image.rename('landcover')\n",
    "    \n",
    "    # Split image into 1 band per class\n",
    "    names = ['cropland', 'land', 'water', 'urban']\n",
    "    mask = image\n",
    "    for i, name in enumerate(names):\n",
    "        image = ee.Image.cat([image, mask.eq(i).rename(name)])\n",
    "     \n",
    "    return image\n",
    "\n",
    "## National Land Cover Database\n",
    "def ImperviousData(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('USGS/NLCD')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Select impervious band\n",
    "    image = image.select('impervious')\n",
    "    \n",
    "    ## Normalize to 1\n",
    "    image = image.divide(100).float()\n",
    "    \n",
    "    return image\n",
    "\n",
    "def WaterQuality(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('projects/vizzuality/skydipper-water-quality/LWQ-100m')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Select impervious band\n",
    "    image = image.select('turbidity_blended_mean')\n",
    "    \n",
    "    return image\n",
    "\n",
    "## ------------------------------------------------------------------- ##\n",
    "\n",
    "def Composite(collection):\n",
    "    dic = {\n",
    "        'Sentinel-2-Top-of-Atmosphere-Reflectance': CloudFreeCompositeS2,\n",
    "        'Landsat-7-Surface-Reflectance': CloudFreeCompositeL7,\n",
    "        'Landsat-8-Surface-Reflectance': CloudFreeCompositeL8,\n",
    "        'USDA-NASS-Cropland-Data-Layers': CroplandData,\n",
    "        'USGS-National-Land-Cover-Database': ImperviousData,\n",
    "        'Lake-Water-Quality-100m': WaterQuality\n",
    "    }\n",
    "    \n",
    "    return dic[collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee_collection_specifics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate `dataset` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table\n",
    "datasets = df_from_query('dataset')\n",
    "\n",
    "for collection in slugs_list:\n",
    "\n",
    "    ds = Skydipper.Dataset(id_hash=collection)\n",
    "    name = ds.attributes.get('name')\n",
    "    provider = ds.attributes.get('provider')\n",
    "\n",
    "    bands = [str(ee_collection_specifics.ee_bands(collection))]\n",
    "    rgb_bands = [str(ee_collection_specifics.ee_bands_rgb(collection))]\n",
    "\n",
    "\n",
    "    dictionary = dict(zip(list(datasets.keys()), [collection, name, bands, rgb_bands, provider]))\n",
    "    \n",
    "    if (datasets['slug'] == collection).any():\n",
    "        datasets = datasets\n",
    "    else:\n",
    "        datasets = datasets.append(pd.DataFrame(dictionary), ignore_index = True)\n",
    "        \n",
    "        # Save table\n",
    "        df_to_csv(datasets, \"dataset\")\n",
    "        df_to_db(datasets, \"dataset\")\n",
    "    \n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Composite images\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = 'Sentinel-2-Top-of-Atmosphere-Reflectance'\n",
    "output_dataset = 'Lake-Water-Quality-100m'\n",
    "init_date = '2019-01-21'\n",
    "end_date = '2019-01-31'\n",
    "collections = [input_dataset, output_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display composite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "composites = []\n",
    "Map = folium.Map(location=[39.31, 0.302])\n",
    "for n, collection in enumerate(collections):\n",
    "    composites.append(ee_collection_specifics.Composite(collection)(init_date, end_date))\n",
    "    \n",
    "    mapid = composites[n].getMapId(ee_collection_specifics.vizz_params_rgb(collection))\n",
    "    tiles_url = EE_TILES.format(**mapid)\n",
    "    folium.TileLayer(\n",
    "    tiles=tiles_url,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name=str(ee_collection_specifics.ee_bands_rgb(collection))).add_to(Map)\n",
    "    \n",
    "Map.add_child(folium.LayerControl())\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Geostore\n",
    "\n",
    "We select the areas from which we will export the training data.\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygons_to_geoStoreMultiPoligon(Polygons):\n",
    "    Polygons = list(filter(None, Polygons))\n",
    "    MultiPoligon = {}\n",
    "    properties = [\"training\", \"validation\", \"test\"]\n",
    "    features = []\n",
    "    for n, polygons in enumerate(Polygons):\n",
    "        multipoligon = []\n",
    "        for polygon in polygons.get('features'):\n",
    "            multipoligon.append(polygon.get('geometry').get('coordinates'))\n",
    "            \n",
    "        features.append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\"name\": properties[n]},\n",
    "            \"geometry\": {\n",
    "                \"type\": \"MultiPolygon\",\n",
    "                \"coordinates\":  multipoligon\n",
    "            }\n",
    "        }\n",
    "        ) \n",
    "        \n",
    "    MultiPoligon = {\n",
    "        \"geojson\": {\n",
    "            \"type\": \"FeatureCollection\", \n",
    "            \"features\": features\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return MultiPoligon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if collections[1] == 'USGS-National-Land-Cover-Database':\n",
    "    trainPolygons = {\"type\":\"FeatureCollection\",\"features\":[{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-123.22265625000001,45.213003555993964],[-122.03613281249999,45.213003555993964],[-122.03613281249999,46.164614496897094],[-123.22265625000001,46.164614496897094],[-123.22265625000001,45.213003555993964]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-122.1240234375,38.16911413556086],[-120.76171875,38.16911413556086],[-120.76171875,39.13006024213511],[-122.1240234375,39.13006024213511],[-122.1240234375,38.16911413556086]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-119.70703125,34.77771580360469],[-118.3447265625,34.77771580360469],[-118.3447265625,35.92464453144099],[-119.70703125,35.92464453144099],[-119.70703125,34.77771580360469]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-115.97167968750001,35.496456056584165],[-114.521484375,35.496456056584165],[-114.521484375,36.73888412439431],[-115.97167968750001,36.73888412439431],[-115.97167968750001,35.496456056584165]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-118.21289062499999,33.797408767572485],[-116.23535156249999,33.797408767572485],[-116.23535156249999,34.379712580462204],[-118.21289062499999,34.379712580462204],[-118.21289062499999,33.797408767572485]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-112.6318359375,33.02708758002874],[-111.4013671875,33.02708758002874],[-111.4013671875,34.016241889667015],[-112.6318359375,34.016241889667015],[-112.6318359375,33.02708758002874]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-105.6005859375,39.40224434029275],[-104.5458984375,39.40224434029275],[-104.5458984375,40.44694705960048],[-105.6005859375,40.44694705960048],[-105.6005859375,39.40224434029275]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-112.67578124999999,40.27952566881291],[-111.4453125,40.27952566881291],[-111.4453125,41.21172151054787],[-112.67578124999999,41.21172151054787],[-112.67578124999999,40.27952566881291]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-97.734375,32.21280106801518],[-95.9326171875,32.21280106801518],[-95.9326171875,33.32134852669881],[-97.734375,33.32134852669881],[-97.734375,32.21280106801518]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-99.36035156249999,29.036960648558267],[-97.822265625,29.036960648558267],[-97.822265625,30.031055426540206],[-99.36035156249999,30.031055426540206],[-99.36035156249999,29.036960648558267]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-95.185546875,38.61687046392973],[-93.9990234375,38.61687046392973],[-93.9990234375,39.639537564366684],[-95.185546875,39.639537564366684],[-95.185546875,38.61687046392973]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-91.2744140625,38.30718056188316],[-89.6484375,38.30718056188316],[-89.6484375,39.16414104768742],[-91.2744140625,39.16414104768742],[-91.2744140625,38.30718056188316]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-88.330078125,41.343824581185686],[-86.8798828125,41.343824581185686],[-86.8798828125,42.391008609205045],[-88.330078125,42.391008609205045],[-88.330078125,41.343824581185686]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-93.91113281249999,44.49650533109348],[-92.5048828125,44.49650533109348],[-92.5048828125,45.583289756006316],[-93.91113281249999,45.583289756006316],[-93.91113281249999,44.49650533109348]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-81.38671875,34.813803317113155],[-80.2880859375,34.813803317113155],[-80.2880859375,35.782170703266075],[-81.38671875,35.782170703266075],[-81.38671875,34.813803317113155]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-85.0341796875,33.17434155100208],[-83.7158203125,33.17434155100208],[-83.7158203125,34.27083595165],[-85.0341796875,34.27083595165],[-85.0341796875,33.17434155100208]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-87.2314453125,35.60371874069731],[-86.17675781249999,35.60371874069731],[-86.17675781249999,36.63316209558658],[-87.2314453125,36.63316209558658],[-87.2314453125,35.60371874069731]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-87.14355468749999,32.91648534731439],[-86.2646484375,32.91648534731439],[-86.2646484375,33.97980872872457],[-87.14355468749999,33.97980872872457],[-87.14355468749999,32.91648534731439]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-81.9140625,27.566721430409707],[-81.03515625,27.566721430409707],[-81.03515625,28.844673680771795],[-81.9140625,28.844673680771795],[-81.9140625,27.566721430409707]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-84.7705078125,38.92522904714054],[-83.75976562499999,38.92522904714054],[-83.75976562499999,40.17887331434696],[-84.7705078125,40.17887331434696],[-84.7705078125,38.92522904714054]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-80.947265625,40.27952566881291],[-79.98046875,40.27952566881291],[-79.98046875,41.178653972331674],[-80.947265625,41.178653972331674],[-80.947265625,40.27952566881291]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-75.2783203125,40.613952441166596],[-73.8720703125,40.613952441166596],[-73.8720703125,41.21172151054787],[-75.2783203125,41.21172151054787],[-75.2783203125,40.613952441166596]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-78.0908203125,38.44498466889473],[-76.728515625,38.44498466889473],[-76.728515625,39.33429742980725],[-78.0908203125,39.33429742980725],[-78.0908203125,38.44498466889473]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-112.6318359375,46.164614496897094],[-111.4453125,46.164614496897094],[-111.4453125,46.86019101567027],[-112.6318359375,46.86019101567027],[-112.6318359375,46.164614496897094]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-117.1142578125,43.229195113965005],[-115.57617187499999,43.229195113965005],[-115.57617187499999,44.08758502824516],[-117.1142578125,44.08758502824516],[-117.1142578125,43.229195113965005]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-96.328125,35.746512259918504],[-95.2734375,35.746512259918504],[-95.2734375,36.4566360115962],[-96.328125,36.4566360115962],[-96.328125,35.746512259918504]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-98.173828125,35.02999636902566],[-96.9873046875,35.02999636902566],[-96.9873046875,35.817813158696616],[-98.173828125,35.817813158696616],[-98.173828125,35.02999636902566]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-92.6806640625,34.379712580462204],[-91.7578125,34.379712580462204],[-91.7578125,35.10193405724606],[-92.6806640625,35.10193405724606],[-92.6806640625,34.379712580462204]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-90.7470703125,34.63320791137959],[-89.3408203125,34.63320791137959],[-89.3408203125,35.71083783530009],[-90.7470703125,35.71083783530009],[-90.7470703125,34.63320791137959]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-107.314453125,34.74161249883172],[-106.12792968749999,34.74161249883172],[-106.12792968749999,35.60371874069731],[-107.314453125,35.60371874069731],[-107.314453125,34.74161249883172]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-94.3505859375,41.1455697310095],[-92.94433593749999,41.1455697310095],[-92.94433593749999,42.19596877629178],[-94.3505859375,42.19596877629178],[-94.3505859375,41.1455697310095]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-85.869140625,40.68063802521456],[-84.5947265625,40.68063802521456],[-84.5947265625,41.64007838467894],[-85.869140625,41.64007838467894],[-85.869140625,40.68063802521456]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-87.099609375,39.30029918615029],[-85.6494140625,39.30029918615029],[-85.6494140625,40.245991504199026],[-87.099609375,40.245991504199026],[-87.099609375,39.30029918615029]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-117.7734375,47.30903424774781],[-116.103515625,47.30903424774781],[-116.103515625,48.1367666796927],[-117.7734375,48.1367666796927],[-117.7734375,47.30903424774781]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-97.91015624999999,37.3002752813443],[-96.8115234375,37.3002752813443],[-96.8115234375,38.09998264736481],[-97.91015624999999,38.09998264736481],[-97.91015624999999,37.3002752813443]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-94.06494140625,32.25926542645933],[-93.4716796875,32.25926542645933],[-93.4716796875,32.7872745269555],[-94.06494140625,32.7872745269555],[-94.06494140625,32.25926542645933]]]}}]}  \n",
    "    validPolygons = {\"type\":\"FeatureCollection\",\"features\":[{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-95.888671875,29.38217507514529],[-95.06469726562499,29.38217507514529],[-95.06469726562499,30.12612436422458],[-95.888671875,30.12612436422458],[-95.888671875,29.38217507514529]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-83.84765625,42.374778361114195],[-82.94677734375,42.374778361114195],[-82.94677734375,42.78733853171998],[-83.84765625,42.78733853171998],[-83.84765625,42.374778361114195]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-96.88568115234375,40.69521661351714],[-95.77606201171875,40.69521661351714],[-95.77606201171875,41.393294288784865],[-96.88568115234375,41.393294288784865],[-96.88568115234375,40.69521661351714]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-105.05126953124999,38.57393751557591],[-104.490966796875,38.57393751557591],[-104.490966796875,39.0831721934762],[-105.05126953124999,39.0831721934762],[-105.05126953124999,38.57393751557591]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-122.62390136718749,46.95776134668866],[-121.84936523437499,46.95776134668866],[-121.84936523437499,48.04136507445029],[-122.62390136718749,48.04136507445029],[-122.62390136718749,46.95776134668866]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-120.157470703125,36.465471886798134],[-119.24560546875001,36.465471886798134],[-119.24560546875001,37.03763967977139],[-120.157470703125,37.03763967977139],[-120.157470703125,36.465471886798134]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-120.02563476562501,39.33854604847979],[-119.55871582031251,39.33854604847979],[-119.55871582031251,39.7240885773337],[-120.02563476562501,39.7240885773337],[-120.02563476562501,39.33854604847979]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-86.30859375,37.61423141542417],[-84.9462890625,37.61423141542417],[-84.9462890625,38.65119833229951],[-86.30859375,38.65119833229951],[-86.30859375,37.61423141542417]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-78.31054687499999,36.914764288955936],[-76.86035156249999,36.914764288955936],[-76.86035156249999,38.03078569382294],[-78.31054687499999,38.03078569382294],[-78.31054687499999,36.914764288955936]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-102.87597656249999,31.541089879585808],[-101.4697265625,31.541089879585808],[-101.4697265625,32.24997445586331],[-102.87597656249999,32.24997445586331],[-102.87597656249999,31.541089879585808]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-83.5400390625,39.50404070558415],[-82.177734375,39.50404070558415],[-82.177734375,40.54720023441049],[-83.5400390625,40.54720023441049],[-83.5400390625,39.50404070558415]]]}}]}\n",
    "    testPolygons = None\n",
    "    \n",
    "    geostore = polygons_to_geoStoreMultiPoligon([trainPolygons, validPolygons, testPolygons])\n",
    "    \n",
    "if collections[1] == 'USDA-NASS-Cropland-Data-Layers':\n",
    "    trainPolygons = {\"type\": \"FeatureCollection\", \"features\": [{\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-122.882080078125, 40.50126945841645], [-122.1240234375, 40.50126945841645], [-122.1240234375, 41.008920735004885], [-122.882080078125, 41.008920735004885], [-122.882080078125, 40.50126945841645]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-122.2283935546875, 39.00637903337455], [-121.607666015625, 39.00637903337455], [-121.607666015625, 39.46588451142044], [-122.2283935546875, 39.46588451142044], [-122.2283935546875, 39.00637903337455]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.355224609375, 38.77978137804918], [-119.608154296875, 38.77978137804918], [-119.608154296875, 39.342794408952365], [-120.355224609375, 39.342794408952365], [-120.355224609375, 38.77978137804918]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-121.90979003906249, 37.70555348721583], [-120.9814453125, 37.70555348721583], [-120.9814453125, 38.39764411353178], [-121.90979003906249, 38.39764411353178], [-121.90979003906249, 37.70555348721583]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.03662109374999, 37.45741810262938], [-119.1851806640625, 37.45741810262938], [-119.1851806640625, 38.08268954483802], [-120.03662109374999, 38.08268954483802], [-120.03662109374999, 37.45741810262938]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.03662109374999, 37.45741810262938], [-119.1851806640625, 37.45741810262938], [-119.1851806640625, 38.08268954483802], [-120.03662109374999, 38.08268954483802], [-120.03662109374999, 37.45741810262938]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.03662109374999, 37.45741810262938], [-119.1851806640625, 37.45741810262938], [-119.1851806640625, 38.08268954483802], [-120.03662109374999, 38.08268954483802], [-120.03662109374999, 37.45741810262938]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-112.554931640625, 33.0178760185549], [-111.588134765625, 33.0178760185549], [-111.588134765625, 33.78827853625996], [-112.554931640625, 33.78827853625996], [-112.554931640625, 33.0178760185549]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-112.87353515625, 40.51379915504413], [-111.829833984375, 40.51379915504413], [-111.829833984375, 41.28606238749825], [-112.87353515625, 41.28606238749825], [-112.87353515625, 40.51379915504413]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-108.19335937499999, 39.095962936305476], [-107.1826171875, 39.095962936305476], [-107.1826171875, 39.85915479295669], [-108.19335937499999, 39.85915479295669], [-108.19335937499999, 39.095962936305476]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-124.25537109375, 30.86451022625836], [-124.25537109375, 30.86451022625836], [-124.25537109375, 30.86451022625836], [-124.25537109375, 30.86451022625836]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-106.875, 37.142803443716836], [-105.49072265625, 37.142803443716836], [-105.49072265625, 38.18638677411551], [-106.875, 38.18638677411551], [-106.875, 37.142803443716836]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-117.31201171875001, 43.27720532212024], [-116.01562499999999, 43.27720532212024], [-116.01562499999999, 44.134913443750726], [-117.31201171875001, 44.134913443750726], [-117.31201171875001, 43.27720532212024]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-115.7080078125, 44.69989765840318], [-114.7412109375, 44.69989765840318], [-114.7412109375, 45.36758436884978], [-115.7080078125, 45.36758436884978], [-115.7080078125, 44.69989765840318]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-120.65185546875, 47.517200697839414], [-119.33349609375, 47.517200697839414], [-119.33349609375, 48.32703913063476], [-120.65185546875, 48.32703913063476], [-120.65185546875, 47.517200697839414]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-119.83886718750001, 45.69083283645816], [-118.38867187500001, 45.69083283645816], [-118.38867187500001, 46.694667307773116], [-119.83886718750001, 46.694667307773116], [-119.83886718750001, 45.69083283645816]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-107.09472656249999, 47.45780853075031], [-105.84228515625, 47.45780853075031], [-105.84228515625, 48.31242790407178], [-107.09472656249999, 48.31242790407178], [-107.09472656249999, 47.45780853075031]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-101.57958984375, 46.93526088057719], [-100.107421875, 46.93526088057719], [-100.107421875, 47.945786463687185], [-101.57958984375, 47.945786463687185], [-101.57958984375, 46.93526088057719]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-101.162109375, 44.32384807250689], [-99.7119140625, 44.32384807250689], [-99.7119140625, 45.22848059584359], [-101.162109375, 45.22848059584359], [-101.162109375, 44.32384807250689]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-100.5908203125, 41.261291493919884], [-99.25048828124999, 41.261291493919884], [-99.25048828124999, 42.114523952464246], [-100.5908203125, 42.114523952464246], [-100.5908203125, 41.261291493919884]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-97.9541015625, 37.142803443716836], [-96.65771484375, 37.142803443716836], [-96.65771484375, 38.13455657705411], [-97.9541015625, 38.13455657705411], [-97.9541015625, 37.142803443716836]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-112.78564453124999, 32.91648534731439], [-111.357421875, 32.91648534731439], [-111.357421875, 33.925129700072], [-112.78564453124999, 33.925129700072], [-112.78564453124999, 32.91648534731439]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-106.435546875, 35.15584570226544], [-105.22705078125, 35.15584570226544], [-105.22705078125, 36.13787471840729], [-106.435546875, 36.13787471840729], [-106.435546875, 35.15584570226544]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-97.3828125, 32.45415593941475], [-96.2841796875, 32.45415593941475], [-96.2841796875, 33.22949814144951], [-97.3828125, 33.22949814144951], [-97.3828125, 32.45415593941475]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-97.97607421875, 35.04798673426734], [-97.00927734375, 35.04798673426734], [-97.00927734375, 35.764343479667176], [-97.97607421875, 35.764343479667176], [-97.97607421875, 35.04798673426734]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-97.97607421875, 35.04798673426734], [-97.00927734375, 35.04798673426734], [-97.00927734375, 35.764343479667176], [-97.97607421875, 35.764343479667176], [-97.97607421875, 35.04798673426734]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-95.4052734375, 47.62097541515849], [-94.24072265625, 47.62097541515849], [-94.24072265625, 48.28319289548349], [-95.4052734375, 48.28319289548349], [-95.4052734375, 47.62097541515849]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-94.19677734375, 41.27780646738183], [-93.09814453125, 41.27780646738183], [-93.09814453125, 42.13082130188811], [-94.19677734375, 42.13082130188811], [-94.19677734375, 41.27780646738183]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-93.71337890625, 37.75334401310656], [-92.6806640625, 37.75334401310656], [-92.6806640625, 38.51378825951165], [-93.71337890625, 38.51378825951165], [-93.71337890625, 37.75334401310656]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-90.63720703125, 34.615126683462194], [-89.47265625, 34.615126683462194], [-89.47265625, 35.69299463209881], [-90.63720703125, 35.69299463209881], [-90.63720703125, 34.615126683462194]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-93.05419921875, 30.44867367928756], [-91.77978515625, 30.44867367928756], [-91.77978515625, 31.57853542647338], [-93.05419921875, 31.57853542647338], [-93.05419921875, 30.44867367928756]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-90.02197265625, 44.276671273775186], [-88.59374999999999, 44.276671273775186], [-88.59374999999999, 44.98034238084973], [-90.02197265625, 44.98034238084973], [-90.02197265625, 44.276671273775186]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-90.63720703125, 38.41055825094609], [-89.49462890625, 38.41055825094609], [-89.49462890625, 39.18117526158749], [-90.63720703125, 39.18117526158749], [-90.63720703125, 38.41055825094609]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-87.56103515625, 35.62158189955968], [-86.28662109375, 35.62158189955968], [-86.28662109375, 36.4566360115962], [-87.56103515625, 36.4566360115962], [-87.56103515625, 35.62158189955968]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-90.63720703125, 31.93351676190369], [-89.49462890625, 31.93351676190369], [-89.49462890625, 32.731840896865684], [-90.63720703125, 32.731840896865684], [-90.63720703125, 31.93351676190369]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-69.54345703125, 44.68427737181225], [-68.5107421875, 44.68427737181225], [-68.5107421875, 45.336701909968134], [-69.54345703125, 45.336701909968134], [-69.54345703125, 44.68427737181225]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-73.212890625, 41.49212083968776], [-72.35595703125, 41.49212083968776], [-72.35595703125, 42.032974332441405], [-73.212890625, 42.032974332441405], [-73.212890625, 41.49212083968776]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-77.93701171875, 38.70265930723801], [-76.97021484375, 38.70265930723801], [-76.97021484375, 39.26628442213066], [-77.93701171875, 39.26628442213066], [-77.93701171875, 38.70265930723801]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-79.25537109375, 35.44277092585766], [-78.15673828125, 35.44277092585766], [-78.15673828125, 36.13787471840729], [-79.25537109375, 36.13787471840729], [-79.25537109375, 35.44277092585766]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-81.4306640625, 33.55970664841198], [-80.44189453125, 33.55970664841198], [-80.44189453125, 34.288991865037524], [-81.4306640625, 34.288991865037524], [-81.4306640625, 33.55970664841198]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-84.90234375, 33.394759218577995], [-83.91357421875, 33.394759218577995], [-83.91357421875, 34.19817309627726], [-84.90234375, 34.19817309627726], [-84.90234375, 33.394759218577995]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-82.28759765625, 28.246327971048842], [-81.2548828125, 28.246327971048842], [-81.2548828125, 29.209713225868185], [-82.28759765625, 29.209713225868185], [-82.28759765625, 28.246327971048842]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-109.88525390624999, 42.65012181368022], [-108.56689453125, 42.65012181368022], [-108.56689453125, 43.50075243569041], [-109.88525390624999, 43.50075243569041], [-109.88525390624999, 42.65012181368022]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-117.61962890624999, 39.04478604850143], [-116.65283203124999, 39.04478604850143], [-116.65283203124999, 39.740986355883564], [-117.61962890624999, 39.740986355883564], [-117.61962890624999, 39.04478604850143]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-102.67822265625, 31.42866311735861], [-101.71142578125, 31.42866311735861], [-101.71142578125, 32.26855544621476], [-102.67822265625, 32.26855544621476], [-102.67822265625, 31.42866311735861]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-119.47631835937499, 36.03133177633187], [-118.58642578124999, 36.03133177633187], [-118.58642578124999, 36.55377524336089], [-119.47631835937499, 36.55377524336089], [-119.47631835937499, 36.03133177633187]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-116.224365234375, 33.091541548655215], [-115.56518554687499, 33.091541548655215], [-115.56518554687499, 33.568861182555565], [-116.224365234375, 33.568861182555565], [-116.224365234375, 33.091541548655215]]]}}]}\n",
    "    validPolygons = {\"type\": \"FeatureCollection\", \"features\": [{\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-122.13208008, 41.25126946], [-121.37402344, 41.25126946], [-121.37402344, 41.75892074], [-122.13208008, 41.75892074], [-122.13208008, 41.25126946]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-121.15979004, 38.45555349], [-120.23144531, 38.45555349], [-120.23144531, 39.14764411], [-121.15979004, 39.14764411], [-121.15979004, 38.45555349]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-111.80493164, 33.76787602], [-110.83813477, 33.76787602], [-110.83813477, 34.53827854], [-111.80493164, 34.53827854], [-111.80493164, 33.76787602]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-106.125, 37.89280344], [-104.74072266, 37.89280344], [-104.74072266, 38.93638677], [-106.125, 38.93638677], [-106.125, 37.89280344]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-119.08886719, 46.44083284], [-117.63867188, 46.44083284], [-117.63867188, 47.44466731], [-119.08886719, 47.44466731], [-119.08886719, 46.44083284]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-99.84082031, 42.01129149], [-98.50048828, 42.01129149], [-98.50048828, 42.86452395], [-99.84082031, 42.86452395], [-99.84082031, 42.01129149]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-96.6328125, 33.20415594], [-95.53417969, 33.20415594], [-95.53417969, 33.97949814], [-96.6328125, 33.97949814], [-96.6328125, 33.20415594]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-93.44677734, 42.02780647], [-92.34814453, 42.02780647], [-92.34814453, 42.8808213], [-93.44677734, 42.8808213], [-93.44677734, 42.02780647]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-89.27197266, 45.02667127], [-87.84375, 45.02667127], [-87.84375, 45.73034238], [-89.27197266, 45.73034238], [-89.27197266, 45.02667127]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-68.79345703, 45.43427737], [-67.76074219, 45.43427737], [-67.76074219, 46.08670191], [-68.79345703, 46.08670191], [-68.79345703, 45.43427737]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-80.68066406, 34.30970665], [-79.69189453, 34.30970665], [-79.69189453, 35.03899187], [-80.68066406, 35.03899187], [-80.68066406, 34.30970665]]]}}, {\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-116.86962891, 39.79478605], [-115.90283203, 39.79478605], [-115.90283203, 40.49098636], [-116.86962891, 40.49098636], [-116.86962891, 39.79478605]]]}}]}\n",
    "    testPolygons = None\n",
    "    \n",
    "    geostore = polygons_to_geoStoreMultiPoligon([trainPolygons, validPolygons, testPolygons])\n",
    "    \n",
    "if collections[1] == 'Lake-Water-Quality-100m':\n",
    "    trainPolygons = {\"type\":\"FeatureCollection\",\"features\":[{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-0.406494140625,38.64476310916202],[0.27740478515625,38.64476310916202],[0.27740478515625,39.74521015328692],[-0.406494140625,39.74521015328692],[-0.406494140625,38.64476310916202]]]}},{\"type\":\"Feature\",\"properties\":{},\"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[[-1.70013427734375,35.15135442846945],[-0.703125,35.15135442846945],[-0.703125,35.94688293218141],[-1.70013427734375,35.94688293218141],[-1.70013427734375,35.15135442846945]]]}}]}\n",
    "    validPolygons = None\n",
    "    testPolygons = None\n",
    "    \n",
    "    geostore = polygons_to_geoStoreMultiPoligon([trainPolygons, validPolygons, testPolygons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFeatures = len(geostore.get('geojson').get('features'))\n",
    "\n",
    "nPolygons = {}\n",
    "for n in range(nFeatures):\n",
    "    multipoly_type = geostore.get('geojson').get('features')[n].get('properties').get('name')\n",
    "    nPolygons[multipoly_type] = len(geostore.get('geojson').get('features')[n].get('geometry').get('coordinates'))\n",
    "    \n",
    "for multipoly_type in nPolygons.keys():\n",
    "    print(f'Number of {multipoly_type} polygons:', nPolygons[multipoly_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipolygon = Skydipper.Geometry(attributes=geostore)\n",
    "multipolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipolygon.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Polygons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geojson_string(geom):\n",
    "    coords = geom.get('coordinates', None)\n",
    "    if coords and not any(isinstance(i, list) for i in coords[0]):\n",
    "        geom['coordinates'] = [coords]\n",
    "    feat_col = {\"type\": \"FeatureCollection\", \"features\": [{\"type\": \"Feature\", \"properties\": {}, \"geometry\": geom}]}\n",
    "    return json.dumps(feat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "features = multipolygon.attributes['geojson']['features']\n",
    "if len(features) > 0:\n",
    "    shapely_geometry = [shape(feature['geometry']) for feature in features]\n",
    "else:\n",
    "    shapely_geometry = None\n",
    "    \n",
    "centroid = list(shapely_geometry[0].centroid.coords)[0][::-1]\n",
    "\n",
    "bbox = multipolygon.attributes['bbox']\n",
    "bounds = [bbox[2:][::-1], bbox[:2][::-1]]       \n",
    "        \n",
    "Map = folium.Map(location=centroid, zoom_start=6)\n",
    "Map.fit_bounds(bounds)\n",
    "for params in ee_collection_specifics.vizz_params(output_dataset):\n",
    "    mapid = composites[1].getMapId(params)\n",
    "    folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name=str(params['bands']),\n",
    " ).add_to(Map)\n",
    " \n",
    "\n",
    "nFeatures = len(features)\n",
    "colors = ['#64D1B8', 'red', 'blue']\n",
    "for n in range(nFeatures):\n",
    "    style_function = lambda x: {\n",
    "        'fillOpacity': 0.0,\n",
    "            'weight': 4,\n",
    "            'color': colors[0]\n",
    "            }\n",
    "    folium.GeoJson(data=get_geojson_string(features[n]['geometry']), style_function=style_function).add_to(Map)\n",
    "\n",
    "Map.add_child(folium.LayerControl())\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data pre-processing\n",
    "\n",
    "We normalize the composite images to have values from 0 to 1.\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = 'Sentinel-2-Top-of-Atmosphere-Reflectance'\n",
    "output_dataset = 'Lake-Water-Quality-100m'\n",
    "init_date = '2019-01-21'\n",
    "end_date = '2019-01-31'\n",
    "scale = 100 #scale in meters\n",
    "norm_type = 'geostore' # options ['global', 'geostore', 'custom']\n",
    "collections = [input_dataset, output_dataset]\n",
    "geostore_id = multipolygon.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize images function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_values(image, collection, scale, norm_type='global', geostore_id=None, values = {}):\n",
    "    \n",
    "    normThreshold = ee_collection_specifics.ee_bands_normThreshold(collection)\n",
    "    \n",
    "    if not norm_type == 'custom':\n",
    "        if norm_type == 'global':\n",
    "            num = 2\n",
    "            lon = np.linspace(-180, 180, num)\n",
    "            lat = np.linspace(-90, 90, num)\n",
    "            \n",
    "            features = []\n",
    "            for i in range(len(lon)-1):\n",
    "                for j in range(len(lat)-1):\n",
    "                    features.append(ee.Feature(ee.Geometry.Rectangle(lon[i], lat[j], lon[i+1], lat[j+1])))\n",
    "        \n",
    "        if norm_type == 'geostore':\n",
    "            try:\n",
    "                geostore = Skydipper.Geometry(id_hash=geostore_id)\n",
    "                features = []\n",
    "                for feature in geostore.attributes['geojson']['features']:\n",
    "                    features.append(ee.Feature(feature))\n",
    "                \n",
    "            except:\n",
    "                print('Geostore_id is needed')\n",
    "        \n",
    "        regReducer = {\n",
    "            'geometry': ee.FeatureCollection(features),\n",
    "            'reducer': ee.Reducer.minMax(),\n",
    "            'maxPixels': 1e10,\n",
    "            'bestEffort': True,\n",
    "            'scale':scale,\n",
    "            'tileScale': 10\n",
    "            \n",
    "        }\n",
    "        \n",
    "        values = image.reduceRegion(**regReducer).getInfo()\n",
    "        \n",
    "        # Avoid outliers by taking into account only the normThreshold% of the data points.\n",
    "        regReducer = {\n",
    "            'geometry': ee.FeatureCollection(features),\n",
    "            'reducer': ee.Reducer.histogram(),\n",
    "            'maxPixels': 1e10,\n",
    "            'bestEffort': True,\n",
    "            'scale':scale,\n",
    "            'tileScale': 10\n",
    "            \n",
    "        }\n",
    "        \n",
    "        hist = image.reduceRegion(**regReducer).getInfo()\n",
    "    \n",
    "        for band in list(normThreshold.keys()):\n",
    "            if normThreshold[band] != 100:\n",
    "                count = np.array(hist.get(band).get('histogram'))\n",
    "                x = np.array(hist.get(band).get('bucketMeans'))\n",
    "            \n",
    "                cumulative_per = np.cumsum(count/count.sum()*100)\n",
    "            \n",
    "                values[band+'_max'] = x[np.where(cumulative_per < normThreshold[band])][-1]\n",
    "    else:\n",
    "        values = values\n",
    "        \n",
    "    return values\n",
    "\n",
    "def normalize_ee_images(image, collection, values):\n",
    "    \n",
    "    Bands = ee_collection_specifics.ee_bands(collection)\n",
    "       \n",
    "    # Normalize [0, 1] ee images\n",
    "    for i, band in enumerate(Bands):\n",
    "        if i == 0:\n",
    "            image_new = image.select(band).clamp(values[band+'_min'], values[band+'_max'])\\\n",
    "                                .subtract(values[band+'_min'])\\\n",
    "                                .divide(values[band+'_max']-values[band+'_min'])\n",
    "        else:\n",
    "            image_new = image_new.addBands(image.select(band).clamp(values[band+'_min'], values[band+'_max'])\\\n",
    "                                    .subtract(values[band+'_min'])\\\n",
    "                                    .divide(values[band+'_max']-values[band+'_min']))\n",
    "            \n",
    "    return image_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate `image` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and image tables\n",
    "datasets = df_from_query('dataset')\n",
    "images = df_from_query('image')\n",
    "\n",
    "for collection in collections:\n",
    "    dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "\n",
    "    # Populate image table\n",
    "    if images[['dataset_id', 'scale', 'init_date', 'end_date']].isin([dataset_id, scale, init_date, end_date]).all(axis=1).any():\n",
    "        images = images\n",
    "    else:\n",
    "        # Create composite\n",
    "        image = ee_collection_specifics.Composite(collection)(init_date, end_date)\n",
    "    \n",
    "        bands = ee_collection_specifics.ee_bands(collection)\n",
    "        image = image.select(bands)\n",
    "        \n",
    "        if ee_collection_specifics.normalize(collection):\n",
    "            # Get min/man values for each band\n",
    "            values = min_max_values(image, collection, scale, norm_type=norm_type, geostore_id=geostore_id)\n",
    "        else:\n",
    "            values = {}\n",
    "            \n",
    "        \n",
    "    \n",
    "        # Append values to table\n",
    "        dictionary = dict(zip(list(images.keys()), [[dataset_id], [''], [scale], [init_date], [end_date], [json.dumps(values)], [norm_type]]))\n",
    "        images = images.append(pd.DataFrame(dictionary), ignore_index = True)\n",
    "        \n",
    "    # Save image table\n",
    "    df_to_csv(images, \"image\")\n",
    "    df_to_db(images, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "Map = folium.Map(location=centroid, zoom_start=6)\n",
    "Map.fit_bounds(bounds)\n",
    "\n",
    "# Read dataset and image tables\n",
    "datasets = df_from_query('dataset')\n",
    "images = df_from_query('image')\n",
    "\n",
    "for collection in collections:\n",
    "\n",
    "    dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "    \n",
    "    df = images[(images['dataset_id'] == dataset_id) & \n",
    "                (images['scale'] == scale) & \n",
    "                (images['init_date'] == init_date) & \n",
    "                (images['end_date'] == end_date) & \n",
    "                (images['norm_type'] == norm_type)\n",
    "               ].copy()\n",
    "    \n",
    "    values = json.loads(df['bands_min_max'].iloc[0])\n",
    "    \n",
    "    # Create composite\n",
    "    image = ee_collection_specifics.Composite(collection)(init_date, end_date)\n",
    "    \n",
    "    # Normalize images\n",
    "    if bool(values): \n",
    "        image = normalize_ee_images(image, collection, values)\n",
    "    \n",
    "        \n",
    "    for params in ee_collection_specifics.vizz_params(collection):\n",
    "        mapid = image.getMapId(params)\n",
    "        folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name=str(params['bands']),\n",
    "      ).add_to(Map)\n",
    "\n",
    "Map.add_child(folium.LayerControl())\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select input/output bands\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bands = ['B2','B3','B4','B5','ndvi','ndwi']\n",
    "output_bands = ['turbidity_blended_mean']\n",
    "bands = [input_bands, output_bands]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Populate `image` table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and image tables\n",
    "datasets = df_from_query('dataset')\n",
    "images = df_from_query('image')\n",
    "\n",
    "for n, collection in enumerate(collections):\n",
    "\n",
    "    dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "    \n",
    "    df = images[(images['dataset_id'] == dataset_id) & \n",
    "                (images['scale'] == scale) & \n",
    "                (images['init_date'] == init_date) & \n",
    "                (images['end_date'] == end_date) & \n",
    "                (images['norm_type'] == norm_type)\n",
    "               ].copy()\n",
    "    \n",
    "    # Take rows where bands_selections column is empty\n",
    "    df1 = df[df['bands_selections'] == ''].copy()\n",
    "    \n",
    "    if df1.any().any():\n",
    "        # Take first index\n",
    "        index = df1.index[0]\n",
    "        images.at[index, 'bands_selections'] = str(bands[n])\n",
    "    else:\n",
    "        if images[['dataset_id', 'bands_selections', 'scale', 'init_date', 'end_date', 'norm_type']].isin(\n",
    "            [dataset_id, str(bands[n]), scale, init_date, end_date, norm_type]).all(axis=1).any():\n",
    "            images = images\n",
    "        else:\n",
    "            df2 = df.iloc[0:1].copy()\n",
    "            df2.at[df2.index[0], 'bands_selections'] = str(bands[n])\n",
    "            images = images.append(df2, ignore_index = True)\n",
    "                   \n",
    "# Save image table\n",
    "df_to_csv(images, \"image\")\n",
    "df_to_db(images, \"image\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create TFRecords for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_ids(collections, bands, scale, init_date, end_date, norm_type):\n",
    "    # Read dataset and image tables\n",
    "    datasets = df_from_query('dataset')\n",
    "    images = df_from_query('image')\n",
    "    \n",
    "    image_ids = []\n",
    "    for n, collection in enumerate(collections):\n",
    "        \n",
    "        dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "        \n",
    "        df = images[(images['dataset_id'] == dataset_id) & \n",
    "                (images['bands_selections'] == str(bands[n])) & \n",
    "                (images['scale'] == scale) & \n",
    "                (images['init_date'] == init_date) & \n",
    "                (images['end_date'] == end_date) & \n",
    "                (images['norm_type'] == norm_type) \n",
    "               ].copy()\n",
    "        \n",
    "        image_ids.append(df.index[0])\n",
    "        \n",
    "    return image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeoJSONs_to_FeatureCollections(geostore):\n",
    "    feature_collections = []\n",
    "    for n in range(len(geostore.get('geojson').get('features'))):\n",
    "        # Make a list of Features\n",
    "        features = []\n",
    "        for i in range(len(geostore.get('geojson').get('features')[n].get('geometry').get('coordinates'))):\n",
    "            features.append(\n",
    "                ee.Feature(\n",
    "                    ee.Geometry.Polygon(\n",
    "                        geostore.get('geojson').get('features')[n].get('geometry').get('coordinates')[i]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        # Create a FeatureCollection from the list.\n",
    "        feature_collections.append(ee.FeatureCollection(features))\n",
    "    return feature_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_TFRecords(collections, bands, init_date, end_date, scale, norm_type, image_ids, geostore_id, \n",
    "                     sample_size, kernel_size, geostore, feature_collections, feature_lists):\n",
    "\n",
    "    ## Stack the 2D images (input and output images of the Neural Network) \n",
    "    ## to create a single image from which samples can be taken\n",
    "    \n",
    "    bucket = 'geo-ai'\n",
    "    folder = 'Data/'+str(image_ids[0])+'_'+ str(image_ids[1])+'/'+str(geostore_id)+'/'+str(kernel_size)+'/'+str(sample_size)\n",
    "    \n",
    "    # These numbers determined experimentally.\n",
    "    nShards  = int(sample_size/20) # Number of shards in each polygon.\n",
    "        \n",
    "    # Read dataset and image tables\n",
    "    datasets = df_from_query('dataset')\n",
    "    images = df_from_query('image')\n",
    "    \n",
    "    for n, collection in enumerate(collections):\n",
    "        \n",
    "        dataset_id = datasets[datasets['slug'] == collection].index[0]\n",
    "        \n",
    "        df = images[(images['dataset_id'] == dataset_id) & \n",
    "                (images['bands_selections'] == str(bands[n])) & \n",
    "                (images['scale'] == scale) & \n",
    "                (images['init_date'] == init_date) & \n",
    "                (images['end_date'] == end_date) &\n",
    "                (images['norm_type'] == norm_type)\n",
    "               ].copy()\n",
    "        \n",
    "        values = json.loads(df['bands_min_max'].iloc[0])\n",
    "    \n",
    "        # Create composite\n",
    "        composite = ee_collection_specifics.Composite(collection)(init_date, end_date)\n",
    "    \n",
    "        # Normalize images\n",
    "        if bool(values): \n",
    "            composite = normalize_ee_images(composite, collection, values)\n",
    "        \n",
    "        if n == 0:\n",
    "            image_stack = composite.select(bands[n])\n",
    "        else:\n",
    "            image_stack = ee.Image.cat([image_stack,composite.select(bands[n])]).float()\n",
    "            \n",
    "    if kernel_size == 1:\n",
    "        # Sample pixels\n",
    "        vector = image_stack.sample(region = feature_collections[0], scale = scale,\\\n",
    "                                    numPixels=sample_size, tileScale=4, seed=999)\n",
    "\n",
    "        # Add random column\n",
    "        vector = vector.randomColumn(seed=999)\n",
    "\n",
    "        # Partition the sample approximately 70-30.\n",
    "        train_dataset = vector.filter(ee.Filter.lt('random', 0.7))\n",
    "        eval_dataset = vector.filter(ee.Filter.gte('random', 0.7))\n",
    "        \n",
    "        # Training and validation size\n",
    "        train_size = train_dataset.size().getInfo()\n",
    "        eval_size = eval_dataset.size().getInfo()\n",
    "        \n",
    "        # Export all the training/evaluation data.   \n",
    "        file_paths = []\n",
    "        base_names = ['training_pixels', 'eval_pixels']\n",
    "        for n, dataset in enumerate([train_dataset, eval_dataset]):\n",
    "            \n",
    "            file_paths.append(bucket+ '/' + folder + '/' + base_names[n])\n",
    "            \n",
    "            # Create the tasks.\n",
    "            task = ee.batch.Export.table.toCloudStorage(\n",
    "              collection = dataset,\n",
    "              description = 'Export '+base_names[n],\n",
    "              fileNamePrefix = folder + '/' + base_names[n],\n",
    "              bucket = bucket,\n",
    "              fileFormat = 'TFRecord',\n",
    "              selectors = bands[0] + bands[1])\n",
    "            \n",
    "            task.start()\n",
    "            \n",
    "    if kernel_size > 1:\n",
    "        # Convert the image into an array image in which each pixel stores (kernel_size x kernel_size) patches of pixels for each band.\n",
    "        list = ee.List.repeat(1, kernel_size)\n",
    "        lists = ee.List.repeat(list, kernel_size)\n",
    "        kernel = ee.Kernel.fixed(kernel_size, kernel_size, lists)\n",
    "        \n",
    "        arrays = image_stack.neighborhoodToArray(kernel)\n",
    "        \n",
    "        # Training and validation size\n",
    "        nFeatures = len(geostore.get('geojson').get('features'))\n",
    "        nPolygons = {}\n",
    "        for n in range(nFeatures):\n",
    "            multipoly_type = geostore.get('geojson').get('features')[n].get('properties').get('name')\n",
    "            nPolygons[multipoly_type] = len(geostore.get('geojson').get('features')[n].get('geometry').get('coordinates'))\n",
    "            \n",
    "        train_size = nPolygons['training']*sample_size\n",
    "        eval_size = nPolygons['validation']*sample_size\n",
    "    \n",
    "        # Export all the training/evaluation data (in many pieces), with one task per geometry.      \n",
    "        file_paths = []\n",
    "        base_names = ['training_patches', 'eval_patches']\n",
    "        for i, feature in enumerate(feature_collections):\n",
    "            for g in range(feature.size().getInfo()):\n",
    "                geomSample = ee.FeatureCollection([])\n",
    "                for j in range(nShards):\n",
    "                    sample = arrays.sample(\n",
    "                        region = ee.Feature(feature_lists[i].get(g)).geometry(), \n",
    "                        scale = scale, \n",
    "                        numPixels = sample_size / nShards, # Size of the shard.\n",
    "                        seed = j,\n",
    "                        tileScale = 8\n",
    "                    )\n",
    "                    geomSample = geomSample.merge(sample)\n",
    "                    \n",
    "                desc = base_names[i] + '_g' + str(g)\n",
    "                \n",
    "                file_paths.append(bucket+ '/' + folder + '/' + desc)\n",
    "                \n",
    "                task = ee.batch.Export.table.toCloudStorage(\n",
    "                    collection = geomSample,\n",
    "                    description = desc, \n",
    "                    bucket = bucket, \n",
    "                    fileNamePrefix = folder + '/' + desc,\n",
    "                    fileFormat = 'TFRecord',\n",
    "                    selectors = bands[0] + bands[1]\n",
    "                )\n",
    "                task.start()\n",
    "                \n",
    "    return task, file_paths, train_size, eval_size, base_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = get_image_ids(collections, bands, scale, init_date, end_date)\n",
    "geostore_id = multipolygon.id\n",
    "kernel_size = 1 #256\n",
    "sample_size = 20000 #1000 # Total sample size in each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the GeoJSON to feature collections\n",
    "feature_collections = GeoJSONs_to_FeatureCollections(geostore)\n",
    "\n",
    "# Convert the feature collections to lists for iteration.\n",
    "feature_lists = list(map(lambda x: x.toList(x.size()), feature_collections))\n",
    "\n",
    "# Export all the training/evaluation data.\n",
    "versions = df_from_query('model_versions')\n",
    "\n",
    "df = versions[['input_image_id', 'output_image_id', 'geostore_id', 'kernel_size', 'sample_size']].isin([image_ids[0], image_ids[1], geostore_id, kernel_size, sample_size]).copy()\n",
    "if not df.all(axis=1).any():\n",
    "    task, file_paths, train_size, eval_size, base_names = export_TFRecords(collections, bands, init_date, end_date, scale, norm_type, image_ids, geostore_id,\n",
    "                                                                           sample_size, kernel_size, geostore, feature_collections, feature_lists)\n",
    "elif not (versions[df.all(axis=1)]['data_status'] == 'COMPLETED').all():\n",
    "    task, file_paths, train_size, eval_size, base_names = export_TFRecords(collections, bands, init_date, end_date, scale, norm_type, image_ids, geostore_id,\n",
    "                                                                           sample_size, kernel_size, geostore, feature_collections, feature_lists)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate `model_versions` tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all the training/evaluation data.\n",
    "versions = df_from_query('model_versions')\n",
    "\n",
    "df = versions[['input_image_id', 'output_image_id', 'geostore_id', 'kernel_size', 'sample_size']].isin([image_ids[0], image_ids[1], geostore_id, kernel_size, sample_size]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all the training/evaluation data.\n",
    "versions = df_from_query('model_versions')\n",
    "\n",
    "if (versions.empty) or not df.all(axis=1).any():\n",
    "    dictionary = dict(zip(list(versions.keys()), [[-9999], [''], [image_ids[0]], [image_ids[1]], [geostore_id], [kernel_size], [sample_size], [json.dumps({})], [-9999], [''], [''], [False], [False]]))\n",
    "    versions = versions.append(pd.DataFrame(dictionary), ignore_index = True, sort=False)\n",
    "    \n",
    "# Save version table\n",
    "df_to_csv(versions, \"model_versions\")\n",
    "df_to_db(versions, \"model_versions\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def check_status_data(task, file_paths):\n",
    "    status_list = list(map(lambda x: str(x), task.list()[:len(file_paths)])) \n",
    "    status_list = list(map(lambda x: x[x.find(\"(\")+1:x.find(\")\")], status_list))\n",
    "    \n",
    "    return status_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read model versions table.\n",
    "versions = df_from_query('model_versions')\n",
    "\n",
    "df = versions[['input_image_id', 'output_image_id', 'geostore_id', 'kernel_size', 'sample_size']].isin([image_ids[0], image_ids[1], geostore_id, kernel_size, sample_size]).copy()\n",
    "if not (versions[df.all(axis=1)]['data_status'] == 'COMPLETED').all():\n",
    "    status_list = check_status_data(task, file_paths)\n",
    "    index = versions.index[-1]\n",
    "    while not status_list == ['COMPLETED'] * len(file_paths):\n",
    "        status_list = check_status_data(task, file_paths)\n",
    "        \n",
    "        #Save temporal status in table\n",
    "        versions.at[index, 'data_status'] = json.dumps(dict(zip(file_paths, status_list)))\n",
    "        df_to_csv(versions, \"model_versions\")\n",
    "        df_to_db(versions, \"model_versions\")\n",
    "        \n",
    "        time.sleep(60)\n",
    "    \n",
    "    #Save final status in table\n",
    "    versions.at[index, 'data_status'] = \"COMPLETED\"\n",
    "\n",
    "    df_to_csv(versions, \"model_versions\")\n",
    "    df_to_db(versions, \"model_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Training the model in AI Platform\n",
    "### Training code package setup\n",
    "\n",
    "It's necessary to create a Python package to hold the training code.  Here we're going to get started with that by creating a folder for the package and adding an empty `__init__.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = 'AI_Platform/cnn_trainer'\n",
    "PACKAGE_FOLDER = '/trainer'\n",
    "\n",
    "!rm -r {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH+PACKAGE_FOLDER}\n",
    "!touch {ROOT_PATH+PACKAGE_FOLDER}/__init__.py\n",
    "!ls -l {ROOT_PATH+PACKAGE_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setuptools file named `setup.py`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH}/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['keras==2.2.4']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='My training application package.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training/evaluation data**\n",
    "\n",
    "The following is code to load training/evaluation data.  Write this into `util.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/util.py\n",
    "\"\"\"Utilities to download and preprocess the data.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "\n",
    "class Util():\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "        self.client = storage.Client(project='skydipper-196010')\n",
    "        self.bucket = self.client.get_bucket('geo-ai')\n",
    "        self.blob = self.bucket.blob(self.path)\n",
    "        self.config = json.loads(self.blob.download_as_string(client=self.client).decode('utf-8'))\n",
    "        \n",
    "    def parse_function(self, proto):\n",
    "        \"\"\"The parsing function.\n",
    "        Read a serialized example into the structure defined by features_dict.\n",
    "        Args:\n",
    "          example_proto: a serialized Example.\n",
    "        Returns: \n",
    "          A dictionary of tensors, keyed by feature name.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define your tfrecord \n",
    "        features = self.config.get('in_bands') + self.config.get('out_bands')\n",
    "        \n",
    "        # Specify the size and shape of patches expected by the model.\n",
    "        kernel_shape = [self.config.get('kernel_size'), self.config.get('kernel_size')]\n",
    "        columns = [\n",
    "          tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) for k in features\n",
    "        ]\n",
    "        features_dict = dict(zip(features, columns))\n",
    "        \n",
    "        # Load one example\n",
    "        parsed_features = tf.io.parse_single_example(proto, features_dict)\n",
    "    \n",
    "        # Convert a dictionary of tensors to a tuple of (inputs, outputs)\n",
    "        inputs_list = [parsed_features.get(key) for key in features]\n",
    "        stacked = tf.stack(inputs_list, axis=0)\n",
    "        \n",
    "        # Convert the tensors into a stack in HWC shape\n",
    "        stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "        \n",
    "        return stacked[:,:,:len(self.config.get('in_bands'))], stacked[:,:,len(self.config.get('in_bands')):]\n",
    "    \n",
    "    def get_dataset(self, glob):\n",
    "        \"\"\"Get the preprocessed training dataset\n",
    "        Returns: \n",
    "        A tf.data.Dataset of training data.\n",
    "        \"\"\"\n",
    "        glob = tf.compat.v1.io.gfile.glob(glob)\n",
    "        \n",
    "        dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "        dataset = dataset.map(self.parse_function, num_parallel_calls=5)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    def get_training_dataset(self):\n",
    "        \"\"\"Get the preprocessed training dataset\n",
    "        Returns: \n",
    "        A tf.data.Dataset of training data.\n",
    "        \"\"\"\n",
    "        glob = self.config.get('data_dir') + '/' + self.config.get('base_names')[0] + '*'\n",
    "        dataset = self.get_dataset(glob)\n",
    "        dataset = dataset.shuffle(self.config.get('shuffle_size')).batch(self.config.get('batch_size')).repeat()\n",
    "        return dataset\n",
    "    \n",
    "    def get_evaluation_dataset(self):\n",
    "        \"\"\"Get the preprocessed evaluation dataset\n",
    "        Returns: \n",
    "          A tf.data.Dataset of evaluation data.\n",
    "        \"\"\"\n",
    "        glob = self.config.get('data_dir') + '/' + self.config.get('base_names')[1] + '*'\n",
    "        dataset = self.get_dataset(glob)\n",
    "        dataset = dataset.batch(1).repeat()\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {ROOT_PATH+PACKAGE_FOLDER+'/models'}\n",
    "!touch {ROOT_PATH+PACKAGE_FOLDER+'/models'}/__init__.py\n",
    "!cp -r ../models/CNN {ROOT_PATH+PACKAGE_FOLDER+'/models'}/CNN\n",
    "!cp -r ../models/MLP {ROOT_PATH+PACKAGE_FOLDER+'/models'}/MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/model.py\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "import json\n",
    "\n",
    "from .models.CNN.regression import deepvel as CNNregDeepVel, segnet as  CNNregSegNet, unet as CNNregUNet\n",
    "from .models.CNN.segmentation import deepvel as CNNsegDeepVel, segnet as  CNNsegSegNet, unet as CNNsegUNet\n",
    "from .models.MLP.regression import sequential1 as MLPregSequential1\n",
    "\n",
    "def select_model(path):\n",
    "    # Read training parameters from GCS\n",
    "    client = storage.Client(project='skydipper-196010')\n",
    "    bucket = client.get_bucket('geo-ai')\n",
    "    blob = bucket.blob(path)\n",
    "    config = json.loads(blob.download_as_string(client=client).decode('utf-8'))\n",
    "    \n",
    "    # Model's dictionary\n",
    "    models = {'CNN':\n",
    "              {\n",
    "                  'regression': \n",
    "                  {\n",
    "                      'deepvel': CNNregDeepVel.create_keras_model,\n",
    "                      'segnet': CNNregSegNet.create_keras_model,\n",
    "                      'unet': CNNregUNet.create_keras_model,\n",
    "                  },\n",
    "                  'segmentation': \n",
    "                  {\n",
    "                      'deepvel': CNNsegDeepVel.create_keras_model,\n",
    "                      'segnet': CNNsegSegNet.create_keras_model,\n",
    "                      'unet': CNNsegUNet.create_keras_model,\n",
    "                  }\n",
    "              }, \n",
    "              'MLP': \n",
    "              {\n",
    "                  'regression': \n",
    "                  {\n",
    "                      'sequential1': MLPregSequential1.create_keras_model,\n",
    "                  }\n",
    "              }\n",
    "             }\n",
    "    \n",
    "    return models.get(config.get('model_type')).get(config.get('model_output')).get(config.get('model_architecture'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training task**\n",
    "\n",
    "The following will create `task.py`, which will get the training and evaluation data, train the model and save it when it's done in a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/task.py\n",
    "\"\"\"Trains a Keras model\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "\n",
    "from .util import Util\n",
    "from . import model\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--params-file',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='GCS location where we have saved the training_params.json file')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='INFO')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read training parameters from GCS\n",
    "    client = storage.Client(project='skydipper-196010')\n",
    "    bucket = client.get_bucket('geo-ai')\n",
    "    blob = bucket.blob(args.params_file)\n",
    "    config = json.loads(blob.download_as_string(client=client).decode('utf-8'))\n",
    "\n",
    "    # Create the Keras Model\n",
    "    selected_model = model.select_model(args.params_file)\n",
    "\n",
    "    if not config.get('output_activation'):\n",
    "        keras_model = selected_model(inputShape = (None, None, len(config.get('in_bands'))), nClasses = len(config.get('out_bands')))\n",
    "    else:\n",
    "        keras_model = selected_model(inputShape = (None, None, len(config.get('in_bands'))), nClasses = len(config.get('out_bands')), output_activation = config.get('output_activation'))\n",
    "\n",
    "    # Compile Keras model\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=config.get('learning_rate'))\n",
    "    keras_model.compile(loss=config.get('loss'), optimizer=optimizer, metrics=config.get('metrics'))\n",
    "\n",
    "\n",
    "    # Pass a tfrecord\n",
    "    util = Util(path = args.params_file) \n",
    "    training_dataset = util.get_training_dataset()\n",
    "    evaluation_dataset = util.get_evaluation_dataset()\n",
    "\n",
    "    # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(os.path.join(config.get('job_dir'), 'logs'))\n",
    "\n",
    "    # Train model\n",
    "    keras_model.fit(\n",
    "        x=training_dataset,\n",
    "        steps_per_epoch=int(config.get('train_size') / config.get('batch_size')),\n",
    "        epochs=config.get('epochs'),\n",
    "        validation_data=evaluation_dataset,\n",
    "        validation_steps=int(config.get('eval_size') / config.get('batch_size')),\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard_cb])\n",
    "\n",
    "    tf.contrib.saved_model.save_keras_model(keras_model, os.path.join(config.get('job_dir'), 'model'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    tf.logging.set_verbosity('INFO')\n",
    "    train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a .tar.gz distribution package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = './AI_Platform/trainer-0.1.tar.gz'\n",
    "source_dir = './AI_Platform/cnn_trainer/'\n",
    "\n",
    "make_tarfile(output_filename, source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_filename = './AI_Platform/cnn_trainer/dist/trainer-0.1.tar.gz'\n",
    "\n",
    "client = storage.Client().from_service_account_json(env.privatekey_path)\n",
    "bucket = client.get_bucket(training_params['bucket'])\n",
    "blob = bucket.blob('Train/trainer-0.1.tar.gz')\n",
    "                     \n",
    "blob.upload_from_filename(\n",
    "    filename = output_filename, \n",
    "    content_type = 'text/plain',\n",
    "    client=client\n",
    ")\n",
    "print(blob.public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for training in AI platform\n",
    "bucket = 'geo-ai'\n",
    "project_id = env.project_id\n",
    "region = \"us-central1\"\n",
    "\n",
    "main_trainer_module = 'trainer.task'\n",
    "\n",
    "model_type = 'MLP'\n",
    "model_output = 'regression'\n",
    "model_architecture = 'sequential1'\n",
    "\n",
    "# Training parameters\n",
    "training_params = {\n",
    "    \"bucket\": bucket,\n",
    "    \"base_names\": base_names,\n",
    "    \"data_dir\": 'gs://' + bucket + '/Data/' + str(image_ids[0])+'_'+ str(image_ids[1])+'/'+str(geostore_id)+'/'+str(kernel_size)+'/'+str(sample_size),\n",
    "    \"in_bands\": bands[0],\n",
    "    \"out_bands\": bands[1],\n",
    "    \"kernel_size\": int(kernel_size),\n",
    "    \"train_size\": train_size,\n",
    "    \"eval_size\": eval_size,\n",
    "    \"model_type\": model_type,\n",
    "    \"model_output\": model_output,\n",
    "    \"model_architecture\": model_architecture,\n",
    "    \"output_activation\": '',\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 100,\n",
    "    \"shuffle_size\": 2000,\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"loss\": \"mse\",\n",
    "    \"metrics\": ['mse']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Populate `model` table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read model table\n",
    "models = df_from_query('model')\n",
    "\n",
    "df = models[['model_type', 'model_output', 'output_image_id']].isin([model_type, model_output, image_ids[1]]).copy()\n",
    "if not df.all(axis=1).any():\n",
    "    dictionary = dict(zip(list(models.keys()), [[''], [model_type], [model_output], [''], [image_ids[1]]]))\n",
    "    models = models.append(pd.DataFrame(dictionary), ignore_index = True, sort=False)\n",
    "\n",
    "# Save model table\n",
    "df_to_csv(models, \"model\")\n",
    "df_to_db(models, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = models[(models['model_type'] == model_type) & (models['model_output'] == model_output) & (models['output_image_id'] == image_ids[1])].index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Populate `model_versions` table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removekey(dictionary, key):\n",
    "    if key in dictionary.keys():\n",
    "        del dictionary[key]\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read model table\n",
    "versions = df_from_query('model_versions')\n",
    "df = versions.copy()\n",
    "df['training_params'] = df['training_params'].apply(lambda x : removekey(json.loads(x),'job_dir'))\n",
    "\n",
    "# Check if the version already exists\n",
    "if (df['training_params'] == training_params).any():\n",
    "    # Get version id\n",
    "    version_id = df[df['training_params'].apply(lambda x : removekey(x,'job_dir')) == training_params].index[0]\n",
    "    \n",
    "    # Check status\n",
    "    status = df.iloc[version_id]['training_status']\n",
    "    print('Version already exists with training status equal to:', status)\n",
    "    \n",
    "    if status == 'SUCCEEDED':\n",
    "        print('The training job successfully completed.')\n",
    "    if (status == 'CANCELLED') or (status == 'FAILED'):\n",
    "        print(f'The training job was {status}.')\n",
    "        if status == 'CANCELLED':  \n",
    "            print('Start training again.')\n",
    "        if status == 'FAILED': \n",
    "            print('Change training parameters and try again.')\n",
    "        # Get training version\n",
    "        training_version = df.iloc[version_id]['version']\n",
    "        \n",
    "        # Update job name\n",
    "        job_name = 'job_v' + str(int(time.time()))\n",
    "            \n",
    "        # Add job directory\n",
    "        training_params = json.loads(df.iloc[version_id]['training_params'])\n",
    "        training_params['job_dir'] = 'gs://' + bucket + '/Models/' + str(model_id) + '/' +  str(training_version) + '/'\n",
    "        \n",
    "        # Save training version and clear status\n",
    "        versions.at[version_id, 'training_params'] =  json.dumps(training_params)\n",
    "        versions.at[version_id, 'training_status'] = ''\n",
    "        \n",
    "        # Remove job_dir\n",
    "        !gsutil rm -r {training_params['job_dir']}\n",
    "        \n",
    "# Create new version  \n",
    "else:\n",
    "    print('Create new version')\n",
    "    # New training version and job name\n",
    "    training_version = str(int(time.time()))\n",
    "    job_name = 'job_v' + training_version\n",
    "    \n",
    "    # Add job directory\n",
    "    training_params['job_dir'] = 'gs://' + bucket + '/Models/' + str(model_id) + '/' +  str(training_version) + '/'\n",
    "    \n",
    "    df = versions[['input_image_id', 'output_image_id', 'geostore_id', 'kernel_size', 'sample_size', 'training_params', 'data_status']].isin(\n",
    "        [image_ids[0], image_ids[1], geostore_id, kernel_size, sample_size, json.dumps(training_params), 'COMPLETED']).copy()\n",
    "    \n",
    "    # Check if untrained version already exists\n",
    "    if (df.all(axis=1).any()):\n",
    "        version_id = df[df.all(axis=1)].index[0]\n",
    "        #versions[(versions['input_image_id'] == image_ids[0]) & (versions['output_image_id'] == image_ids[1]) & \n",
    "                              #(versions['geostore_id'] == geostore_id)  & (versions['kernel_size'] == kernel_size) & \n",
    "                              #(versions['sample_size'] == sample_size)].index[0]\n",
    "    \n",
    "        versions.at[version_id, 'model_id'] = model_id\n",
    "        versions.at[version_id, 'model_architecture'] = model_architecture\n",
    "        versions.at[version_id, 'training_params'] = json.dumps(training_params)\n",
    "        versions.at[version_id, 'version'] = training_version\n",
    "        \n",
    "    else:\n",
    "        dictionary = dict(zip(list(versions.keys()), [[''], [''], [image_ids[0]], [image_ids[1]], [geostore_id], [kernel_size], [sample_size], [''], [''], ['COMPLETED'], [''], [''], ['']]))\n",
    "        versions = versions.append(pd.DataFrame(dictionary), ignore_index = True, sort=False)\n",
    "        version_id = versions.index[-1]\n",
    "        \n",
    "        versions.at[version_id, 'model_id'] = int(model_id)\n",
    "        versions.at[version_id, 'model_architecture'] = model_architecture\n",
    "        versions.at[version_id, 'training_params'] = json.dumps(training_params)\n",
    "        versions.at[version_id, 'version'] = int(training_version)\n",
    "\n",
    "        \n",
    "# Save version table\n",
    "versions = versions.astype({'model_id': 'int64', \n",
    "                            'version': 'int64', \n",
    "                            'eeified': bool, \n",
    "                            'deployed': bool})\n",
    "\n",
    "df_to_csv(versions, \"model_versions\")\n",
    "df_to_db(versions, \"model_versions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save training parameters**\n",
    "\n",
    "These training parameters need to be stored in a place where other code can access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = 'Models/' + str(model_id) + '/' +  str(training_version) + '/training_params.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = storage.Client(project=env.project_id)\n",
    "client = storage.Client.from_service_account_json(env.privatekey_path)\n",
    "bucket = client.get_bucket(training_params['bucket'])\n",
    "blob = bucket.blob(params_path)\n",
    "\n",
    "blob.upload_from_string(\n",
    "    data=json.dumps(training_params),\n",
    "    content_type='application/json',\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a training job to AI Platform for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up your GCP project**\n",
    "\n",
    "Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authenticate your GCP account**\n",
    "\n",
    "Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS {env.privatekey_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit a training job to AI Platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = {'scaleTier': 'CUSTOM',             \n",
    "    'masterType': 'large_model_v100', # A single NVIDIA Tesla V100 GPU \n",
    "    'packageUris': ['gs://'+training_params['bucket']+'/'+'Train/trainer-0.1.tar.gz'],\n",
    "    'pythonModule': main_trainer_module,\n",
    "    'args': ['--params-file', params_path],\n",
    "    'region': region,\n",
    "    'jobDir': training_params['job_dir'],\n",
    "    'runtimeVersion': '1.14',\n",
    "    'pythonVersion': '3.5'}\n",
    "\n",
    "job_spec = {'jobId': job_name, 'trainingInput': training_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating training job: ' + job_name)\n",
    "\n",
    "# Save your project ID in the format the APIs need\n",
    "project = 'projects/{}'.format(env.project_id)\n",
    "\n",
    "# Get a Python representation of the AI Platform Training services\n",
    "credentials = GoogleCredentials.from_stream(env.privatekey_path)\n",
    "ml = discovery.build('ml', 'v1', credentials = credentials)\n",
    "\n",
    "# Create a request to call projects.jobs.create.\n",
    "request = ml.projects().jobs().create(body=job_spec,\n",
    "              parent=project)\n",
    "\n",
    "\n",
    "# Make the call.\n",
    "try:\n",
    "    response = request.execute()\n",
    "    print(response)\n",
    "\n",
    "except errors.HttpError as err:\n",
    "    # Something went wrong, print out some information.\n",
    "    print('There was an error creating the training job. Check the details:')\n",
    "    print(err._get_reason())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save training status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status_training(job_name, project_id):\n",
    "    desc = !gcloud ai-platform jobs describe {job_name} --project {project_id}\n",
    "    return desc.grep('state:')[0].split(':')[1].strip()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read version table\n",
    "versions = df_from_query('model_versions')\n",
    "\n",
    "status = check_status_training(job_name, env.project_id)\n",
    "while not status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "    status = check_status_training(job_name, env.project_id)\n",
    "    \n",
    "    #Save temporal status in table\n",
    "    versions.at[version_id, 'training_status'] = status    \n",
    "    df_to_csv(versions, \"model_versions\")\n",
    "    df_to_db(versions, \"model_versions\")\n",
    "    \n",
    "    time.sleep(60)\n",
    "\n",
    "#Save final status in table\n",
    "versions.at[version_id, 'training_status'] = status\n",
    "df_to_csv(versions, \"model_versions\")\n",
    "df_to_db(versions, \"model_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Prepare the model for making predictions in Earth Engine\n",
    "\n",
    "Before we can use the model in Earth Engine, it needs to be hosted by AI Platform.  But before we can host the model on AI Platform we need to *EEify* (a new word!) it.  The EEification process merely appends some extra operations to the input and outputs of the model in order to accomdate the interchange format between pixels from Earth Engine (float32) and inputs to AI Platform (base64).  (See [this doc](https://cloud.google.com/ml-engine/docs/online-predict#binary_data_in_prediction_input) for details.)  \n",
    "\n",
    "**`earthengine model prepare`**\n",
    "\n",
    "The EEification process is handled for you using the Earth Engine command `earthengine model prepare`.  To use that command, we need to specify the input and output model directories and the name of the input and output nodes in the TensorFlow computation graph.  We can do all that programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "# Read version table\n",
    "versions = df_from_query('model_versions')\n",
    "\n",
    "model_path = training_params.get('job_dir') + 'model/'\n",
    "\n",
    "meta_graph_def = saved_model_utils.get_meta_graph_def(model_path, 'serve')\n",
    "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# model only has a single input and a single output.\n",
    "input_name = None\n",
    "for k,v in inputs.items():\n",
    "    input_name = v.name\n",
    "    break\n",
    "\n",
    "output_name = None\n",
    "for k,v in outputs.items():\n",
    "    output_name = v.name\n",
    "    break\n",
    "\n",
    "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
    "# AI Platform inputs and outputs, respectively.\n",
    "import json\n",
    "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "output_dict = \"'\" + json.dumps({output_name: \"prediction\"}) + \"'\"\n",
    "\n",
    "# Put the EEified model next to the trained model directory.\n",
    "EEified_path = training_params.get('job_dir') + 'eeified/' \n",
    "\n",
    "# You need to set the project before using the model prepare command.\n",
    "!earthengine set_project {project_id}\n",
    "!earthengine model prepare --source_dir {model_path} --dest_dir {EEified_path} --input {input_dict} --output {output_dict}\n",
    "\n",
    "# Populate model_versions table\n",
    "versions.at[version_id, 'eeified'] = True\n",
    "df_to_csv(versions, \"model_versions\")\n",
    "df_to_db(versions, \"model_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployed the model to AI Platform\n",
    "\n",
    "Before it's possible to get predictions from the trained and EEified model, it needs to be deployed on AI Platform.  The first step is to create the model.  The second step is to create a version.  See [this guide](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models) for details.  Note that models and versions can be monitored from the [AI Platform models page](http://console.cloud.google.com/ai-platform/models) of the Cloud Console. \n",
    "\n",
    "To ensure that the model is ready for predictions without having to warm up nodes, you can use a configuration yaml file to set the scaling type of this version to autoScaling, and, set a minimum number of nodes for the version. This will ensure there are always nodes on stand-by, however, you will be charged as long as they are running. For this example, we'll set the minNodes to 10. That means that at a minimum, 10 nodes are always up and running and waiting for predictions. The number of nodes will also scale up automatically if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-central1\"\n",
    "version_name = 'v' + training_version\n",
    "\n",
    "models = df_from_query('model')\n",
    "model_name = models.iloc[model_id]['model_name']\n",
    "model_description = models.iloc[model_id]['model_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating model: ' + model_name)\n",
    "\n",
    "# Store your full project ID in a variable in the format the API needs.\n",
    "project = 'projects/{}'.format(env.project_id)\n",
    "\n",
    "# Build a representation of the Cloud ML API.\n",
    "credentials = GoogleCredentials.from_stream(env.privatekey_path)\n",
    "ml = discovery.build('ml', 'v1', credentials=credentials)\n",
    "\n",
    "# Create a dictionary with the fields from the request body.\n",
    "request_dict = {'name': model_name,\n",
    "               'description': model_description}\n",
    "\n",
    "# Create a request to call projects.models.create.\n",
    "request = ml.projects().models().create(\n",
    "              parent=project, body=request_dict)\n",
    "\n",
    "# Make the call.\n",
    "try:\n",
    "    response = request.execute()\n",
    "    print(response)\n",
    "except errors.HttpError as err:\n",
    "    # Something went wrong, print out some information.\n",
    "    print('There was an error creating the model. Check the details:')\n",
    "    print(err._get_reason())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a representation of the Cloud ML API.\n",
    "credentials = GoogleCredentials.from_stream(env.privatekey_path)\n",
    "ml = discovery.build('ml', 'v1', credentials=credentials)\n",
    "\n",
    "# Create a dictionary with the fields from the request body.\n",
    "request_dict = {\n",
    "    'name': version_name,\n",
    "    'deploymentUri': EEified_path,\n",
    "    'runtimeVersion': '1.14',\n",
    "    'pythonVersion': '3.5',\n",
    "    'framework': 'TENSORFLOW',\n",
    "    'autoScaling': {\n",
    "        \"minNodes\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a request to call projects.models.versions.create.\n",
    "request = ml.projects().models().versions().create(\n",
    "    parent=f'projects/{env.project_id}/models/{model_name}',\n",
    "    body=request_dict\n",
    ")\n",
    "\n",
    "# Make the call.\n",
    "try:\n",
    "    response = request.execute()\n",
    "    print(response)\n",
    "except errors.HttpError as err:\n",
    "    # Something went wrong, print out some information.\n",
    "    print('There was an error creating the model. Check the details:')\n",
    "    print(err._get_reason())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save deployment status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status_deployment(model_name, version_name):\n",
    "    desc = !gcloud ai-platform versions describe {version_name} --model={model_name}\n",
    "    return desc.grep('state:')[0].split(':')[1].strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read version table\n",
    "versions = df_from_query('model_versions')\n",
    "\n",
    "status = check_status_deployment(model_name, version_name)\n",
    "while not status == 'READY':\n",
    "    status = check_status_deployment(model_name, version_name)\n",
    "    \n",
    "    #Save temporal status in table\n",
    "    versions.at[version_id, 'deployed'] = False   \n",
    "    df_to_csv(versions, \"model_versions\")\n",
    "    df_to_db(versions, \"model_versions\")\n",
    "    \n",
    "    time.sleep(60)\n",
    "\n",
    "#Save final status in table\n",
    "versions.at[version_id, 'deployed'] = True\n",
    "df_to_csv(versions, \"model_versions\")\n",
    "df_to_db(versions, \"model_versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Predict in Earth Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = df_from_query('dataset')\n",
    "images = df_from_query('image')\n",
    "models = df_from_query('model')\n",
    "versions = df_from_query('model_versions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select pre-trained models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(models['model_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MLP_regression_4'\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select versions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = models[models['model_name'] == model_name].index[0]\n",
    "model_type = models.iloc[model_id]['model_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_names = list(map(lambda x: int(x), list(versions[versions['model_id'] == model_id]['version'])))\n",
    "print(version_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = version_names[2]\n",
    "version_id = versions[versions['version'] == version].index[0]\n",
    "version_name = 'v'+ str(version)\n",
    "print(version_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id = versions[versions['version'] == version].index[0]\n",
    "training_params =json.loads(versions[versions['version'] == version]['training_params'][version_id])\n",
    "image_ids = list(versions.iloc[version_id][['input_image_id', 'output_image_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = list(datasets.iloc[list(images.iloc[image_ids]['dataset_id'])]['slug'])\n",
    "bands = [training_params.get('in_bands'), training_params.get('out_bands')]\n",
    "scale, init_date, end_date = list(images.iloc[image_ids[0]][['scale', 'init_date', 'end_date']])\n",
    "scale = float(scale)\n",
    "project_id = env.project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Datasets: ', collections)\n",
    "print('Bands: ', bands)\n",
    "print('scale: ', scale)\n",
    "print('init_date: ', init_date)\n",
    "print('end_date: ', end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select new date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_date_new = '2019-01-21'\n",
    "end_date_new = '2019-01-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Polygon object from Geojson**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts={'geojson': {'type': 'FeatureCollection',\n",
    "  'features': [{'type': 'Feature',\n",
    "    'properties': {},\n",
    "    'geometry': {'type': 'Polygon',\n",
    "     'coordinates': [[[0.5548095703125, 40.496048060627885],\n",
    "                      [0.9558105468749999,40.496048060627885],\n",
    "                      [0.9558105468749999,40.83667117059108],\n",
    "                      [0.5548095703125,40.83667117059108],\n",
    "                      [0.5548095703125,40.496048060627885]]]}}]}}\n",
    "\n",
    "geometry = Skydipper.Geometry(attributes=atts)\n",
    "geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ee.Model.fromAiPlatformPredictor`**\n",
    "\n",
    "There is now a trained model, prepared for serving to Earth Engine, hosted and versioned on AI Platform.  \n",
    "We can now connect Earth Engine directly to the trained model for inference.  You do that with the `ee.Model.fromAiPlatformPredictor` command.\n",
    "For this command to work, we need to know a lot about the model.  To connect to the model, you need to know the name and version.\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "You need to be able to recreate the imagery on which it was trained in order to perform inference.  Specifically, you need to create an array-valued input from the scaled data and use that for input.  (Recall that the new input node is named `array`, which is convenient because the array image has one band, named `array` by default.)  The inputs will be provided as 144x144 patches (`inputTileSize`), at 30-meter resolution (`proj`), but 8 pixels will be thrown out (`inputOverlapSize`) to minimize boundary effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_id = versions.iloc[version_id]['input_image_id']\n",
    "    \n",
    "values = json.loads(images.iloc[input_image_id]['bands_min_max'])\n",
    "# Create composite\n",
    "image = ee_collection_specifics.Composite(collections[0])(init_date_new, end_date_new)\n",
    "\n",
    "# Normalize images\n",
    "if bool(values): \n",
    "    image = normalize_ee_images(image, collections[0], values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select bands and convert them into float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.select(bands[0]).float()\n",
    "image.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outputs**\n",
    "\n",
    "The output (which you also need to know)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = int(versions['kernel_size'].iloc[version_id])\n",
    "if kernel_size == 1:\n",
    "    input_tile_size = [1, 1]\n",
    "    input_overlap_size = [0, 0]\n",
    "if kernel_size >1 :\n",
    "    input_tile_size = [144, 144]\n",
    "    input_overlap_size = [8, 8]\n",
    "    \n",
    "# Load the trained model and use it for prediction.\n",
    "model = ee.Model.fromAiPlatformPredictor(\n",
    "    projectName = project_id,\n",
    "    modelName = model_name,\n",
    "    version = version_name,\n",
    "    inputTileSize = input_tile_size,\n",
    "    inputOverlapSize = input_overlap_size,\n",
    "    proj = ee.Projection('EPSG:4326').atScale(scale),\n",
    "    fixInputProj = True,\n",
    "    outputBands = {'prediction': {\n",
    "        'type': ee.PixelType.float(),\n",
    "        'dimensions': 1,\n",
    "      }                  \n",
    "    }\n",
    ")\n",
    "predictions = model.predictImage(image.toArray()).arrayFlatten([bands[1]])\n",
    "predictions.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the prediction area with the polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip the prediction area with the polygon\n",
    "polygon = ee.Geometry.Polygon(geometry.attributes.get('geojson').get('features')[0].get('geometry').get('coordinates'))\n",
    "predictions = predictions.clip(polygon)\n",
    "\n",
    "# Get centroid\n",
    "centroid = polygon.centroid().getInfo().get('coordinates')[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentate image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_output == 'segmentation':\n",
    "    maxValues = predictions.reduce(ee.Reducer.max())\n",
    "\n",
    "    predictions = predictions.addBands(maxValues)\n",
    "\n",
    "    expression = \"\"\n",
    "    for n, band in enumerate(bands[1]):\n",
    "        expression = expression + f\"(b('{band}') == b('max')) ? {str(n+1)} : \"\n",
    "\n",
    "    expression = expression + f\"0\"\n",
    "\n",
    "    segmentation = predictions.expression(expression)\n",
    "    predictions = predictions.addBands(segmentation.mask(segmentation).select(['constant'], ['categories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display**\n",
    "\n",
    "Use folium to visualize the input imagery and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 1})\n",
    "Map = folium.Map(location=centroid, zoom_start=10)\n",
    "folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(Map)\n",
    "\n",
    "for band in bands[1]:\n",
    "    mapid = predictions.getMapId({'bands': [band], 'min': 0, 'max': 1})\n",
    "    \n",
    "    folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name=band,\n",
    "      ).add_to(Map)\n",
    "\n",
    "\n",
    "if model_output == 'segmentation':\n",
    "    mapid = predictions.getMapId({'bands': ['categories'], 'min': 1, 'max': len(bands[1])})\n",
    "    \n",
    "    folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name='categories',\n",
    "      ).add_to(Map)\n",
    "    \n",
    "Map.add_child(folium.LayerControl())\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
