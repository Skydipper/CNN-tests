{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trining and Prediction with Keras in AI Platform\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/cloud-samples-data/ml-engine/census/keras-tensorflow-cmle.png\" alt=\"Keras, TensorFlow, and AI Platform logos\" width=\"300px\">\n",
    "\n",
    "This notebook collection is inspired by the tutorial [Getting started: Training and prediction with Keras](https://cloud.google.com/ml-engine/docs/tensorflow/getting-started-keras)\n",
    "\n",
    "## [Recommended project structure](https://cloud.google.com/ml-engine/docs/tensorflow/packaging-trainer#to_include_additional_pypi_dependencies)\n",
    "\n",
    "To train models with AI Platform, you must upload your code and any dependencies into a Cloud Storage bucket that your Google Cloud Platform project can access.\n",
    "\n",
    "You can structure your training application in any way you like. However, the following structure is commonly used in AI Platform samples, and having your project's organization be similar to the samples can make it easier for you to follow the samples.\n",
    "\n",
    "- Use a main project directory, containing your `setup.py` file.\n",
    "\n",
    "    `setup.py` contains the package's standard dependencies.\n",
    "\n",
    "- Use a subdirectory named `trainer` to store your main application module.\n",
    "\n",
    "- Name your main application module `task.py`.\n",
    "\n",
    "- Create whatever other subdirectories in your main project directory that you need to implement your application.\n",
    "\n",
    "- Create an `__init__.py` file in every subdirectory. These files are used by Setuptools to identify directories with code to package, and may be empty.\n",
    "\n",
    "In the AI Platform samples, the `trainer` directory usually contains the following source files:\n",
    "\n",
    "- `task.py` contains the application logic that manages the training job.\n",
    "\n",
    "- `model.py` contains the TensorFlow graph codeâ€”the logic of the model.\n",
    "\n",
    "- `util.py` if present, contains code to run the training application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using Keras with tf.data from numpy arrays on the memory\n",
    "### Training application structure\n",
    "\n",
    "The training code is located in the `./AI_Platform/tf-keras/numpy` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `util.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads data into preprocessed (train_x, train_y, eval_x, eval_y) dataframes.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (train_x, train_y, eval_x, eval_y), where train_x and eval_x are\n",
    "        numpy arrays with features for training and train_y and eval_y are\n",
    "        numpy arrays with the corresponding labels.\n",
    "    \"\"\"\n",
    "    # Load image data from MNIST.\n",
    "    (train_x, train_y),(eval_x, eval_y) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # We convert the input data to (60000, 28, 28, 1), float32 and normalize our data values to the range [0, 1].\n",
    "    train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], train_x.shape[2], 1)\n",
    "    eval_x = eval_x.reshape(eval_x.shape[0], eval_x.shape[1], eval_x.shape[2], 1)\n",
    "\n",
    "    train_x = train_x.astype('float32')\n",
    "    eval_x = eval_x.astype('float32')\n",
    "    train_x /= 255\n",
    "    eval_x /= 255\n",
    "\n",
    "    # Preprocess class labels \n",
    "    train_y = train_y.astype(np.int32)\n",
    "    eval_y = eval_y.astype(np.int32)\n",
    "\n",
    "    train_y = np_utils.to_categorical(train_y, 10)\n",
    "    eval_y = np_utils.to_categorical(eval_y, 10)\n",
    "\n",
    "    return train_x, train_y, eval_x, eval_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defines a Keras model and input function for training.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras import Model\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation, Flatten # Keras core layes\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D # Keras CNN layers\n",
    "\n",
    "\n",
    "def input_fn(features, labels, shuffle, num_epochs, batch_size):\n",
    "    \"\"\"Generates an input function to be used for model training.\n",
    "\n",
    "    Args:\n",
    "      features: numpy array of features used for training or inference\n",
    "      labels: numpy array of labels for each example\n",
    "      shuffle: boolean for whether to shuffle the data or not (set True for\n",
    "        training, False for evaluation)\n",
    "      num_epochs: number of epochs to provide the data for\n",
    "      batch_size: batch size for training\n",
    "\n",
    "    Returns:\n",
    "      A tf.data.Dataset that can provide data to the Keras model for training or\n",
    "        evaluation\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(features))\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def create_keras_model(inputShape, learning_rate):\n",
    "    # Model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "   \n",
    "    # Convolutional layers    \n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "  \n",
    "    # Fully connected Dense layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "  \n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "  \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='myModel')\n",
    "  \n",
    "    # Custom Optimizer:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate)\n",
    "\n",
    "    # Compile Keras model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer, metrics=['accuracy'])\n",
    "      \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains a Keras model to predict handwritten numbers from MNIST data.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "from . import util\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='local or GCS location for writing checkpoints and exporting models')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='number of times to go through the data, default=20')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help='number of records to read during each training step, default=128')\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        default=.01,\n",
    "        type=float,\n",
    "        help='learning rate for gradient descent, default=.01')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='INFO')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "\n",
    "    Args:\n",
    "      args: dictionary of arguments - see get_args() for details\n",
    "    \"\"\"\n",
    "\n",
    "    train_x, train_y, eval_x, eval_y = util.load_data()\n",
    "\n",
    "    # dimensions\n",
    "    num_train_examples = train_x.shape[0]\n",
    "    inputShape = train_x.shape[1:]\n",
    "    num_eval_examples = eval_x.shape[0]\n",
    "\n",
    "    # Create the Keras Model\n",
    "    keras_model = model.create_keras_model(\n",
    "        inputShape = inputShape, learning_rate=args.learning_rate)\n",
    "\n",
    "    # Pass a numpy array \n",
    "    training_dataset = model.input_fn(\n",
    "        features=train_x,\n",
    "        labels=train_y,\n",
    "        shuffle=True,\n",
    "        num_epochs=args.num_epochs,\n",
    "        batch_size=args.batch_size)\n",
    "\n",
    "    # Pass a numpy array \n",
    "    validation_dataset = model.input_fn(\n",
    "        features=eval_x,\n",
    "        labels=eval_y,\n",
    "        shuffle=False,\n",
    "        num_epochs=args.num_epochs,\n",
    "        batch_size=num_eval_examples)\n",
    "\n",
    "    # Setup Learning Rate decay.\n",
    "    lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch: args.learning_rate + 0.02 * (0.5 ** (1 + epoch)),\n",
    "        verbose=True)\n",
    "\n",
    "     # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "        os.path.join(args.job_dir, 'keras_tensorboard'),\n",
    "        histogram_freq=1)\n",
    "\n",
    "    # Train model\n",
    "    keras_model.fit(\n",
    "        training_dataset,\n",
    "        steps_per_epoch=int(num_train_examples / args.batch_size),\n",
    "        epochs=args.num_epochs,\n",
    "        validation_data=validation_dataset,\n",
    "        validation_steps=1,\n",
    "        verbose=1,\n",
    "        callbacks=[lr_decay_cb, tensorboard_cb])\n",
    "\n",
    "    export_path = tf.contrib.saved_model.save_keras_model(\n",
    "        keras_model, os.path.join(args.job_dir, 'keras_export'))\n",
    "    export_path = export_path.decode('utf-8')\n",
    "    print('Model exported to: ', export_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    tf.logging.set_verbosity(args.verbosity)\n",
    "    train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Keras with tf.data from TFRecord files on Google Cloud Store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a TFRecord file**\n",
    "\n",
    "Creating your dataset is pretty straightforward. All you need to do is to define your dataset using something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Helperfunctions to make your feature definition more readable\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _float_feature(array):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=array))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def writeTFRecord(filename, image, label):\n",
    "    \"\"\"\n",
    "    filename : string\n",
    "        TFRecord file name. \n",
    "    image : float\n",
    "         Input numpy array.\n",
    "    image : int\n",
    "         Output numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create filewriter\n",
    "    options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    writer = tf.python_io.TFRecordWriter(filename, options=options)\n",
    "    \n",
    "    for i in range(label.shape[0]):\n",
    "        \n",
    "        # Define the features of your tfrecord\n",
    "        #feature = {'image':  _bytes_feature(tf.compat.as_bytes(image[i].tostring())),\n",
    "        #           'label':  _int64_feature(int(label[i]))}\n",
    "        \n",
    "        feature = {'image':  _float_feature(image[i]),\n",
    "                   'label':  _int64_feature(int(label[i]))}\n",
    "        \n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "        # Serialize to string and write to file\n",
    "        writer.write(example.SerializeToString())\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data should have 1D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nData_train = x_train.shape[0]\n",
    "nData_test = x_test.shape[0]\n",
    "nDimIn = x_train.shape[1]*x_train.shape[2]\n",
    "input_shape = (x_train.shape[1], x_train.shape[2])\n",
    "\n",
    "\n",
    "x_train_vec = x_train.reshape([nData_train,-1])\n",
    "x_test_vec = x_test.reshape([nData_test,-1])\n",
    "\n",
    "print(x_train_vec.shape, x_test_vec.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save `tfrecords`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_tfrecords = './data/mnist_train.tfrecords'\n",
    "path_test_tfrecords = './data/mnist_test.tfrecords'\n",
    "\n",
    "writeTFRecord(path_train_tfrecords, x_train_vec, y_train)\n",
    "writeTFRecord(path_test_tfrecords, x_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uploading objects to Cloud Storage bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name, privatekey_path):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client.from_service_account_json(privatekey_path)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print('File {} uploaded to {}.'.format(\n",
    "        source_file_name,\n",
    "        destination_blob_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'skydipper_materials'\n",
    "privatekey_path =  \"/Users/ikersanchez/Vizzuality/Keys/Skydipper/skydipper-562ee3e31bb2.json\" #\"/Users/keys/privatekey.json\"\n",
    "\n",
    "source_file_name = './data/mnist_train.tfrecords'\n",
    "destination_blob_name = 'mist_tfrecords/mnist_train.tfrecords'\n",
    "upload_blob(bucket_name, source_file_name, destination_blob_name, privatekey_path)\n",
    "\n",
    "source_file_name = './data/mnist_test.tfrecords'\n",
    "destination_blob_name = 'mist_tfrecords/mnist_test.tfrecords'\n",
    "upload_blob(bucket_name, source_file_name, destination_blob_name, privatekey_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Reading a single record from TFRecord files](https://www.tensorflow.org/tutorials/load_data/tf_records#reading_a_tfrecord_file_2)**\n",
    "\n",
    "The following example imports the data as is, as a tf.Example message. This can be useful to verify that a the file contains the data that we expect. This can also be useful if the input data is stored as TFRecords but you would prefer to input NumPy data (or some other input data type), for example [here](https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays), since this example allows us to read the values themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'skydipper_materials'\n",
    "fullpath_train = 'gs://'+bucket_name+'/'+'mist_tfrecords/mnist_train.tfrecords'\n",
    "fullpath_test = 'gs://'+bucket_name+'/'+'mist_tfrecords/mnist_test.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-42a7450c3d08>:2: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "features {\n",
      "  feature {\n",
      "    key: \"image\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 84.0\n",
      "        value: 185.0\n",
      "        value: 159.0\n",
      "        value: 151.0\n",
      "        value: 60.0\n",
      "        value: 36.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 222.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 241.0\n",
      "        value: 198.0\n",
      "        value: 198.0\n",
      "        value: 198.0\n",
      "        value: 198.0\n",
      "        value: 198.0\n",
      "        value: 198.0\n",
      "        value: 198.0\n",
      "        value: 198.0\n",
      "        value: 170.0\n",
      "        value: 52.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 67.0\n",
      "        value: 114.0\n",
      "        value: 72.0\n",
      "        value: 114.0\n",
      "        value: 163.0\n",
      "        value: 227.0\n",
      "        value: 254.0\n",
      "        value: 225.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 250.0\n",
      "        value: 229.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 140.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 17.0\n",
      "        value: 66.0\n",
      "        value: 14.0\n",
      "        value: 67.0\n",
      "        value: 67.0\n",
      "        value: 67.0\n",
      "        value: 59.0\n",
      "        value: 21.0\n",
      "        value: 236.0\n",
      "        value: 254.0\n",
      "        value: 106.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 83.0\n",
      "        value: 253.0\n",
      "        value: 209.0\n",
      "        value: 18.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 22.0\n",
      "        value: 233.0\n",
      "        value: 255.0\n",
      "        value: 83.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 129.0\n",
      "        value: 254.0\n",
      "        value: 238.0\n",
      "        value: 44.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 59.0\n",
      "        value: 249.0\n",
      "        value: 254.0\n",
      "        value: 62.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 133.0\n",
      "        value: 254.0\n",
      "        value: 187.0\n",
      "        value: 5.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 9.0\n",
      "        value: 205.0\n",
      "        value: 248.0\n",
      "        value: 58.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 126.0\n",
      "        value: 254.0\n",
      "        value: 182.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 75.0\n",
      "        value: 251.0\n",
      "        value: 240.0\n",
      "        value: 57.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 19.0\n",
      "        value: 221.0\n",
      "        value: 254.0\n",
      "        value: 166.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 3.0\n",
      "        value: 203.0\n",
      "        value: 254.0\n",
      "        value: 219.0\n",
      "        value: 35.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 38.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 77.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 31.0\n",
      "        value: 224.0\n",
      "        value: 254.0\n",
      "        value: 115.0\n",
      "        value: 1.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 133.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 52.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 61.0\n",
      "        value: 242.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 52.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 121.0\n",
      "        value: 254.0\n",
      "        value: 254.0\n",
      "        value: 219.0\n",
      "        value: 40.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 121.0\n",
      "        value: 254.0\n",
      "        value: 207.0\n",
      "        value: 18.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "        value: 0.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"label\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 7\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "record_iterator = tf.python_io.tf_record_iterator(path=fullpath_test, options=options)\n",
    "\n",
    "for string_record in record_iterator:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "    \n",
    "    print(example)\n",
    "    \n",
    "    # Exit after 1 iteration as this is purely demonstrative.\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Recover the images from the TFRecord file](https://www.tensorflow.org/tutorials/load_data/tf_records#read_the_tfrecord_file)**\n",
    "\n",
    "We now have the file `images.tfrecords`. We can now iterate over the records in the file to read back what we wrote. Since, for our use case we will just reproduce the image, the only feature we need is the image string. We can extract that using the getters described above, namely `example.features.feature['image'].bytes_list.value[0]`. We also use the labels to determine the number of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(proto):\n",
    "    # define your tfrecord again. Remember that you saved your image as a string.\n",
    "    #features = {'image': tf.FixedLenFeature([], tf.string),\n",
    "    #            'label': tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "    features = {'image': tf.FixedLenFeature([], tf.float32),\n",
    "                'label': tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.parse_single_example(proto, features)\n",
    "    \n",
    "    # Turn your saved image string into an array\n",
    "    #image = tf.decode_raw(parsed_features['image'], tf.float32)\n",
    "    image = parsed_features['image']\n",
    "    \n",
    "    # Normalize\n",
    "    image = tf.divide(image, 255.0)\n",
    "    \n",
    "    # Bring your picture back in shape\n",
    "    #image = tf.reshape(image, [input_shape[0], input_shape[1], 1])\n",
    "    \n",
    "    # Create a one hot array for your labels\n",
    "    label = tf.one_hot(parsed_features['label'], 10)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((28, 28, 1), (10,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This works with arrays as well\n",
    "dataset = tf.data.TFRecordDataset(fullpath_train, compression_type='GZIP')\n",
    "    \n",
    "# Maps the preprocessing function. You can set the number of parallel loaders here\n",
    "dataset = dataset.map(parse_function, num_parallel_calls=8)\n",
    "    \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {image: (28, 28, 1), label: (10,)}, types: {image: tf.float32, label: tf.float32}>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset = tf.data.TFRecordDataset(fullpath_train, compression_type='GZIP')\n",
    "\n",
    "\n",
    "def parse_function(proto):\n",
    "    # define your tfrecord again. Remember that you saved your image as a string.  \n",
    "    features = {'image': tf.FixedLenFeature([28, 28, 1], tf.float32),\n",
    "                'label': tf.FixedLenFeature([], tf.int64)}\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed_features = tf.parse_single_example(proto, features)\n",
    "\n",
    "    # Normalize image\n",
    "    parsed_features['image'] = tf.divide(parsed_features['image'], 255.0)\n",
    "    \n",
    "    # Create a one hot array for your labels\n",
    "    parsed_features['label'] = tf.one_hot(parsed_features['label'], 10)\n",
    "    \n",
    "    return parsed_features#image, label\n",
    "\n",
    "dataset = image_dataset.map(parse_function)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover the images from the TFRecord file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for image_features in dataset:\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = tf.data.TFRecordDataset(fullpath_train, compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "n = sum(1 for _ in tf.python_io.tf_record_iterator(fullpath_test, options=options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Image:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13193c2b0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for image_features in dataset:\n",
    "    image = image_features['image'].numpy()\n",
    "    label = image_features['label'].numpy()\n",
    "    \n",
    "    break\n",
    "    \n",
    "print('Label:', label)\n",
    "print('Image:')\n",
    "plt.imshow(image[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training application structure\n",
    "\n",
    "The training code is located in the `./AI_Platform/tf-keras/TFRecords` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `util.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def parse_function(proto):\n",
    "    # define your tfrecord again. Remember that you saved your image as a string.  \n",
    "    features = {'image': tf.FixedLenFeature([28, 28, 1], tf.float32),\n",
    "                'label': tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.parse_single_example(proto, features)\n",
    "    \n",
    "    # Turn your saved image string into an array\n",
    "    image = parsed_features['image']\n",
    "    \n",
    "    # Normalize\n",
    "    image = tf.divide(image, 255.0)\n",
    "    \n",
    "    # Create a one hot array for your labels\n",
    "    label = tf.one_hot(parsed_features['label'], 10)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def create_dataset(filepath, shuffle_size, num_epochs, batch_size):\n",
    "    \n",
    "    # This works with arrays as well\n",
    "    dataset = tf.data.TFRecordDataset(filepath, compression_type='GZIP')\n",
    "    \n",
    "    # Maps the preprocessing function. You can set the number of parallel loaders here\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=8)\n",
    "    \n",
    "    # This dataset will go on forever\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # Set the number of datapoints you want to load and shuffle \n",
    "    dataset = dataset.shuffle(shuffle_size)\n",
    "    \n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    \n",
    "    # Set the batchsize\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    # Create an iterator\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defines a Keras model for training.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras import Model\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation, Flatten # Keras core layes\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D # Keras CNN layers\n",
    "\n",
    "def create_keras_model(inputShape, learning_rate):\n",
    "    # Model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "   \n",
    "    # Convolutional layers    \n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "  \n",
    "    # Fully connected Dense layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "  \n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "  \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='myModel')\n",
    "  \n",
    "    # Custom Optimizer:\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate)\n",
    "\n",
    "    # Compile Keras model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer, metrics=['accuracy'])\n",
    "      \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains a Keras model to predict handwritten numbers from MNIST data.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "from . import util\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='local or GCS location for writing checkpoints and exporting models')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='number of times to go through the data, default=20')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help='number of records to read during each training step, default=128')\n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        default=.01,\n",
    "        type=float,\n",
    "        help='learning rate for gradient descent, default=.01')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='INFO')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "\n",
    "    Args:\n",
    "      args: dictionary of arguments - see get_args() for details\n",
    "    \"\"\"\n",
    "    \n",
    "    training_filepath = 'gs://skydipper_materials/mist_tfrecords/mnist_train.tfrecords'\n",
    "    validation_filepath = 'gs://skydipper_materials/mist_tfrecords/mnist_test.tfrecords'\n",
    "    \n",
    "    # dimensions\n",
    "    options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    num_train_examples = sum(1 for _ in tf.python_io.tf_record_iterator(training_filepath, options=options))\n",
    "    num_eval_examples = sum(1 for _ in tf.python_io.tf_record_iterator(validation_filepath, options=options))\n",
    "\n",
    "    # Create the Keras Model\n",
    "    keras_model = model.create_keras_model(\n",
    "        inputShape = (28,28,1), learning_rate=args.learning_rate)\n",
    "\n",
    "    # Pass a tfrecord\n",
    "    training_dataset = util.create_dataset(\n",
    "        filepath = training_filepath, \n",
    "        shuffle_size = num_train_examples, \n",
    "        num_epochs = args.num_epochs, \n",
    "        batch_size = args.batch_size)\n",
    "\n",
    "    # Pass a tfrecord\n",
    "    validation_dataset = util.create_dataset(\n",
    "        filepath = validation_filepath, \n",
    "        shuffle_size = num_eval_examples, \n",
    "        num_epochs = args.num_epochs, \n",
    "        batch_size = args.batch_size)\n",
    "\n",
    "    # Setup Learning Rate decay.\n",
    "    lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch: args.learning_rate + 0.02 * (0.5 ** (1 + epoch)),\n",
    "        verbose=True)\n",
    "\n",
    "     # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "        os.path.join(args.job_dir, 'keras_tensorboard'),\n",
    "        histogram_freq=1)\n",
    "\n",
    "    # Train model\n",
    "    keras_model.fit(\n",
    "        training_dataset,\n",
    "        steps_per_epoch=int(num_train_examples / args.batch_size),\n",
    "        epochs=args.num_epochs,\n",
    "        validation_data=validation_dataset,\n",
    "        validation_steps=1,\n",
    "        verbose=1,\n",
    "        callbacks=[lr_decay_cb, tensorboard_cb])\n",
    "\n",
    "    export_path = tf.contrib.saved_model.save_keras_model(\n",
    "        keras_model, os.path.join(args.job_dir, 'keras_export'))\n",
    "    export_path = export_path.decode('utf-8')\n",
    "    print('Model exported to: ', export_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    tf.logging.set_verbosity(args.verbosity)\n",
    "    train_and_evaluate(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
