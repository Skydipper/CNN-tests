{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple deep multi-layer perceptron Sentinel 2 for the classification of the urban and agriculture areas\n",
    "\n",
    "This notebook walks you through a simple example of using Earth Engine and Keras.\n",
    "\n",
    "Specifically, we will train a neural network to recognize land, water, urbar, and cropland pixels in a Sentinel 2 scene ([gee](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2)). \n",
    "For this simple example we will use the output of the USDA NASS Cropland Data Layers ([gee](https://developers.google.com/earth-engine/datasets/catalog/USDA_NASS_CDL)) as training data.\n",
    "\n",
    "## Configure the Environment\n",
    "\n",
    "We begin by importing a number of useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from functools import reduce\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Earth Engine client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, region, Vizz = None):\n",
    "    \"\"\"\n",
    "    Displays images in notebook\n",
    "    \"\"\" \n",
    "    ## Visualization\n",
    "    if Vizz:\n",
    "        image = image.visualize(**Vizz)\n",
    "        \n",
    "    visual = Image(url=image.getThumbUrl({\n",
    "                'region':region\n",
    "                }))\n",
    "    \n",
    "    display(visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CloudMaskS2(image):\n",
    "    \"\"\"\n",
    "    European Space Agency (ESA) clouds from 'QA60', i.e. Quality Assessment band at 60m\n",
    "    parsed by Nick Clinton\n",
    "    \"\"\"\n",
    "    qa = image.select('QA60')\n",
    "\n",
    "    # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "    cloudBitMask = int(2**10)\n",
    "    cirrusBitMask = int(2**11)\n",
    "\n",
    "    # Both flags set to zero indicates clear conditions.\n",
    "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(\\\n",
    "            qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "\n",
    "    return image.updateMask(mask).divide(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CloudFreeCompositeS2(Collection_id, startDate, stopDate, geom):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection(Collection_id)\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterBounds(geom).filterDate(startDate,stopDate)\\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\\\n",
    "            .map(CloudMaskS2)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    return composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel 2  \n",
    "### Sentinel-2 MSI: MultiSpectral Instrument, Level-1C ([gee](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2))\n",
    "**Dataset Availability**: 2015-06-23T00:00:00 - Present\n",
    "\n",
    "**Wavebands**\n",
    "\n",
    "|Band \t|Use \t\t|Wavelength (nm) |Resolution (m)|\n",
    "|-------|-----------|----------------|--------------|\n",
    "|B1 \t|Aerosols \t|443 \t|60|\n",
    "|B2 \t|Blue \t\t|490 \t|10|\n",
    "|B3 \t|Green \t\t|560 \t|10|\n",
    "|B4 \t|Red \t\t|665 \t|10|\n",
    "|B6 \t|Red Edge 2 |740 \t|20|\n",
    "|B8 \t|NIR        |835 \t|10|\n",
    "|B8a \t|Red Edge 4 |865 \t|20|\n",
    "|B9 \t|Water vapor|940 \t|60|\n",
    "|B10 \t|Cirrus \t|1375 \t|60|\n",
    "|B11 \t|SWIR 1 \t|1610 \t|20|\n",
    "|B12 \t|SWIR 2 \t|2190 \t|20|\n",
    "|QA60   |ESA Cloud  | n/a   |60|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEE Image Collection ID\n",
    "Collection_id = 'COPERNICUS/S2'\n",
    "# Start and stop of time series\n",
    "startDate = ee.Date('2016-01-01')\n",
    "stopDate  = ee.Date('2016-12-31')\n",
    "# Scale in meters\n",
    "scale = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cloud Free Composite**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/api/thumb?thumbid=c3fdeb7d39daada2e6b7c317e4f02f45&token=26f3ff7b5f8874b7ab069a7a071f3e76\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Area of Interest (AoI)\n",
    "geom = ee.Geometry.Point(-112.4007, 43.1805).buffer(12000)\n",
    "region = geom.bounds().getInfo()['coordinates']\n",
    "# Visualization parameters\n",
    "vis = {'min':0,'max':0.3, 'bands':['B4','B3','B2']}\n",
    "# Cloud Free Composite\n",
    "image = CloudFreeCompositeS2(Collection_id, startDate, stopDate, geom)\n",
    "# Display Composite\n",
    "display_image(image, region, Vizz = vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/api/thumb?thumbid=4a5d1141cc831129a866fed2558ef483&token=58c6a1f18a9c27112ee1986ce5bc0e75\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization parameters\n",
    "vis = {'min':0,'max':0.5, 'gamma':1.5, 'bands':['B8']}\n",
    "# Display Composite\n",
    "display_image(image, region, Vizz = vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDVI = (RED-NIR)/(RED+NIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/api/thumb?thumbid=b2908927950e14ed8a4ee20fd727d004&token=e153eefce8d7499afa158cf4e368952e\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization parameters\n",
    "palette = ['blue', 'white', 'green']\n",
    "vis = {'min': -0.8, 'max': 0.8, 'bands':'nd', 'palette': palette}\n",
    "# Calculate NDVI\n",
    "image_ndvi = image.normalizedDifference(['B8','B4'])\n",
    "# Display NDVI\n",
    "display_image(image_ndvi, region, Vizz = vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDWI = (GREEN-NIR)/(GREEN+NIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/api/thumb?thumbid=a4538e1f9b17108d9785553f3b162016&token=02a6fcd3b73f5bae6fa5ee7eb8e35032\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization parameters\n",
    "palette = ['blue', 'white', 'green']\n",
    "vis = {'min': -0.8, 'max': 0.8, 'bands':'nd', 'palette': palette}\n",
    "# Calculate NDWI\n",
    "image_ndwi = image.normalizedDifference(['B8','B3'])\n",
    "# Display NDWI\n",
    "display_image(image_ndwi, region, Vizz = vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropland Data Layers\n",
    "### USDA NASS Cropland Data Layers ([gee](https://developers.google.com/earth-engine/datasets/catalog/USDA_NASS_CDL))\n",
    "\n",
    "**Dataset Availability**: January 1997 - Present\n",
    "\n",
    "**Resolution**\n",
    "30 meters\n",
    "\n",
    "**Bands**\n",
    "\n",
    "|Name \t    |Min|Max |Description \t|\n",
    "|-----------|---|----|--------------|\n",
    "|cropland \t|1 \t|254 |Main crop-specific land cover classification.|\n",
    "|cultivated |1 \t|2   |Classification layer for identifying cultivated and non-cultivated land cover. Available from 2013 to 2017.|\n",
    "|confidence |0 \t|100 |Per-pixel predicted confidence of the given classification, with 0 being the least confident and 100 the most confident.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEE Image Collection ID\n",
    "Collection_id = 'USDA/NASS/CDL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth land cover classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ee.ImageCollection(Collection_id)\\\n",
    "    .filterBounds(geom)\\\n",
    "    .filterDate(startDate,stopDate)\n",
    "\n",
    "# First image\n",
    "image = ee.Image(dataset.first())\n",
    "\n",
    "# Choose the scale\n",
    "image =  image.reproject(crs='EPSG:4326', scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/api/thumb?thumbid=dd99c7e950ef5a5998ea83ebc1b618f6&token=0b0fa06675fd47919d515510d036f128\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis = {'min':1,'max':254, 'bands':'cropland'}\n",
    "display_image(image, region, Vizz = vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download datasets\n",
    "We download and stack datasets from two different Areas of Interest (AOIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central position of (AOIs)\n",
    "points = [[-120.7224, 37.3872], [-112.6799, 42.9816]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ee_datasets import ee_datasets\n",
    "\n",
    "for n, point in enumerate(points):\n",
    "    sentinel = ee_datasets(point = point, buffer = 10000 , startDate = startDate, stopDate = stopDate, scale = scale, collection = 'Sentinel2')\n",
    "    cropland = ee_datasets(point = point, buffer = 10000 , startDate = startDate, stopDate = stopDate, scale = scale, collection = 'CroplandDataLayers')\n",
    "    dataset_x = sentinel.read_datasets()\n",
    "    dataset_y = cropland.read_datasets()\n",
    "    if n == 0:\n",
    "        data_x = dataset_x\n",
    "        data_y = dataset_y\n",
    "    else:\n",
    "        szy1, szx1 = data_x.shape[:2]\n",
    "        szy2, szx2 = dataset_x.shape[:2]\n",
    "        if szy1 != szy2 or szx1 != szx2:\n",
    "            szy = min(szy1, szy2)\n",
    "            szx = min(szx1, szx2)\n",
    "            \n",
    "            data_x = np.stack((data_x[:szy,:szx,:], dataset_x[:szy,:szx,:]), axis=0)\n",
    "            data_y = np.stack((data_y[:szy,:szx,:], dataset_y[:szy,:szx,:]), axis=0)\n",
    "        else:\n",
    "            data_x = np.stack((data_x, dataset_x), axis=0)\n",
    "            data_y = np.stack((data_y, dataset_y), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display channels**\n",
    "\n",
    "We display the input and output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_channels(data, nChannels, titles = False):\n",
    "    if nChannels == 1:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(data[:,:,0])\n",
    "        if titles:\n",
    "            plt.title(titles[0])\n",
    "    else:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=nChannels, figsize=(5*nChannels,5))\n",
    "        for i in range(nChannels):\n",
    "            ax = axs[i]\n",
    "            ax.imshow(data[:,:,i])\n",
    "            if titles:\n",
    "                ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel 2 composite for the for the fist AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_x[0,:,:,:], data_x.shape[3], titles=['Blue', 'Green', 'Red', 'NIR', 'NDVI', 'NDWI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth land cover classification for the for the fist AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_y[0,:,:,:], data_y.shape[3], titles=['Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel 2 composite for the for the second AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_x[1,:,:,:], data_x.shape[3], titles=['Blue', 'Green', 'Red', 'NIR', 'NDVI', 'NDWI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth land cover classification for the for the second AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_y[1,:,:,:], data_y.shape[3], titles=['Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess class labels\n",
    "\n",
    "Each class in encoded as a value in the range between 0 to 254. For training a Neural Network in Keras we have to convert the 1-dimensional class arrays to N classes-dimensional matrices. To simplify the problem here we regroup all the classes into 4 categories, namely, land, water, urban, and cropland areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area of Interest (AoI)\n",
    "geom = ee.Geometry.Point(points[0]).buffer(10000)\n",
    "# Start and stop of time series\n",
    "startDate = ee.Date('2016')\n",
    "stopDate  = ee.Date('2017')\n",
    "# Read the ImageCollection\n",
    "dataset = ee.ImageCollection('USDA/NASS/CDL')\\\n",
    "    .filterBounds(geom)\\\n",
    "    .filterDate(startDate,stopDate)\n",
    "# Get the cropland class values and names\n",
    "cropland_info = pd.DataFrame({'cropland_class_values':dataset.getInfo().get('features')[0].get('properties').get('cropland_class_values'),\n",
    "                              'cropland_class_palette':dataset.getInfo().get('features')[0].get('properties').get('cropland_class_palette'),\n",
    "                              'cropland_class_names':dataset.getInfo().get('features')[0].get('properties').get('cropland_class_names')\n",
    "                             })\n",
    "cropland_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique classes in this are is equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(data_y[:,:,:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the number of pixels by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, count = np.unique(data_y[0,:,:,0], return_counts=True)\n",
    "df = pd.DataFrame({'cropland_class_values': value, 'cropland_class_counts': count})\n",
    "df.sort_values(by='cropland_class_counts', ascending=False, inplace=True)\n",
    "df = pd.merge(df, cropland_info, how='left', on=['cropland_class_values'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new classes:\n",
    "- Land\n",
    "- Water\n",
    "- Urban\n",
    "- Croplands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values(array, class_labels, new_label):\n",
    "    array_new = np.copy(array)\n",
    "    for i in range(len(class_labels)):\n",
    "        array_new[np.where(array == class_labels[i])] = new_label\n",
    "        \n",
    "    return array_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New classes\n",
    "land = ['Shrubland', 'Barren', 'Grassland/Pasture', 'Deciduous Forest', 'Evergreen Forest', 'Mixed Forest', 'Wetlands', 'Woody Wetlands', 'Herbaceous Wetlands']\n",
    "water = ['Water', 'Open Water', 'Aquaculture']\n",
    "urban = ['Developed', 'Developed/Open Space', 'Developed/High Intensity', 'Developed/Low Intensity', 'Developed/Med Intensity']\n",
    "\n",
    "class_labels_0 = np.array(cropland_info[cropland_info['cropland_class_names'].isin(land)]['cropland_class_values'])\n",
    "class_labels_1 = np.array(cropland_info[cropland_info['cropland_class_names'].isin(water)]['cropland_class_values'])\n",
    "class_labels_2 = np.array(cropland_info[cropland_info['cropland_class_names'].isin(urban)]['cropland_class_values'])\n",
    "class_labels_3 = np.array(cropland_info[(~cropland_info['cropland_class_names'].isin(land)) & \n",
    "                                        (~cropland_info['cropland_class_names'].isin(water)) & \n",
    "                                        (~cropland_info['cropland_class_names'].isin(urban))]['cropland_class_values'])\n",
    "\n",
    "# We replace the class labels\n",
    "new_data_y = np.copy(data_y[:,:,:,0])\n",
    "new_data_y = replace_values(new_data_y, class_labels_3, 3.)\n",
    "new_data_y = replace_values(new_data_y, class_labels_2, 2.)\n",
    "new_data_y = replace_values(new_data_y, class_labels_1, 1.)\n",
    "new_data_y = replace_values(new_data_y, class_labels_0, 0.)\n",
    "\n",
    "# Convert 1-dimensional class arrays to 4-dimensional class matrices\n",
    "from keras.utils import np_utils\n",
    "new_data_y = np_utils.to_categorical(new_data_y, 4)\n",
    "data_y = new_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New classification for the for the first AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_y[0,:,:,:], data_y.shape[3], titles=['Land', 'Water', 'Urban', 'Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New classification for the for the second AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_y[1,:,:,:], data_y.shape[3], titles=['Land', 'Water', 'Urban', 'Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select input channels**\n",
    "\n",
    "We will only use NIR, NDVI, and NDWI as an input channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data_x[:,:,:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess datasets for training a Fully Connected Network (FCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    size = data.shape\n",
    "    for i in range(size[-1]):\n",
    "        mx = data[:,:,:,i].max()\n",
    "        mn = data[:,:,:,i].min()\n",
    "        \n",
    "        data[:,:,:,i] = (data[:,:,:,i]-mn)/(mx-mn)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = normalize_data(data_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resize the images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(data):\n",
    "    size = data.shape\n",
    "    new_size = []\n",
    "    new_size.append(reduce(lambda x, y: x*y, size[:-1]))\n",
    "    new_size.append(size[-1])\n",
    "    new_size = tuple(new_size)\n",
    "    return size, new_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_x, new_size_x = reshape_data(data_x)\n",
    "size_y, new_size_y = reshape_data(data_y)\n",
    "\n",
    "data_x_new = data_x.reshape(new_size_x)\n",
    "data_y_new = data_y.reshape(new_size_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomize the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_datasets(data_x, data_y):\n",
    "    t=data_x.shape[0]\n",
    "    arr_t = np.arange(t)\n",
    "    np.random.shuffle(arr_t)\n",
    "    data_x = data_x[arr_t,:]\n",
    "    data_y = data_y[arr_t,:]\n",
    "    \n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_randm, y_randm = randomize_datasets(data_x_new, data_y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(x, y, val_size=20):\n",
    "    t=x.shape[0]\n",
    "    size = int(t*((100-val_size)/100))\n",
    "    \n",
    "    xt = x[:size,:]\n",
    "    xv = x[size:,:]\n",
    "    yt = y[:size,:]\n",
    "    yv = y[size:,:]\n",
    "    \n",
    "    return xt, xv, yt, yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation = train_validation_split(x_randm, y_randm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Keras model\n",
    "\n",
    "Here we define a neural network with two hidden layers with `relu` nonlinearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "batch_size = 32\n",
    "num_bands = 3\n",
    "num_classes = 4\n",
    "epochs = 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation = 'relu', input_shape=(num_bands,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To saves the model weights after each epoch if the validation loss decreased\n",
    "checkpointer = ModelCheckpoint(filepath=\"{0}_weights.hdf5\".format('FCN'), verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_validation, y_validation), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_validation, y_validation, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_bands = 3\n",
    "num_classes = 4\n",
    "epochs = 1\n",
    "\n",
    "model = Sequential()\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation = 'relu', input_shape=(num_bands,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "model.load_weights(\"{0}_weights.hdf5\".format('FCN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we test the FCN in a AOI that is close to one of the training AOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central position of (AOIs)\n",
    "point = [-112.4336, 43.1682]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel = ee_datasets(point = point, buffer = 10000 , startDate = startDate, stopDate = stopDate, scale = scale, collection = 'Sentinel2')\n",
    "dataset_x = sentinel.read_datasets()\n",
    "cropland = ee_datasets(point = point, buffer = 10000 , startDate = startDate, stopDate = stopDate, scale = scale, collection = 'CroplandDataLayers')\n",
    "dataset_y = cropland.read_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the NIR, NDVI, NDWI channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = dataset_x[:,:,3:]\n",
    "data_y = dataset_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel 2 composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_x, data_x.shape[2], titles=['NIR', 'NDVI', 'NDWI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth land cover classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_y, data_y.shape[2], titles=['Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess class labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace the class labels\n",
    "new_data_y = np.copy(data_y[:,:,0])\n",
    "new_data_y = replace_values(new_data_y, class_labels_3, 3.)\n",
    "new_data_y = replace_values(new_data_y, class_labels_2, 2.)\n",
    "new_data_y = replace_values(new_data_y, class_labels_1, 1.)\n",
    "new_data_y = replace_values(new_data_y, class_labels_0, 0.)\n",
    "\n",
    "# Convert 1-dimensional class arrays to 4-dimensional class matrices\n",
    "from keras.utils import np_utils\n",
    "new_data_y = np_utils.to_categorical(new_data_y, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(new_data_y, new_data_y.shape[2], titles=['Land', 'Water', 'Urban', 'Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess input dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "def normalize_data(data):\n",
    "    size = data.shape\n",
    "    for i in range(size[-1]):\n",
    "        mx = data[:,:,i].max()\n",
    "        mn = data[:,:,i].min()\n",
    "        \n",
    "        data[:,:,i] = (data[:,:,i]-mn)/(mx-mn)\n",
    "    return data\n",
    "data_x_norm = normalize_data(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "data_x_norm = normalize_data(data_x)\n",
    "# Resize\n",
    "size_x, new_size_x = reshape_data(data_x_norm)\n",
    "\n",
    "x_input = data_x_norm.reshape(new_size_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output = model.predict(x_input, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resize the output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_output = y_output.reshape((size_x[0], size_x[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_y_output, data_y_output.shape[2], titles=['Land', 'Water', 'Urbar', 'Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We binarize the output taking the highest pixel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "def max_pixels(x):\n",
    "    x_new = x*0\n",
    "    max_val = np.amax(x, axis=2)\n",
    "    size = x.shape\n",
    "    for i in range(size[-1]):\n",
    "        ima = x[:,:,i]*0\n",
    "        ima[np.where(x[:,:,i] == max_val)] = 1\n",
    "        x_new[:,:,i]= ima\n",
    "\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_output_max = max_pixels(data_y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_y_output_max, data_y_output.shape[2], titles=['Land', 'Water', 'Urban', 'Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the ground truth with the prediction we find that the FCN performes quite well for the land, water, and cropland areas but it fails in detecting urban areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we test the FCN in a AOI that is very different from those used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = [19.7368, -17.9489]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Landsat_dataset import landsat_datasets\n",
    "landsat = landsat_datasets(point = point, buffer = 12000 , startDate = '2014', stopDate = '2017', scale = 30)\n",
    "dataset_x = landsat.read_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the NIR and NDVI channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_x = dataset_x[:,:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lansat 7 composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(dataset_x, dataset_x.shape[2], titles=['NIR', 'NDVI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess input dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "dataset_x_norm = normalize_data(dataset_x)\n",
    "# Resize \n",
    "size_x, new_size_x = reshape_data(dataset_x_norm)\n",
    "\n",
    "x_input = dataset_x_norm.reshape(new_size_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output = model.predict(x_input, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resize the output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_output = y_output.reshape((size_x[0], size_x[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_y_output, data_y_output.shape[2], titles=['Land', 'Water', 'Urbar', 'Cropland'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_output_max = max_pixels(data_y_output)\n",
    "display_channels(data_y_output_max, data_y_output.shape[2], titles=['Land', 'Water', 'Urbar', 'Cropland'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
