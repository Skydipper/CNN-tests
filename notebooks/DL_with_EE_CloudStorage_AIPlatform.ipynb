{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Google Earth Engine, Cloud Storage and AI Platform\n",
    "\n",
    "This notebook is inspired by the following tutorials:\n",
    "\n",
    "- [Getting started: Training and prediction with Keras](https://cloud.google.com/ml-engine/docs/tensorflow/getting-started-keras)\n",
    "- [Down to Earth with AI Platform](https://medium.com/google-earth/down-to-earth-with-ai-platform-7bc363abf4fa)\n",
    "- [Deploying to AI Platform](https://developers.google.com/earth-engine/tf_examples#deploying-to-ai-platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup software libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.202'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and initialize the Earth Engine library.\n",
    "import ee\n",
    "ee.Initialize()\n",
    "ee.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.3\n"
     ]
    }
   ],
   "source": [
    "# Folium setup.\n",
    "import folium\n",
    "print(folium.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import functools\n",
    "import json\n",
    "from pprint import pprint\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earth Engine ImageCollection attributes\n",
    "\n",
    "We define the different attributes that we will need for each Earth Engine ImageCollection all through the notebook. \n",
    "\n",
    "We include them in the `ee_collection_specifics.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ee_collection_specifics.py\n",
    "\n",
    "\"\"\"\n",
    "Information on Earth Engine collections stored here (e.g. bands, collection ids, etc.)\n",
    "\"\"\"\n",
    "\n",
    "import ee\n",
    "\n",
    "def ee_collections(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine image collection names\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': 'COPERNICUS/S2',\n",
    "        'Landsat7_SR': 'LANDSAT/LE07/C01/T1_SR',\n",
    "        'Landsat8_SR': 'LANDSAT/LC08/C01/T1_SR',\n",
    "        'CroplandDataLayers': 'USDA/NASS/CDL',\n",
    "        'NationalLandCoverDatabase': 'USGS/NLCD'\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine band names\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel2_TOA': ['B1','B2','B3','B4','B5','B6','B7','B8A','B8','B11','B12'],\n",
    "        'Landsat7_SR': ['B1','B2','B3','B4','B5','B6','B7'],\n",
    "        'Landsat8_SR': ['B1','B2','B3','B4','B5','B6','B7','B10','B11'],\n",
    "        'CroplandDataLayers': ['landcover', 'cropland', 'land', 'water', 'urban'],\n",
    "        'NationalLandCoverDatabase': ['impervious']\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands_rgb(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine rgb band names\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel2_TOA': ['B4','B3','B2'],\n",
    "        'Landsat7_SR': ['B3','B2','B1'],\n",
    "        'Landsat8_SR': ['B4', 'B3', 'B2'],\n",
    "        'CroplandDataLayers': ['landcover'],\n",
    "        'NationalLandCoverDatabase': ['impervious']\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands_normThreshold(collection):\n",
    "    \"\"\"\n",
    "    Normalization threshold percentage\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel2_TOA': {'B1': 75,'B2': 75,'B3': 75,'B4': 75,'B5': 80,'B6': 80,'B7': 80,'B8A': 80,'B8': 80,'B11': 100,'B12': 100},\n",
    "        'Landsat7_SR': {'B1': 95,'B2': 95,'B3': 95,'B4': 100,'B5': 100,'B6': 100,'B7': 100},\n",
    "        'Landsat8_SR': {'B1': 90,'B2': 95,'B3': 95,'B4': 95,'B5': 100,'B6': 100,'B7': 100,'B10': 100,'B11': 100},\n",
    "        'CroplandDataLayers': {'landcover': 100, 'cropland': 100, 'land': 100, 'water': 100, 'urban': 100},\n",
    "        'NationalLandCoverDatabase': {'impervious': 100}\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def normalize(collection):\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': True,\n",
    "        'Landsat7_SR': True,\n",
    "        'Landsat8_SR': True,\n",
    "        'CroplandDataLayers': False,\n",
    "        'NationalLandCoverDatabase': False\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def vizz_params_rgb(collection):\n",
    "    \"\"\"\n",
    "    Visualization parameters\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': {'min':0,'max':3000, 'bands':['B4','B3','B2']},\n",
    "        'Landsat7_SR': {'min':0,'max':3000, 'gamma':1.4, 'bands':['B3','B2','B1']},\n",
    "        'Landsat8_SR': {'min':0,'max':3000, 'gamma':1.4, 'bands':['B4','B3','B2']},\n",
    "        'CroplandDataLayers': {'min':0,'max':3, 'bands':['landcover']},\n",
    "        'NationalLandCoverDatabase': {'min': 0, 'max': 1, 'bands':['impervious']}\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def vizz_params(collection):\n",
    "    \"\"\"\n",
    "    Visualization parameters\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': [{'min':0,'max':1, 'bands':['B4','B3','B2']}, \n",
    "                      {'min':0,'max':1, 'bands':['B1']},\n",
    "                      {'min':0,'max':1, 'bands':['B5']},\n",
    "                      {'min':0,'max':1, 'bands':['B6']},\n",
    "                      {'min':0,'max':1, 'bands':['B7']},\n",
    "                      {'min':0,'max':1, 'bands':['B8A']},\n",
    "                      {'min':0,'max':1, 'bands':['B8']},\n",
    "                      {'min':0,'max':1, 'bands':['B11']},\n",
    "                      {'min':0,'max':1, 'bands':['B12']}],\n",
    "        'Landsat7_SR': [{'min':0,'max':1, 'gamma':1.4, 'bands':['B3','B2','B1']}, \n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B4']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B5']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B7']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B6']}],\n",
    "        'Landsat8_SR': [{'min':0,'max':1, 'gamma':1.4, 'bands':['B4','B3','B2']}, \n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B1']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B5']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B6']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B7']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B10']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B11']}],\n",
    "        'CroplandDataLayers': [{'min':0,'max':3, 'bands':['landcover']},\n",
    "                               {'min':0,'max':1, 'bands':['cropland']},\n",
    "                               {'min':0,'max':1, 'bands':['land']},\n",
    "                               {'min':0,'max':1, 'bands':['water']},\n",
    "                               {'min':0,'max':1, 'bands':['urban']}],\n",
    "        'NationalLandCoverDatabase': [{'min': 0, 'max': 1, 'bands':['impervious']}]\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "## ------------------------- Filter datasets ------------------------- ##\n",
    "## Lansat 7 Cloud Free Composite\n",
    "def CloudMaskL7sr(image):\n",
    "    qa = image.select('pixel_qa')\n",
    "    #If the cloud bit (5) is set and the cloud confidence (7) is high\n",
    "    #or the cloud shadow bit is set (3), then it's a bad pixel.\n",
    "    cloud = qa.bitwiseAnd(1 << 5).And(qa.bitwiseAnd(1 << 7)).Or(qa.bitwiseAnd(1 << 3))\n",
    "    #Remove edge pixels that don't occur in all bands\n",
    "    mask2 = image.mask().reduce(ee.Reducer.min())\n",
    "    return image.updateMask(cloud.Not()).updateMask(mask2)\n",
    "\n",
    "def CloudFreeCompositeL7(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate).map(CloudMaskL7sr)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Lansat 8 Cloud Free Composite\n",
    "def CloudMaskL8sr(image):\n",
    "    opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "    thermalBands = ['B10', 'B11']\n",
    "\n",
    "    cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
    "    cloudsBitMask = ee.Number(2).pow(5).int()\n",
    "    qa = image.select('pixel_qa')\n",
    "    mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
    "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
    "    mask2 = image.mask().reduce('min')\n",
    "    mask3 = image.select(opticalBands).gt(0).And(\n",
    "            image.select(opticalBands).lt(10000)).reduce('min')\n",
    "    mask = mask1.And(mask2).And(mask3)\n",
    "    \n",
    "    return image.updateMask(mask)\n",
    "\n",
    "def CloudFreeCompositeL8(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate).map(CloudMaskL8sr)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Sentinel 2 Cloud Free Composite\n",
    "def CloudMaskS2(image):\n",
    "    \"\"\"\n",
    "    European Space Agency (ESA) clouds from 'QA60', i.e. Quality Assessment band at 60m\n",
    "    parsed by Nick Clinton\n",
    "    \"\"\"\n",
    "    AerosolsBands = ['B1']\n",
    "    VIBands = ['B2', 'B3', 'B4']\n",
    "    RedBands = ['B5', 'B6', 'B7', 'B8A']\n",
    "    NIRBands = ['B8']\n",
    "    SWIRBands = ['B11', 'B12']\n",
    "\n",
    "    qa = image.select('QA60')\n",
    "\n",
    "    # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "    cloudBitMask = int(2**10)\n",
    "    cirrusBitMask = int(2**11)\n",
    "\n",
    "    # Both flags set to zero indicates clear conditions.\n",
    "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(\\\n",
    "            qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "\n",
    "    return image.updateMask(mask)\n",
    "\n",
    "def CloudFreeCompositeS2(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('COPERNICUS/S2')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\\\n",
    "            .map(CloudMaskS2)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Cropland Data Layers\n",
    "def CroplandData(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('USDA/NASS/CDL')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Change classes\n",
    "    land = ['65', '131', '141', '142', '143', '152', '176', '87', '190', '195']\n",
    "    water = ['83', '92', '111']\n",
    "    urban = ['82', '121', '122', '123', '124']\n",
    "    \n",
    "    classes = []\n",
    "    for n, i in enumerate([land,water,urban]):\n",
    "        a = ''\n",
    "        for m, j in enumerate(i):\n",
    "            if m < len(i)-1:\n",
    "                a = a + 'crop == '+ j + ' || '\n",
    "            else: \n",
    "                a = a + 'crop == '+ j\n",
    "        classes.append('('+a+') * '+str(n+1))\n",
    "    classes = ' + '.join(classes)\n",
    "    \n",
    "    image = image.expression(classes, {'crop': image.select(['cropland'])})\n",
    "    \n",
    "    image =image.rename('landcover')\n",
    "    \n",
    "    # Split image into 1 band per class\n",
    "    names = ['cropland', 'land', 'water', 'urban']\n",
    "    mask = image\n",
    "    for i, name in enumerate(names):\n",
    "        image = ee.Image.cat([image, mask.eq(i).rename(name)])\n",
    "     \n",
    "    return image\n",
    "\n",
    "## National Land Cover Database\n",
    "def ImperviousData(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('USGS/NLCD')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Select impervious band\n",
    "    image = image.select('impervious')\n",
    "    \n",
    "    ## Normalize to 1\n",
    "    image = image.divide(100).float()\n",
    "    \n",
    "    return image\n",
    "\n",
    "## ------------------------------------------------------------------- ##\n",
    "\n",
    "def Composite(collection):\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': CloudFreeCompositeS2,\n",
    "        'Landsat7_SR': CloudFreeCompositeL7,\n",
    "        'Landsat8_SR': CloudFreeCompositeL8,\n",
    "        'CroplandDataLayers': CroplandData,\n",
    "        'NationalLandCoverDatabase': ImperviousData,\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee_collection_specifics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite image\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'CroplandDataLayers'\n",
    "startDate = '2016-01-01'\n",
    "stopDate = '2016-12-31'\n",
    "scale = 10 #scale in meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display composite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "composite = ee_collection_specifics.Composite(collection)(startDate, stopDate)\n",
    "mapid = composite.getMapId(ee_collection_specifics.vizz_params_rgb(collection))\n",
    "\n",
    "tiles_url = EE_TILES.format(**mapid)\n",
    "\n",
    "map = folium.Map(location=[38.1623, -121.6911])\n",
    "folium.TileLayer(\n",
    "tiles=tiles_url,\n",
    "attr='Google Earth Engine',\n",
    "overlay=True,\n",
    "name=str(ee_collection_specifics.ee_bands_rgb(collection))).add_to(map)\n",
    "    \n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "We normalize the composite images to have values from 0 to 1.\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'Landsat8_SR'\n",
    "startDate = '2014-01-01'\n",
    "stopDate = '2014-12-31'\n",
    "scale = 30 #scale in meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create composite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ee_collection_specifics.Composite(collection)(startDate, stopDate)\n",
    "\n",
    "bands = ee_collection_specifics.ee_bands(collection)\n",
    "image = image.select(bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_values(image, collection, scale):\n",
    "    \n",
    "    normThreshold = ee_collection_specifics.ee_bands_normThreshold(collection)\n",
    "    \n",
    "    num = 2\n",
    "    lon = np.linspace(-180, 180, num)\n",
    "    lat = np.linspace(-90, 90, num)\n",
    "    \n",
    "    features = []\n",
    "    for i in range(len(lon)-1):\n",
    "        for j in range(len(lat)-1):\n",
    "            features.append(ee.Feature(ee.Geometry.Rectangle(lon[i], lat[j], lon[i+1], lat[j+1])))\n",
    "    \n",
    "    regReducer = {\n",
    "        'geometry': ee.FeatureCollection(features),\n",
    "        'reducer': ee.Reducer.minMax(),\n",
    "        'maxPixels': 1e10,\n",
    "        'bestEffort': True,\n",
    "        'scale':scale\n",
    "        \n",
    "    }\n",
    "    \n",
    "    values = image.reduceRegion(**regReducer).getInfo()\n",
    "    print(values)\n",
    "    \n",
    "    # Avoid outliers by taking into account only the normThreshold% of the data points.\n",
    "    regReducer = {\n",
    "        'geometry': ee.FeatureCollection(features),\n",
    "        'reducer': ee.Reducer.histogram(),\n",
    "        'maxPixels': 1e10,\n",
    "        'bestEffort': True,\n",
    "        'scale':scale\n",
    "        \n",
    "    }\n",
    "    \n",
    "    hist = image.reduceRegion(**regReducer).getInfo()\n",
    "\n",
    "    for band in list(normThreshold.keys()):\n",
    "        if normThreshold[band] != 100:\n",
    "            count = np.array(hist.get(band).get('histogram'))\n",
    "            x = np.array(hist.get(band).get('bucketMeans'))\n",
    "        \n",
    "            cumulative_per = np.cumsum(count/count.sum()*100)\n",
    "        \n",
    "            values[band+'_max'] = x[np.where(cumulative_per < normThreshold[band])][-1]\n",
    "        \n",
    "    return values\n",
    "\n",
    "def normalize_ee_images(image, collection, values):\n",
    "    \n",
    "    Bands = ee_collection_specifics.ee_bands(collection)\n",
    "       \n",
    "    # Normalize [0, 1] ee images\n",
    "    for i, band in enumerate(Bands):\n",
    "        if i == 0:\n",
    "            image_new = image.select(band).clamp(values[band+'_min'], values[band+'_max'])\\\n",
    "                                .subtract(values[band+'_min'])\\\n",
    "                                .divide(values[band+'_max']-values[band+'_min'])\n",
    "        else:\n",
    "            image_new = image_new.addBands(image.select(band).clamp(values[band+'_min'], values[band+'_max'])\\\n",
    "                                    .subtract(values[band+'_min'])\\\n",
    "                                    .divide(values[band+'_max']-values[band+'_min']))\n",
    "            \n",
    "    return image_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if ee_collection_specifics.normalize(collection):\n",
    "    # Get min man values for each band\n",
    "    values = min_max_values(image, collection, scale)\n",
    "    print(values)\n",
    "\n",
    "    # Normalize images\n",
    "    image = normalize_ee_images(image, collection, values)\n",
    "else:\n",
    "    values = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display composite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "map = folium.Map(location=[38.1623, -121.6911])\n",
    "for params in ee_collection_specifics.vizz_params(collection):\n",
    "    mapid = image.getMapId(params)\n",
    "    folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name=str(params['bands']),\n",
    "  ).add_to(map)\n",
    "    \n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create TFRecords for training\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inCollection = 'Landsat8_SR'\n",
    "outCollection = 'NationalLandCoverDatabase'#'CroplandDataLayers'\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['impervious']#['cropland', 'land', 'water', 'urban']\n",
    "startDate = '2016-01-01'\n",
    "stopDate = '2016-12-31'\n",
    "scale = 30 #scale in meters\n",
    "sampleSize = 1000 # Total sample size in each polygon.\n",
    "datasetName = 'Landsat8_Impervious'\n",
    "trainPolys = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"MultiPolygon\",\n",
    "        \"coordinates\":  [\n",
    "[[[  -122.882080078125,  40.50126945841645],[  -122.1240234375,  40.50126945841645],[  -122.1240234375,  41.008920735004885],[  -122.882080078125,  41.008920735004885],[  -122.882080078125,  40.50126945841645]]],\n",
    "[[[  -122.2283935546875,  39.00637903337455],[  -121.607666015625,  39.00637903337455],[  -121.607666015625,  39.46588451142044],[  -122.2283935546875,  39.46588451142044],[  -122.2283935546875,  39.00637903337455]]],\n",
    "[[[  -120.355224609375,  38.77978137804918],[  -119.608154296875,  38.77978137804918],[  -119.608154296875,  39.342794408952365],[  -120.355224609375,  39.342794408952365],[  -120.355224609375,  38.77978137804918]]],\n",
    "[[[  -121.90979003906249,  37.70555348721583],[  -120.9814453125,  37.70555348721583],[  -120.9814453125,  38.39764411353178],[  -121.90979003906249,  38.39764411353178],[  -121.90979003906249,  37.70555348721583]]],\n",
    "[[[  -120.03662109374999,  37.45741810262938],[  -119.1851806640625,  37.45741810262938],[  -119.1851806640625,  38.08268954483802],[  -120.03662109374999,  38.08268954483802],[  -120.03662109374999,  37.45741810262938]]],\n",
    "[[[  -120.03662109374999,  37.45741810262938],[  -119.1851806640625,  37.45741810262938],[  -119.1851806640625,  38.08268954483802],[  -120.03662109374999,  38.08268954483802],[  -120.03662109374999,  37.45741810262938]]],\n",
    "[[[  -120.03662109374999,  37.45741810262938],[  -119.1851806640625,  37.45741810262938],[  -119.1851806640625,  38.08268954483802],[  -120.03662109374999,  38.08268954483802],[  -120.03662109374999,  37.45741810262938]]],\n",
    "[[[  -112.554931640625,  33.0178760185549],[  -111.588134765625,  33.0178760185549],[  -111.588134765625,  33.78827853625996],[  -112.554931640625,  33.78827853625996],[  -112.554931640625,  33.0178760185549]]],\n",
    "[[[  -112.87353515625,  40.51379915504413],[  -111.829833984375,  40.51379915504413],[  -111.829833984375,  41.28606238749825],[  -112.87353515625,  41.28606238749825],[  -112.87353515625,  40.51379915504413]]],\n",
    "[[[  -108.19335937499999,  39.095962936305476],[  -107.1826171875,  39.095962936305476],[  -107.1826171875,  39.85915479295669],[  -108.19335937499999,  39.85915479295669],[  -108.19335937499999,  39.095962936305476]]],\n",
    "[[[  -124.25537109375,  30.86451022625836],[  -124.25537109375,  30.86451022625836],[  -124.25537109375,  30.86451022625836],[  -124.25537109375,  30.86451022625836]]],\n",
    "[[[  -106.875,  37.142803443716836],[  -105.49072265625,  37.142803443716836],[  -105.49072265625,  38.18638677411551],[  -106.875,  38.18638677411551],[  -106.875,  37.142803443716836]]],\n",
    "[[[  -117.31201171875001,  43.27720532212024],[  -116.01562499999999,  43.27720532212024],[  -116.01562499999999,  44.134913443750726],[  -117.31201171875001,  44.134913443750726],[  -117.31201171875001,  43.27720532212024]]],\n",
    "[[[  -115.7080078125,  44.69989765840318],[  -114.7412109375,  44.69989765840318],[  -114.7412109375,  45.36758436884978],[  -115.7080078125,  45.36758436884978],[  -115.7080078125,  44.69989765840318]]],\n",
    "[[[  -120.65185546875,  47.517200697839414],[  -119.33349609375,  47.517200697839414],[  -119.33349609375,  48.32703913063476],[  -120.65185546875,  48.32703913063476],[  -120.65185546875,  47.517200697839414]]],\n",
    "[[[  -119.83886718750001,  45.69083283645816],[  -118.38867187500001,  45.69083283645816],[  -118.38867187500001,  46.694667307773116],[  -119.83886718750001,  46.694667307773116],[  -119.83886718750001,  45.69083283645816]]],\n",
    "[[[  -107.09472656249999,  47.45780853075031],[  -105.84228515625,  47.45780853075031],[  -105.84228515625,  48.31242790407178],[  -107.09472656249999,  48.31242790407178],[  -107.09472656249999,  47.45780853075031]]],\n",
    "[[[  -101.57958984375,  46.93526088057719],[  -100.107421875,  46.93526088057719],[  -100.107421875,  47.945786463687185],[  -101.57958984375,  47.945786463687185],[  -101.57958984375,  46.93526088057719]]],\n",
    "[[[  -101.162109375,  44.32384807250689],[  -99.7119140625,  44.32384807250689],[  -99.7119140625,  45.22848059584359],[  -101.162109375,  45.22848059584359],[  -101.162109375,  44.32384807250689]]],\n",
    "[[[  -100.5908203125,  41.261291493919884],[  -99.25048828124999,  41.261291493919884],[  -99.25048828124999,  42.114523952464246],[  -100.5908203125,  42.114523952464246],[  -100.5908203125,  41.261291493919884]]],\n",
    "[[[  -97.9541015625,  37.142803443716836],[  -96.65771484375,  37.142803443716836],[  -96.65771484375,  38.13455657705411],[  -97.9541015625,  38.13455657705411],[  -97.9541015625,  37.142803443716836]]],\n",
    "[[[  -112.78564453124999,  32.91648534731439],[  -111.357421875,  32.91648534731439],[  -111.357421875,  33.925129700072],[  -112.78564453124999,  33.925129700072],[  -112.78564453124999,  32.91648534731439]]],\n",
    "[[[  -106.435546875,  35.15584570226544],[  -105.22705078125,  35.15584570226544],[  -105.22705078125,  36.13787471840729],[  -106.435546875,  36.13787471840729],[  -106.435546875,  35.15584570226544]]],\n",
    "[[[  -97.3828125,  32.45415593941475],[  -96.2841796875,  32.45415593941475],[  -96.2841796875,  33.22949814144951],[  -97.3828125,  33.22949814144951],[  -97.3828125,  32.45415593941475]]],\n",
    "[[[  -97.97607421875,  35.04798673426734],[  -97.00927734375,  35.04798673426734],[  -97.00927734375,  35.764343479667176],[  -97.97607421875,  35.764343479667176],[  -97.97607421875,  35.04798673426734]]],\n",
    "[[[  -97.97607421875,  35.04798673426734],[  -97.00927734375,  35.04798673426734],[  -97.00927734375,  35.764343479667176],[  -97.97607421875,  35.764343479667176],[  -97.97607421875,  35.04798673426734]]],\n",
    "[[[  -95.4052734375,  47.62097541515849],[  -94.24072265625,  47.62097541515849],[  -94.24072265625,  48.28319289548349],[  -95.4052734375,  48.28319289548349],[  -95.4052734375,  47.62097541515849]]],\n",
    "[[[  -94.19677734375,  41.27780646738183],[  -93.09814453125,  41.27780646738183],[  -93.09814453125,  42.13082130188811],[  -94.19677734375,  42.13082130188811],[  -94.19677734375,  41.27780646738183]]],\n",
    "[[[  -93.71337890625,  37.75334401310656],[  -92.6806640625,  37.75334401310656],[  -92.6806640625,  38.51378825951165],[  -93.71337890625,  38.51378825951165],[  -93.71337890625,  37.75334401310656]]],\n",
    "[[[  -90.63720703125,  34.615126683462194],[  -89.47265625,  34.615126683462194],[  -89.47265625,  35.69299463209881],[  -90.63720703125,  35.69299463209881],[  -90.63720703125,  34.615126683462194]]],\n",
    "[[[  -93.05419921875,  30.44867367928756],[  -91.77978515625,  30.44867367928756],[  -91.77978515625,  31.57853542647338],[  -93.05419921875,  31.57853542647338],[  -93.05419921875,  30.44867367928756]]],\n",
    "[[[  -90.02197265625,  44.276671273775186],[  -88.59374999999999,  44.276671273775186],[  -88.59374999999999,  44.98034238084973],[  -90.02197265625,  44.98034238084973],[  -90.02197265625,  44.276671273775186]]],\n",
    "[[[  -90.63720703125,  38.41055825094609],[  -89.49462890625,  38.41055825094609],[  -89.49462890625,  39.18117526158749],[  -90.63720703125,  39.18117526158749],[  -90.63720703125,  38.41055825094609]]],\n",
    "[[[  -87.56103515625,  35.62158189955968],[  -86.28662109375,  35.62158189955968],[  -86.28662109375,  36.4566360115962],[  -87.56103515625,  36.4566360115962],[  -87.56103515625,  35.62158189955968]]],\n",
    "[[[  -90.63720703125,  31.93351676190369],[  -89.49462890625,  31.93351676190369],[  -89.49462890625,  32.731840896865684],[  -90.63720703125,  32.731840896865684],[  -90.63720703125,  31.93351676190369]]],\n",
    "[[[  -69.54345703125,  44.68427737181225],[  -68.5107421875,  44.68427737181225],[  -68.5107421875,  45.336701909968134],[  -69.54345703125,  45.336701909968134],[  -69.54345703125,  44.68427737181225]]],\n",
    "[[[  -73.212890625,  41.49212083968776],[  -72.35595703125,  41.49212083968776],[  -72.35595703125,  42.032974332441405],[  -73.212890625,  42.032974332441405],[  -73.212890625,  41.49212083968776]]],\n",
    "[[[  -77.93701171875,  38.70265930723801],[  -76.97021484375,  38.70265930723801],[  -76.97021484375,  39.26628442213066],[  -77.93701171875,  39.26628442213066],[  -77.93701171875,  38.70265930723801]]],\n",
    "[[[  -79.25537109375,  35.44277092585766],[  -78.15673828125,  35.44277092585766],[  -78.15673828125,  36.13787471840729],[  -79.25537109375,  36.13787471840729],[  -79.25537109375,  35.44277092585766]]],\n",
    "[[[  -81.4306640625,  33.55970664841198],[  -80.44189453125,  33.55970664841198],[  -80.44189453125,  34.288991865037524],[  -81.4306640625,  34.288991865037524],[  -81.4306640625,  33.55970664841198]]],\n",
    "[[[  -84.90234375,  33.394759218577995],[  -83.91357421875,  33.394759218577995],[  -83.91357421875,  34.19817309627726],[  -84.90234375,  34.19817309627726],[  -84.90234375,  33.394759218577995]]],\n",
    "[[[  -82.28759765625,  28.246327971048842],[  -81.2548828125,  28.246327971048842],[  -81.2548828125,  29.209713225868185],[  -82.28759765625,  29.209713225868185],[  -82.28759765625,  28.246327971048842]]],\n",
    "[[[  -109.88525390624999,  42.65012181368022],[  -108.56689453125,  42.65012181368022],[  -108.56689453125,  43.50075243569041],[  -109.88525390624999,  43.50075243569041],[  -109.88525390624999,  42.65012181368022]]],\n",
    "[[[  -117.61962890624999,  39.04478604850143],[  -116.65283203124999,  39.04478604850143],[  -116.65283203124999,  39.740986355883564],[  -117.61962890624999,  39.740986355883564],[  -117.61962890624999,  39.04478604850143]]],\n",
    "[[[  -102.67822265625,  31.42866311735861],[  -101.71142578125,  31.42866311735861],[  -101.71142578125,  32.26855544621476],[  -102.67822265625,  32.26855544621476],[  -102.67822265625,  31.42866311735861]]],\n",
    "[[[  -119.47631835937499,  36.03133177633187],[  -118.58642578124999,  36.03133177633187],[  -118.58642578124999,  36.55377524336089],[  -119.47631835937499,  36.55377524336089],[  -119.47631835937499,  36.03133177633187]]],\n",
    "[[[  -116.224365234375,  33.091541548655215],[  -115.56518554687499,  33.091541548655215],[  -115.56518554687499,  33.568861182555565],[  -116.224365234375,  33.568861182555565],[  -116.224365234375,  33.091541548655215]]]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "evalPolys = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"MultiPolygon\",\n",
    "        \"coordinates\":  [\n",
    "[[[-122.13208008,   41.25126946],[-121.37402344,   41.25126946],[-121.37402344,   41.75892074],[-122.13208008,   41.75892074],[-122.13208008,   41.25126946]]],\n",
    "[[[-121.15979004,   38.45555349],[-120.23144531,   38.45555349],[-120.23144531,   39.14764411],[-121.15979004,   39.14764411],[-121.15979004,   38.45555349]]],\n",
    "[[[-111.80493164,   33.76787602],[-110.83813477,   33.76787602],[-110.83813477,   34.53827854],[-111.80493164,   34.53827854],[-111.80493164,   33.76787602]]],\n",
    "[[[-106.125     ,   37.89280344],[-104.74072266,   37.89280344],[-104.74072266,   38.93638677],[-106.125     ,   38.93638677],[-106.125     ,   37.89280344]]],\n",
    "[[[-119.08886719,   46.44083284],[-117.63867188,   46.44083284],[-117.63867188,   47.44466731],[-119.08886719,   47.44466731],[-119.08886719,   46.44083284]]],\n",
    "[[[-99.84082031,  42.01129149],[-98.50048828,  42.01129149],[-98.50048828,  42.86452395],[-99.84082031,  42.86452395],[-99.84082031,  42.01129149]]],\n",
    "[[[-96.6328125 ,  33.20415594],[-95.53417969,  33.20415594],[-95.53417969,  33.97949814],[-96.6328125 ,  33.97949814],[-96.6328125 ,  33.20415594]]],\n",
    "[[[-93.44677734,  42.02780647],[-92.34814453,  42.02780647],[-92.34814453,  42.8808213 ],[-93.44677734,  42.8808213 ],[-93.44677734,  42.02780647]]],\n",
    "[[[-89.27197266,  45.02667127],[-87.84375   ,  45.02667127],[-87.84375   ,  45.73034238],[-89.27197266,  45.73034238],[-89.27197266,  45.02667127]]],\n",
    "[[[-68.79345703,  45.43427737],[-67.76074219,  45.43427737],[-67.76074219,  46.08670191],[-68.79345703,  46.08670191],[-68.79345703,  45.43427737]]],\n",
    "[[[-80.68066406,  34.30970665],[-79.69189453,  34.30970665],[-79.69189453,  35.03899187],[-80.68066406,  35.03899187],[-80.68066406,  34.30970665]]],\n",
    "[[[-116.86962891,   39.79478605],[-115.90283203,   39.79478605],[-115.90283203,   40.49098636],[-116.86962891,   40.49098636],[-116.86962891,   39.79478605]]]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Polygons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "#  Convert the GeoJSONs to feature collections\n",
    "trainFeatures = ee.FeatureCollection(trainPolys.get('features'))\n",
    "evalFeatures = ee.FeatureCollection(evalPolys.get('features'))\n",
    "\n",
    "polyImage = ee.Image(0).byte().paint(trainFeatures, 1).paint(evalFeatures, 2)\n",
    "polyImage = polyImage.updateMask(polyImage)\n",
    "\n",
    "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
    "map = folium.Map(location=[38., -100.], zoom_start=5)\n",
    "folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='training polygons',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An array of images**\n",
    "\n",
    "We have to stack the 2D images (input and output images of the Neural Network) to create a single image from which samples can be taken. Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band. This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [neighborhoodToArray()](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_into_array(url, collections, bands, kernelSize, startDate, stopDate, scale):\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    \n",
    "    for i, collection in enumerate(collections):\n",
    "        payload =   {\n",
    "            \"collection\": collection,\n",
    "            \"start\": startDate,\n",
    "            \"end\": stopDate,\n",
    "            \"scale\": scale\n",
    "        }\n",
    "        \n",
    "        output = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "        \n",
    "        if i == 0:\n",
    "            image = ee.deserializer.fromJSON(output.json()['composite']).select(bands[i])\n",
    "        else:\n",
    "            featureStack = ee.Image.cat([image,\\\n",
    "                                         ee.deserializer.fromJSON(output.json()['composite']).select(bands[i])\\\n",
    "                                        ]).float()\n",
    "            \n",
    "    list = ee.List.repeat(1, kernelSize)\n",
    "    lists = ee.List.repeat(list, kernelSize)\n",
    "    kernel = ee.Kernel.fixed(kernelSize, kernelSize, lists)\n",
    "    \n",
    "    arrays = featureStack.neighborhoodToArray(kernel)\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = f'https://us-central1-skydipper-196010.cloudfunctions.net/ee_pre_processing'\n",
    "collections = [inCollection, outCollection]\n",
    "bands = [inBands, outBands]\n",
    "kernelSize = 256\n",
    "\n",
    "arrays = image_into_array(url, collections, bands, kernelSize, startDate, stopDate, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export TFRecords**\n",
    "\n",
    "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export. The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point. It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record. You do NOT need to export each training/testing patch to a different image. Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the computed value too large error. Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_TFRecords(arrays, scale, nShards, sampleSize, features, polysLists, baseNames, bucket, folder, selectors):\n",
    "    # Export all the training/evaluation data (in many pieces), with one task per geometry.\n",
    "    filePaths = []\n",
    "    for i, feature in enumerate(features):\n",
    "        for g in range(feature.size().getInfo()):\n",
    "            geomSample = ee.FeatureCollection([])\n",
    "            for j in range(nShards):\n",
    "                sample = arrays.sample(\n",
    "                    region = ee.Feature(polysLists[i].get(g)).geometry(), \n",
    "                    scale = scale, \n",
    "                    numPixels = sampleSize / nShards, # Size of the shard.\n",
    "                    seed = j,\n",
    "                    tileScale = 8\n",
    "                )\n",
    "                geomSample = geomSample.merge(sample)\n",
    "                \n",
    "            desc = baseNames[i] + '_g' + str(g)\n",
    "            \n",
    "            filePaths.append(bucket+ '/' + folder + '/' + desc)\n",
    "            \n",
    "            task = ee.batch.Export.table.toCloudStorage(\n",
    "                collection = geomSample,\n",
    "                description = desc, \n",
    "                bucket = bucket, \n",
    "                fileNamePrefix = folder + '/' + desc,\n",
    "                fileFormat = 'TFRecord',\n",
    "                selectors = selectors\n",
    "            )\n",
    "            task.start()\n",
    "            \n",
    "    return filePaths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeoJSONs_to_FeatureCollections(multipolygon):\n",
    "    # Make a list of Features\n",
    "    features = []\n",
    "    for i in range(len(multipolygon.get('features')[0].get('geometry').get('coordinates'))):\n",
    "        features.append(\n",
    "            ee.Feature(\n",
    "                ee.Geometry.Polygon(\n",
    "                    multipolygon.get('features')[0].get('geometry').get('coordinates')[i]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # Create a FeatureCollection from the list and print it.\n",
    "    return ee.FeatureCollection(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the GeoJSONs to feature collections\n",
    "trainFeatures = GeoJSONs_to_FeatureCollections(trainPolys)\n",
    "evalFeatures = GeoJSONs_to_FeatureCollections(evalPolys)\n",
    "\n",
    "# Convert the feature collections to lists for iteration.\n",
    "trainPolysList = trainFeatures.toList(trainFeatures.size())\n",
    "evalPolysList = evalFeatures.toList(evalFeatures.size())\n",
    "\n",
    "# These numbers determined experimentally.\n",
    "nShards  = int(sampleSize/20)#100 # Number of shards in each polygon.\n",
    "\n",
    "features = [trainFeatures, evalFeatures]\n",
    "polysLists = [trainPolysList, evalPolysList]\n",
    "baseNames = ['training_patches', 'eval_patches']\n",
    "bucket = 'skydipper_materials'\n",
    "folder = 'cnn-models/'+datasetName+'/data'\n",
    "selectors = inBands + outBands\n",
    "\n",
    "# Export all the training/evaluation data (in many pieces), with one task per geometry.\n",
    "filePaths = export_TFRecords(arrays, scale, nShards, sampleSize, features, polysLists, baseNames, bucket, folder, selectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Inspect data\n",
    "Load the data exported from Earth Engine into a tf.data.Dataset. \n",
    "\n",
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow setup.\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(proto):\n",
    "    \"\"\"The parsing function.\n",
    "    Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "    Args:\n",
    "      example_proto: a serialized Example.\n",
    "    Returns: \n",
    "      A dictionary of tensors, keyed by feature name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define your tfrecord \n",
    "    features = inBands + outBands\n",
    "    \n",
    "    # Specify the size and shape of patches expected by the model.\n",
    "    kernel_shape = [kernel_size, kernel_size]\n",
    "    columns = [\n",
    "      tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) for k in features\n",
    "    ]\n",
    "    features_dict = dict(zip(features, columns))\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.io.parse_single_example(proto, features_dict)\n",
    "\n",
    "    # Convert a dictionary of tensors to a tuple of (inputs, outputs)\n",
    "    inputsList = [parsed_features.get(key) for key in features]\n",
    "    stacked = tf.stack(inputsList, axis=0)\n",
    "    \n",
    "    # Convert the tensors into a stack in HWC shape\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    \n",
    "    return stacked[:,:,:len(inBands)], stacked[:,:,len(inBands):]\n",
    "\n",
    "def get_dataset(glob, inBands, outBands, kernel_size, buffer_size, batch_size):\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "    Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "    \"\"\"\n",
    "    glob = tf.compat.v1.io.gfile.glob(glob)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=5)\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = 'Landsat8_Impervious'#'Landsat8_Cropland'\n",
    "baseNames = ['training_patches', 'eval_patches']\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['impervious']#['cropland', 'land', 'water', 'urban']\n",
    "        \n",
    "bucket = env.bucket_name\n",
    "folder = 'cnn-models/'+datasetName+'/data'\n",
    "kernel_size = 256\n",
    "buffer_size = 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob = 'gs://' + bucket + '/' + folder + '/' + baseNames[0] + '_g0'+ '*'\n",
    "dataset = get_dataset(glob, inBands, outBands, kernel_size, buffer_size, batch_size)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the first record**\n",
    "\n",
    "Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arr = iter(dataset.take(1)).next()\n",
    "input_arr = training_arr[0].numpy()\n",
    "print(input_arr.shape)\n",
    "output_arr = training_arr[1].numpy()\n",
    "print(output_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And display the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_channels(data, nChannels, titles = False):\n",
    "    if nChannels == 1:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(data[:,:,0])\n",
    "        if titles:\n",
    "            plt.title(titles[0])\n",
    "    else:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=nChannels, figsize=(5*nChannels,5))\n",
    "        for i in range(nChannels):\n",
    "            ax = axs[i]\n",
    "            ax.imshow(data[:,:,i])\n",
    "            if titles:\n",
    "                ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(input_arr[9,:,:,:], input_arr.shape[3], titles=inBands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(output_arr[9,:,:,:], output_arr.shape[3], titles=outBands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Training the model in AI Platform\n",
    "### Training code package setup\n",
    "\n",
    "It's necessary to create a Python package to hold the training code.  Here we're going to get started with that by creating a folder for the package and adding an empty `__init__.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "-rw-r--r--  1 ikersanchez  staff  0 Jan  9 14:23 __init__.py\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = 'AI_Platform/cnn_trainer'\n",
    "PACKAGE_FOLDER = '/trainer'\n",
    "\n",
    "!rm -r {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH+PACKAGE_FOLDER}\n",
    "!touch {ROOT_PATH+PACKAGE_FOLDER}/__init__.py\n",
    "!ls -l {ROOT_PATH+PACKAGE_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Files**\n",
    "\n",
    "`env.py` file into the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp env.py {ROOT_PATH+PACKAGE_FOLDER}/env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**\n",
    "\n",
    "These variables need to be stored in a place where other code can access them.  There are a variety of ways of accomplishing that, but here we'll use the `%%writefile` command to write the contents of the code cell to a file called `config.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AI_Platform/cnn_trainer/trainer/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/config.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import env\n",
    "\n",
    "# Define your Google Cloud Storage bucket\n",
    "bucket = env.bucket_name\n",
    "\n",
    "# Specify names of output locations in Cloud Storage.\n",
    "dataset_name = 'Landsat8_Impervious'#'Landsat8_Cropland'\n",
    "job_dir = 'gs://' + bucket + '/' + 'cnn-models/'+ dataset_name +'/trainer'\n",
    "model_dir = job_dir + '/model'\n",
    "logs_dir = job_dir + '/logs'\n",
    "\n",
    "# Pre-computed training and eval data.\n",
    "base_names = ['training_patches', 'eval_patches']\n",
    "folder = 'cnn-models/'+dataset_name+'/data'\n",
    "\n",
    "# Specify inputs/outputs to the model\n",
    "in_bands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "out_bands = ['impervious']#['cropland', 'land', 'water', 'urban']\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "kernel_size = 256\n",
    "\n",
    "# Sizes of the training and evaluation datasets.\n",
    "train_size = 1000*47\n",
    "eval_size = 1000*12\n",
    "\n",
    "# Specify model training parameters.\n",
    "model_type = 'regression'\n",
    "model_architecture = 'deepvel'\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "shuffle_size = 2000\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
    "loss = 'mse'\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training/evaluation data**\n",
    "\n",
    "The following is code to load training/evaluation data.  Write this into `util.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AI_Platform/cnn_trainer/trainer/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/util.py\n",
    "\"\"\"Utilities to download and preprocess the data.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import config\n",
    "\n",
    "def parse_function(proto):\n",
    "    \"\"\"The parsing function.\n",
    "    Read a serialized example into the structure defined by features_dict.\n",
    "    Args:\n",
    "      example_proto: a serialized Example.\n",
    "    Returns: \n",
    "      A dictionary of tensors, keyed by feature name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define your tfrecord \n",
    "    features = config.in_bands + config.out_bands\n",
    "    \n",
    "    # Specify the size and shape of patches expected by the model.\n",
    "    kernel_shape = [config.kernel_size, config.kernel_size]\n",
    "    columns = [\n",
    "      tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) for k in features\n",
    "    ]\n",
    "    features_dict = dict(zip(features, columns))\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.io.parse_single_example(proto, features_dict)\n",
    "\n",
    "    # Convert a dictionary of tensors to a tuple of (inputs, outputs)\n",
    "    inputs_list = [parsed_features.get(key) for key in features]\n",
    "    stacked = tf.stack(inputs_list, axis=0)\n",
    "    \n",
    "    # Convert the tensors into a stack in HWC shape\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    \n",
    "    return stacked[:,:,:len(config.in_bands)], stacked[:,:,len(config.in_bands):]\n",
    "\n",
    "def get_dataset(glob):\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "    Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "    \"\"\"\n",
    "    glob = tf.compat.v1.io.gfile.glob(glob)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=5)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_training_dataset():\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "    Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "    \"\"\"\n",
    "    glob = 'gs://' + config.bucket + '/' + config.folder + '/' + config.base_names[0] + '*'\n",
    "    dataset = get_dataset(glob)\n",
    "    dataset = dataset.shuffle(config.shuffle_size).batch(config.batch_size).repeat()\n",
    "    return dataset\n",
    "\n",
    "def get_evaluation_dataset():\n",
    "    \"\"\"Get the preprocessed evaluation dataset\n",
    "    Returns: \n",
    "      A tf.data.Dataset of evaluation data.\n",
    "    \"\"\"\n",
    "    glob = 'gs://' + config.bucket + '/' + config.folder + '/' + config.base_names[1] + '*'\n",
    "    dataset = get_dataset(glob)\n",
    "    dataset = dataset.batch(1).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that `util.py` is functioning as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((None, 256, 256, 7), (None, 256, 256, 1)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from AI_Platform.cnn_trainer.trainer import config\n",
    "from AI_Platform.cnn_trainer.trainer import util\n",
    "\n",
    "training_dataset = util.get_training_dataset()\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "We rewrite the desired model (previously specified in `config.py`) into `model.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_Platform.cnn_trainer.trainer import config\n",
    "!cp ../models/{config.model_type}/{config.model_architecture+'.py'} {ROOT_PATH+PACKAGE_FOLDER}/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that `model.py` is functioning as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deepvel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 6 4096        image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 6 36928       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, None, 6 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, None, 6 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 6 36928       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 6 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, None, 6 0           batch_normalization_1[0][0]      \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 6 36928       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 6 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 6 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 6 36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 6 36928       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 6 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 6 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 6 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 6 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, None, 6 0           batch_normalization_5[0][0]      \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 6 36928       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 6 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 6 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 6 36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 6 256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, None, 6 0           batch_normalization_7[0][0]      \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 6 36928       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 6 36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 6 256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, None, 6 0           batch_normalization_9[0][0]      \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 6 36928       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 6 256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 6 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 6 36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 6 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, None, 6 0           batch_normalization_11[0][0]     \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 6 36928       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 6 256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, None, 6 0           batch_normalization_12[0][0]     \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output (Conv2D)                 (None, None, None, 1 65          add_6[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 487,553\n",
      "Trainable params: 485,889\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from AI_Platform.cnn_trainer.trainer import config\n",
    "from AI_Platform.cnn_trainer.trainer import model\n",
    "\n",
    "model = model.create_keras_model(inputShape = (None, None, len(config.in_bands)), nClasses = len(config.out_bands))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training task**\n",
    "\n",
    "The following will create `task.py`, which will get the training and evaluation data, train the model and save it when it's done in a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AI_Platform/cnn_trainer/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/task.py\n",
    "\"\"\"Trains a Keras model\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import config\n",
    "from . import util\n",
    "from . import model\n",
    "          \n",
    "def train_and_evaluate():\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the Keras Model\n",
    "    keras_model = model.create_keras_model(inputShape = (None, None, len(config.in_bands)), nClasses = len(config.out_bands))\n",
    "\n",
    "    # Compile Keras model\n",
    "    keras_model.compile(loss=config.loss, optimizer=config.optimizer, metrics=config.metrics)\n",
    "\n",
    "\n",
    "    # Pass a tfrecord\n",
    "    training_dataset = util.get_training_dataset()\n",
    "    evaluation_dataset = util.get_evaluation_dataset()\n",
    "\n",
    "    # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(config.logs_dir)\n",
    "\n",
    "    # Train model\n",
    "    keras_model.fit(\n",
    "        x=training_dataset,\n",
    "        steps_per_epoch=int(config.train_size / config.batch_size),\n",
    "        epochs=config.epochs,\n",
    "        validation_data=evaluation_dataset,\n",
    "        validation_steps=int(config.eval_size / config.batch_size),\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard_cb])\n",
    "\n",
    "    tf.contrib.saved_model.save_keras_model(keras_model, os.path.join(config.model_dir, str(int(time.time()))))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity('INFO')\n",
    "    train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using GPUs**\n",
    "\n",
    "AI Platform lets you run any TensorFlow training application on a GPU-enabled machine. Learn more about [using GPUs for training models in the cloud](https://cloud.google.com/ml-engine/docs/tensorflow/using-gpus#submit-job).\n",
    "We define a `config.yaml` file that describes the GPU options we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AI_Platform/cnn_trainer/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ROOT_PATH}/config.yaml\n",
    "\n",
    "trainingInput:\n",
    "    scaleTier: CUSTOM\n",
    "    # A single NVIDIA Tesla V100 GPU\n",
    "    masterType: large_model_v100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the package to AI Platform for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# INSERT YOUR PROJECT HERE!\n",
    "PROJECT_ID = env.project_id\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "JOB_NAME = 'job_v' + str(int(time.time()))\n",
    "TRAINER_PACKAGE_PATH = 'AI_Platform/cnn_trainer/trainer/'\n",
    "MAIN_TRAINER_MODULE = 'trainer.task'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up your GCP project**\n",
    "\n",
    "Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authenticate your GCP account**\n",
    "\n",
    "Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS 'privatekey.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit a training job to AI Platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
    "    --job-dir {config.job_dir} \\\n",
    "    --package-path {TRAINER_PACKAGE_PATH} \\\n",
    "    --module-name {MAIN_TRAINER_MODULE} \\\n",
    "    --region {REGION} \\\n",
    "    --config {ROOT_PATH}/config.yaml \\\n",
    "    --runtime-version 1.14 \\\n",
    "    --python-version 3.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monitor the training job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = !gcloud ai-platform jobs describe {JOB_NAME} --project {PROJECT_ID}\n",
    "state = desc.grep('state:')[0].split(':')[1].strip()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Prepare the model for making predictions in Earth Engine\n",
    "\n",
    "Before we can use the model in Earth Engine, it needs to be hosted by AI Platform.  But before we can host the model on AI Platform we need to *EEify* (a new word!) it.  The EEification process merely appends some extra operations to the input and outputs of the model in order to accomdate the interchange format between pixels from Earth Engine (float32) and inputs to AI Platform (base64).  (See [this doc](https://cloud.google.com/ml-engine/docs/online-predict#binary_data_in_prediction_input) for details.)  \n",
    "\n",
    "**`earthengine model prepare`**\n",
    "\n",
    "The EEification process is handled for you using the Earth Engine command `earthengine model prepare`.  To use that command, we need to specify the input and output model directories and the name of the input and output nodes in the TensorFlow computation graph.  We can do all that programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "\n",
    "# Specify names of input locations in Cloud Storage.\n",
    "dataset_name = 'Landsat8_Cropland'\n",
    "job_dir = 'gs://' + env.bucket_name + '/' + 'cnn-models/' + dataset_name + '/trainer'\n",
    "model_dir = job_dir + '/model'\n",
    "PROJECT_ID = env.project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the directory with the latest timestamp, in case you've trained multiple times\n",
    "exported_model_dirs = ! gsutil ls {model_dir}\n",
    "saved_model_path = exported_model_dirs[-1]\n",
    "\n",
    "folder_name = saved_model_path.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_path, 'serve')\n",
    "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# model only has a single input and a single output.\n",
    "input_name = None\n",
    "for k,v in inputs.items():\n",
    "    input_name = v.name\n",
    "    break\n",
    "\n",
    "output_name = None\n",
    "for k,v in outputs.items():\n",
    "    output_name = v.name\n",
    "    break\n",
    "\n",
    "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
    "# AI Platform inputs and outputs, respectively.\n",
    "import json\n",
    "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "output_dict = \"'\" + json.dumps({output_name: \"prediction\"}) + \"'\"\n",
    "\n",
    "# Put the EEified model next to the trained model directory.\n",
    "EEIFIED_DIR = job_dir + '/eeified/' + folder_name\n",
    "\n",
    "# You need to set the project before using the model prepare command.\n",
    "#!earthengine set_project {PROJECT_ID}\n",
    "#!earthengine model prepare --source_dir {saved_model_path} --dest_dir {EEIFIED_DIR} --input {input_dict} --output {output_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployed the model to AI Platform**\n",
    "\n",
    "Before it's possible to get predictions from the trained and EEified model, it needs to be deployed on AI Platform.  The first step is to create the model.  The second step is to create a version.  See [this guide](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models) for details.  Note that models and versions can be monitored from the [AI Platform models page](http://console.cloud.google.com/ai-platform/models) of the Cloud Console. \n",
    "\n",
    "To ensure that the model is ready for predictions without having to warm up nodes, you can use a configuration yaml file to set the scaling type of this version to autoScaling, and, set a minimum number of nodes for the version. This will ensure there are always nodes on stand-by, however, you will be charged as long as they are running. For this example, we'll set the minNodes to 10. That means that at a minimum, 10 nodes are always up and running and waiting for predictions. The number of nodes will also scale up automatically if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "autoScaling:\n",
    "    minNodes: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "REGION = \"us-west1\"\n",
    "MODEL_NAME = 'deepvel_'+dataset_name\n",
    "VERSION_NAME = 'v' + folder_name\n",
    "print('Creating version: ' + VERSION_NAME)\n",
    "\n",
    "!gcloud ai-platform models create {MODEL_NAME} \n",
    "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --model {MODEL_NAME} \\\n",
    "  --origin {EEIFIED_DIR} \\\n",
    "  --runtime-version=1.14 \\\n",
    "  --framework \"TENSORFLOW\" \\\n",
    "  --python-version=3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Predict in Earth Engine\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inCollection = 'Landsat8_SR'\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "startDate = '2016-01-01'\n",
    "stopDate = '2016-12-31'\n",
    "scale = 30 #scale in meters\n",
    "\n",
    "# Model variables\n",
    "PROJECT_ID = env.project_id\n",
    "MODEL_NAME = 'deepvel_Landsat8_Cropland'\n",
    "VERSION_NAME = 'v1576262418'\n",
    "modelType = 'segmentation'\n",
    "\n",
    "# polygon where we want to display de predictions\n",
    "geometry = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [\n",
    "              -121.59221649169923,\n",
    "              38.53796141693008\n",
    "            ],\n",
    "            [\n",
    "              -121.44081115722655,\n",
    "              38.53796141693008\n",
    "            ],\n",
    "            [\n",
    "              -121.44081115722655,\n",
    "              38.626526838378076\n",
    "            ],\n",
    "            [\n",
    "              -121.59221649169923,\n",
    "              38.626526838378076\n",
    "            ],\n",
    "            [\n",
    "              -121.59221649169923,\n",
    "              38.53796141693008\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ee.Model.fromAiPlatformPredictor`**\n",
    "\n",
    "There is now a trained model, prepared for serving to Earth Engine, hosted and versioned on AI Platform.  \n",
    "We can now connect Earth Engine directly to the trained model for inference.  You do that with the `ee.Model.fromAiPlatformPredictor` command.\n",
    "For this command to work, we need to know a lot about the model.  To connect to the model, you need to know the name and version.\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "You need to be able to recreate the imagery on which it was trained in order to perform inference.  Specifically, you need to create an array-valued input from the scaled data and use that for input.  (Recall that the new input node is named `array`, which is convenient because the array image has one band, named `array` by default.)  The inputs will be provided as 144x144 patches (`inputTileSize`), at 30-meter resolution (`proj`), but 8 pixels will be thrown out (`inputOverlapSize`) to minimize boundary effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "payload =   {\n",
    "    \"collection\": inCollection,\n",
    "    \"start\": startDate,\n",
    "    \"end\": stopDate,\n",
    "    \"scale\": scale\n",
    "}\n",
    "\n",
    "url = f'https://us-central1-skydipper-196010.cloudfunctions.net/ee_pre_processing'\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "output_pre_processing = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "output_pre_processing.json()\n",
    "\n",
    "image = ee.deserializer.fromJSON(output_pre_processing.json()['composite'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select bands and convert them into float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.select(inBands).float()\n",
    "image.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outputs**\n",
    "\n",
    "The output (which you also need to know)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and use it for prediction.\n",
    "model = ee.Model.fromAiPlatformPredictor(\n",
    "    projectName = PROJECT_ID,\n",
    "    modelName = MODEL_NAME,\n",
    "    version = VERSION_NAME,\n",
    "    inputTileSize = [144, 144],\n",
    "    inputOverlapSize = [8, 8],\n",
    "    proj = ee.Projection('EPSG:4326').atScale(scale),\n",
    "    fixInputProj = True,\n",
    "    outputBands = {'prediction': {\n",
    "        'type': ee.PixelType.float(),\n",
    "        'dimensions': 1,\n",
    "      }                  \n",
    "    }\n",
    ")\n",
    "predictions = model.predictImage(image.toArray()).arrayFlatten([outBands])\n",
    "predictions.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the prediction area with the polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip the prediction area with the polygon\n",
    "polygon = ee.Geometry.Polygon(geometry.get('features')[0].get('geometry').get('coordinates'))\n",
    "predictions = predictions.clip(polygon)\n",
    "\n",
    "# Get centroid\n",
    "centroid = polygon.centroid().getInfo().get('coordinates')[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentate image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelType == 'segmentation':\n",
    "    maxValues = predictions.reduce(ee.Reducer.max())\n",
    "\n",
    "    predictions = predictions.addBands(maxValues)\n",
    "\n",
    "    expression = \"\"\n",
    "    for n, band in enumerate(outBands):\n",
    "        expression = expression + f\"(b('{band}') == b('max')) ? {str(n+1)} : \"\n",
    "\n",
    "    expression = expression + f\"0\"\n",
    "\n",
    "    segmentation = predictions.expression(expression)\n",
    "    predictions = predictions.addBands(segmentation.mask(segmentation).select(['constant'], ['categories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display**\n",
    "\n",
    "Use folium to visualize the input imagery and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 1})\n",
    "map = folium.Map(location=centroid, zoom_start=12)\n",
    "folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(map)\n",
    "\n",
    "for band in outBands:\n",
    "    mapid = predictions.getMapId({'bands': [band], 'min': 0, 'max': 1})\n",
    "    \n",
    "    folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name=band,\n",
    "      ).add_to(map)\n",
    "\n",
    "\n",
    "if modelType == 'segmentation':\n",
    "    mapid = predictions.getMapId({'bands': ['categories'], 'min': 1, 'max': len(outBands)})\n",
    "    \n",
    "    folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name='categories',\n",
    "      ).add_to(map)\n",
    "    \n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Predict in AI Platform\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inCollection = 'Landsat8_SR'\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "startDate = '2016-01-01'\n",
    "stopDate = '2016-12-31'\n",
    "scale = 30 #scale in meters\n",
    "\n",
    "# Model variables\n",
    "PROJECT_ID = env.project_id\n",
    "MODEL_NAME = 'segnet_Landsat8_Cropland'\n",
    "VERSION_NAME = VERSION_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify names of input locations in Cloud Storage.\n",
    "dataset_name = 'Landsat8_Cropland'\n",
    "job_dir = 'gs://' + bucket + '/' + 'cnn-models/' + dataset_name + '/trainer'\n",
    "model_dir = job_dir + '/model' \n",
    "\n",
    "exported_model_dirs = ! gsutil ls {model_dir}\n",
    "\n",
    "# Pick the directory with the latest timestamp, in case you've trained multiple times\n",
    "saved_model_path = exported_model_dirs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = 'segnet_Landsat8_Cropland'\n",
    "VERSION_NAME = VERSION_NAME\n",
    "print('Creating version: ' + VERSION_NAME)\n",
    "\n",
    "!gcloud ai-platform models create {MODEL_NAME} \n",
    "\n",
    "# Create model version based on that SavedModel directory\n",
    "! gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --model {MODEL_NAME} \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --python-version 3.5 \\\n",
    "  --framework tensorflow \\\n",
    "  --origin {saved_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = 'Landsat8_Cropland'\n",
    "baseNames = ['training_patches', 'eval_patches']\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "        \n",
    "folder = 'cnn-models/'+datasetName+'/data'\n",
    "kernel_size = 256\n",
    "buffer_size = 100\n",
    "batch_size = 4\n",
    "\n",
    "glob = 'gs://' + env.bucket_name + '/' + folder + '/' + baseNames[0] + '_g0'+ '*'\n",
    "dataset = get_dataset(glob, inBands, outBands, kernel_size, buffer_size, batch_size)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(input_arr[2,:80,:80,:], input_arr.shape[3], titles=inBands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formatting input data for online prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = input_arr[2,:80,:80,:]\n",
    "data = np.around(data,2).tolist()\n",
    "instance = {\"image\" : data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requesting predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import googleapiclient\n",
    "\n",
    "def predict_json(project, model, instances, privatekey_path, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "\n",
    "    Args:\n",
    "        project (str): project where the AI Platform Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"    \n",
    "    # To authenticate set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "    credentials = service_account.Credentials.from_service_account_file(privatekey_path)\n",
    "    \n",
    "    # Create the AI Platform service object.\n",
    "    service = googleapiclient.discovery.build('ml', 'v1', credentials=credentials)\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit the online prediction request**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privatekey_path = 'privatekey.json'\n",
    "response = predict_json(project=PROJECT_ID, model=MODEL_NAME, instances=instance, privatekey_path=privatekey_path, version=VERSION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.array(response[0].get('output'))\n",
    "output.shape\n",
    "display_channels(output, output.shape[2], titles=['cropland', 'land', 'water', 'urban'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data post-processing\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
