{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Google Earth Engine, Cloud Storage and AI Platform\n",
    "\n",
    "This notebook is inspired by the following tutorials:\n",
    "\n",
    "- [Getting started: Training and prediction with Keras](https://cloud.google.com/ml-engine/docs/tensorflow/getting-started-keras)\n",
    "- [Down to Earth with AI Platform](https://medium.com/google-earth/down-to-earth-with-ai-platform-7bc363abf4fa)\n",
    "- [Deploying to AI Platform](https://developers.google.com/earth-engine/tf_examples#deploying-to-ai-platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup software libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.202'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and initialize the Earth Engine library.\n",
    "import ee\n",
    "ee.Initialize()\n",
    "ee.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.3\n"
     ]
    }
   ],
   "source": [
    "# Folium setup.\n",
    "import folium\n",
    "print(folium.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import functools\n",
    "import json\n",
    "from pprint import pprint\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earth Engine ImageCollection attributes\n",
    "\n",
    "We define the different attributes that we will need for each Earth Engine ImageCollection all through the notebook. \n",
    "\n",
    "We include them in the `ee_collection_specifics.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ee_collection_specifics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ee_collection_specifics.py\n",
    "\n",
    "\"\"\"\n",
    "Information on Earth Engine collections stored here (e.g. bands, collection ids, etc.)\n",
    "\"\"\"\n",
    "\n",
    "import ee\n",
    "\n",
    "def ee_collections(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine image collection names\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': 'COPERNICUS/S2',\n",
    "        'Landsat7_SR': 'LANDSAT/LE07/C01/T1_SR',\n",
    "        'Landsat8_SR': 'LANDSAT/LC08/C01/T1_SR',\n",
    "        'CroplandDataLayers': 'USDA/NASS/CDL',\n",
    "        'NationalLandCoverDatabase': 'USGS/NLCD'\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine band names\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel2_TOA': ['B1','B2','B3','B4','B5','B6','B7','B8A','B8','B11','B12'],\n",
    "        'Landsat7_SR': ['B1','B2','B3','B4','B5','B6','B7'],\n",
    "        'Landsat8_SR': ['B1','B2','B3','B4','B5','B6','B7','B10','B11'],\n",
    "        'CroplandDataLayers': ['landcover', 'cropland', 'land', 'water', 'urban'],\n",
    "        'NationalLandCoverDatabase': ['impervious']\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands_rgb(collection):\n",
    "    \"\"\"\n",
    "    Earth Engine rgb band names\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel2_TOA': ['B4','B3','B2'],\n",
    "        'Landsat7_SR': ['B3','B2','B1'],\n",
    "        'Landsat8_SR': ['B4', 'B3', 'B2'],\n",
    "        'CroplandDataLayers': ['landcover'],\n",
    "        'NationalLandCoverDatabase': ['impervious']\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def ee_bands_normThreshold(collection):\n",
    "    \"\"\"\n",
    "    Normalization threshold percentage\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {\n",
    "        'Sentinel2_TOA': {'B1': 75,'B2': 75,'B3': 75,'B4': 75,'B5': 80,'B6': 80,'B7': 80,'B8A': 80,'B8': 80,'B11': 100,'B12': 100},\n",
    "        'Landsat7_SR': {'B1': 95,'B2': 95,'B3': 95,'B4': 100,'B5': 100,'B6': 100,'B7': 100},\n",
    "        'Landsat8_SR': {'B1': 90,'B2': 95,'B3': 95,'B4': 95,'B5': 100,'B6': 100,'B7': 100,'B10': 100,'B11': 100},\n",
    "        'CroplandDataLayers': {'landcover': 100, 'cropland': 100, 'land': 100, 'water': 100, 'urban': 100},\n",
    "        'NationalLandCoverDatabase': {'impervious': 100}\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def normalize(collection):\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': True,\n",
    "        'Landsat7_SR': True,\n",
    "        'Landsat8_SR': True,\n",
    "        'CroplandDataLayers': False,\n",
    "        'NationalLandCoverDatabase': False\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def vizz_params_rgb(collection):\n",
    "    \"\"\"\n",
    "    Visualization parameters\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': {'min':0,'max':3000, 'bands':['B4','B3','B2']},\n",
    "        'Landsat7_SR': {'min':0,'max':3000, 'gamma':1.4, 'bands':['B3','B2','B1']},\n",
    "        'Landsat8_SR': {'min':0,'max':3000, 'gamma':1.4, 'bands':['B4','B3','B2']},\n",
    "        'CroplandDataLayers': {'min':0,'max':3, 'bands':['landcover']},\n",
    "        'NationalLandCoverDatabase': {'min': 0, 'max': 1, 'bands':['impervious']}\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "def vizz_params(collection):\n",
    "    \"\"\"\n",
    "    Visualization parameters\n",
    "    \"\"\"\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': [{'min':0,'max':1, 'bands':['B4','B3','B2']}, \n",
    "                      {'min':0,'max':1, 'bands':['B1']},\n",
    "                      {'min':0,'max':1, 'bands':['B5']},\n",
    "                      {'min':0,'max':1, 'bands':['B6']},\n",
    "                      {'min':0,'max':1, 'bands':['B7']},\n",
    "                      {'min':0,'max':1, 'bands':['B8A']},\n",
    "                      {'min':0,'max':1, 'bands':['B8']},\n",
    "                      {'min':0,'max':1, 'bands':['B11']},\n",
    "                      {'min':0,'max':1, 'bands':['B12']}],\n",
    "        'Landsat7_SR': [{'min':0,'max':1, 'gamma':1.4, 'bands':['B3','B2','B1']}, \n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B4']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B5']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B7']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B6']}],\n",
    "        'Landsat8_SR': [{'min':0,'max':1, 'gamma':1.4, 'bands':['B4','B3','B2']}, \n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B1']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B5']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B6']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B7']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B10']},\n",
    "                     {'min':0,'max':1, 'gamma':1.4, 'bands':['B11']}],\n",
    "        'CroplandDataLayers': [{'min':0,'max':3, 'bands':['landcover']},\n",
    "                               {'min':0,'max':1, 'bands':['cropland']},\n",
    "                               {'min':0,'max':1, 'bands':['land']},\n",
    "                               {'min':0,'max':1, 'bands':['water']},\n",
    "                               {'min':0,'max':1, 'bands':['urban']}],\n",
    "        'NationalLandCoverDatabase': [{'min': 0, 'max': 1, 'bands':['impervious']}]\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n",
    "## ------------------------- Filter datasets ------------------------- ##\n",
    "## Lansat 7 Cloud Free Composite\n",
    "def CloudMaskL7sr(image):\n",
    "    qa = image.select('pixel_qa')\n",
    "    #If the cloud bit (5) is set and the cloud confidence (7) is high\n",
    "    #or the cloud shadow bit is set (3), then it's a bad pixel.\n",
    "    cloud = qa.bitwiseAnd(1 << 5).And(qa.bitwiseAnd(1 << 7)).Or(qa.bitwiseAnd(1 << 3))\n",
    "    #Remove edge pixels that don't occur in all bands\n",
    "    mask2 = image.mask().reduce(ee.Reducer.min())\n",
    "    return image.updateMask(cloud.Not()).updateMask(mask2)\n",
    "\n",
    "def CloudFreeCompositeL7(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate).map(CloudMaskL7sr)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Lansat 8 Cloud Free Composite\n",
    "def CloudMaskL8sr(image):\n",
    "    opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "    thermalBands = ['B10', 'B11']\n",
    "\n",
    "    cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
    "    cloudsBitMask = ee.Number(2).pow(5).int()\n",
    "    qa = image.select('pixel_qa')\n",
    "    mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
    "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
    "    mask2 = image.mask().reduce('min')\n",
    "    mask3 = image.select(opticalBands).gt(0).And(\n",
    "            image.select(opticalBands).lt(10000)).reduce('min')\n",
    "    mask = mask1.And(mask2).And(mask3)\n",
    "    \n",
    "    return image.updateMask(mask)\n",
    "\n",
    "def CloudFreeCompositeL8(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate).map(CloudMaskL8sr)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Sentinel 2 Cloud Free Composite\n",
    "def CloudMaskS2(image):\n",
    "    \"\"\"\n",
    "    European Space Agency (ESA) clouds from 'QA60', i.e. Quality Assessment band at 60m\n",
    "    parsed by Nick Clinton\n",
    "    \"\"\"\n",
    "    AerosolsBands = ['B1']\n",
    "    VIBands = ['B2', 'B3', 'B4']\n",
    "    RedBands = ['B5', 'B6', 'B7', 'B8A']\n",
    "    NIRBands = ['B8']\n",
    "    SWIRBands = ['B11', 'B12']\n",
    "\n",
    "    qa = image.select('QA60')\n",
    "\n",
    "    # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "    cloudBitMask = int(2**10)\n",
    "    cirrusBitMask = int(2**11)\n",
    "\n",
    "    # Both flags set to zero indicates clear conditions.\n",
    "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(\\\n",
    "            qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "\n",
    "    return image.updateMask(mask)\n",
    "\n",
    "def CloudFreeCompositeS2(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('COPERNICUS/S2')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\\\n",
    "            .map(CloudMaskS2)\n",
    "\n",
    "    ## Composite\n",
    "    composite = collection.median()\n",
    "    \n",
    "    return composite\n",
    "\n",
    "## Cropland Data Layers\n",
    "def CroplandData(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('USDA/NASS/CDL')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Change classes\n",
    "    land = ['65', '131', '141', '142', '143', '152', '176', '87', '190', '195']\n",
    "    water = ['83', '92', '111']\n",
    "    urban = ['82', '121', '122', '123', '124']\n",
    "    \n",
    "    classes = []\n",
    "    for n, i in enumerate([land,water,urban]):\n",
    "        a = ''\n",
    "        for m, j in enumerate(i):\n",
    "            if m < len(i)-1:\n",
    "                a = a + 'crop == '+ j + ' || '\n",
    "            else: \n",
    "                a = a + 'crop == '+ j\n",
    "        classes.append('('+a+') * '+str(n+1))\n",
    "    classes = ' + '.join(classes)\n",
    "    \n",
    "    image = image.expression(classes, {'crop': image.select(['cropland'])})\n",
    "    \n",
    "    image =image.rename('landcover')\n",
    "    \n",
    "    # Split image into 1 band per class\n",
    "    names = ['cropland', 'land', 'water', 'urban']\n",
    "    mask = image\n",
    "    for i, name in enumerate(names):\n",
    "        image = ee.Image.cat([image, mask.eq(i).rename(name)])\n",
    "     \n",
    "    return image\n",
    "\n",
    "## National Land Cover Database\n",
    "def ImperviousData(startDate, stopDate):\n",
    "    ## Define your collection\n",
    "    collection = ee.ImageCollection('USGS/NLCD')\n",
    "\n",
    "    ## Filter \n",
    "    collection = collection.filterDate(startDate,stopDate)\n",
    "\n",
    "    ## First image\n",
    "    image = ee.Image(collection.first())\n",
    "    \n",
    "    ## Select impervious band\n",
    "    image = image.select('impervious')\n",
    "    \n",
    "    ## Normalize to 1\n",
    "    image = image.divide(100).float()\n",
    "    \n",
    "    return image\n",
    "\n",
    "## ------------------------------------------------------------------- ##\n",
    "\n",
    "def Composite(collection):\n",
    "    dic = {\n",
    "        'Sentinel2_TOA': CloudFreeCompositeS2,\n",
    "        'Landsat7_SR': CloudFreeCompositeL7,\n",
    "        'Landsat8_SR': CloudFreeCompositeL8,\n",
    "        'CroplandDataLayers': CroplandData,\n",
    "        'NationalLandCoverDatabase': ImperviousData,\n",
    "    }\n",
    "    \n",
    "    return dic[collection]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee_collection_specifics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite image\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'CroplandDataLayers'\n",
    "startDate = '2016-01-01'\n",
    "stopDate = '2016-12-31'\n",
    "scale = 10 #scale in meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display composite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x11d4766d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "composite = ee_collection_specifics.Composite(collection)(startDate, stopDate)\n",
    "mapid = composite.getMapId(ee_collection_specifics.vizz_params_rgb(collection))\n",
    "\n",
    "tiles_url = EE_TILES.format(**mapid)\n",
    "\n",
    "map = folium.Map(location=[38.1623, -121.6911])\n",
    "folium.TileLayer(\n",
    "tiles=tiles_url,\n",
    "attr='Google Earth Engine',\n",
    "overlay=True,\n",
    "name=str(ee_collection_specifics.ee_bands_rgb(collection))).add_to(map)\n",
    "    \n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "We normalize the composite images to have values from 0 to 1.\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'Sentinel2_TOA'\n",
    "startDate = '2016-01-01'\n",
    "stopDate = '2016-12-31'\n",
    "scale = 30 #scale in meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create composite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ee_collection_specifics.Composite(collection)(startDate, stopDate)\n",
    "\n",
    "bands = ee_collection_specifics.ee_bands(collection)\n",
    "image = image.select(bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_values(image, collection, scale):\n",
    "    \n",
    "    normThreshold = ee_collection_specifics.ee_bands_normThreshold(collection)\n",
    "    \n",
    "    num = 2\n",
    "    lon = np.linspace(-180, 180, num)\n",
    "    lat = np.linspace(-90, 90, num)\n",
    "    \n",
    "    features = []\n",
    "    for i in range(len(lon)-1):\n",
    "        for j in range(len(lat)-1):\n",
    "            features.append(ee.Feature(ee.Geometry.Rectangle(lon[i], lat[j], lon[i+1], lat[j+1])))\n",
    "    \n",
    "    regReducer = {\n",
    "        'geometry': ee.FeatureCollection(features),\n",
    "        'reducer': ee.Reducer.minMax(),\n",
    "        'maxPixels': 1e10,\n",
    "        'bestEffort': True,\n",
    "        'scale':scale\n",
    "        \n",
    "    }\n",
    "    \n",
    "    values = image.reduceRegion(**regReducer).getInfo()\n",
    "    print(values)\n",
    "    \n",
    "    # Avoid outliers by taking into account only the normThreshold% of the data points.\n",
    "    regReducer = {\n",
    "        'geometry': ee.FeatureCollection(features),\n",
    "        'reducer': ee.Reducer.histogram(),\n",
    "        'maxPixels': 1e10,\n",
    "        'bestEffort': True,\n",
    "        'scale':scale\n",
    "        \n",
    "    }\n",
    "    \n",
    "    hist = image.reduceRegion(**regReducer).getInfo()\n",
    "\n",
    "    for band in list(normThreshold.keys()):\n",
    "        if normThreshold[band] != 100:\n",
    "            count = np.array(hist.get(band).get('histogram'))\n",
    "            x = np.array(hist.get(band).get('bucketMeans'))\n",
    "        \n",
    "            cumulative_per = np.cumsum(count/count.sum()*100)\n",
    "        \n",
    "            values[band+'_max'] = x[np.where(cumulative_per < normThreshold[band])][-1]\n",
    "        \n",
    "    return values\n",
    "\n",
    "def normalize_ee_images(image, collection, values):\n",
    "    \n",
    "    Bands = ee_collection_specifics.ee_bands(collection)\n",
    "       \n",
    "    # Normalize [0, 1] ee images\n",
    "    for i, band in enumerate(Bands):\n",
    "        if i == 0:\n",
    "            image_new = image.select(band).clamp(values[band+'_min'], values[band+'_max'])\\\n",
    "                                .subtract(values[band+'_min'])\\\n",
    "                                .divide(values[band+'_max']-values[band+'_min'])\n",
    "        else:\n",
    "            image_new = image_new.addBands(image.select(band).clamp(values[band+'_min'], values[band+'_max'])\\\n",
    "                                    .subtract(values[band+'_min'])\\\n",
    "                                    .divide(values[band+'_max']-values[band+'_min']))\n",
    "            \n",
    "    return image_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B11_max': 9660.0, 'B11_min': 1.0, 'B12_max': 7536.0, 'B12_min': 1.0, 'B1_max': 9722.0, 'B1_min': 1.0, 'B2_max': 10946.0, 'B2_min': 1.0, 'B3_max': 10163.0, 'B3_min': 1.0, 'B4_max': 11217.0, 'B4_min': 1.0, 'B5_max': 11316.0, 'B5_min': 1.0, 'B6_max': 11421.0, 'B6_min': 1.0, 'B7_max': 11253.0, 'B7_min': 1.0, 'B8A_max': 11161.0, 'B8A_min': 1.0, 'B8_max': 11176.0, 'B8_min': 1.0}\n",
      "{'B11_max': 9660.0, 'B11_min': 1.0, 'B12_max': 7536.0, 'B12_min': 1.0, 'B1_max': 3807.6194751381217, 'B1_min': 1.0, 'B2_max': 3294.657679738562, 'B2_min': 1.0, 'B3_max': 2783.693302891933, 'B3_min': 1.0, 'B4_max': 2721.0618374558303, 'B4_min': 1.0, 'B5_max': 4767.120137299771, 'B5_min': 1.0, 'B6_max': 4577.511415525114, 'B6_min': 1.0, 'B7_max': 4383.79792147806, 'B7_min': 1.0, 'B8A_max': 4191.087336244542, 'B8A_min': 1.0, 'B8_max': 4063.190789473684, 'B8_min': 1.0}\n",
      "CPU times: user 44.7 ms, sys: 36.6 ms, total: 81.2 ms\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if ee_collection_specifics.normalize(collection):\n",
    "    # Get min man values for each band\n",
    "    values = min_max_values(image, collection, scale)\n",
    "    print(values)\n",
    "\n",
    "    # Normalize images\n",
    "    image = normalize_ee_images(image, collection, values)\n",
    "else:\n",
    "    values = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display composite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <script>L_PREFER_CANVAS=false; L_NO_TOUCH=false; L_DISABLE_3D=false;</script>
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.js"></script>
    <script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
    <meta name="viewport" content="width=device-width,
        initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <style>#map_f10a3817340042509413182c23bff97b {
        position: relative;
        width: 100.0%;
        height: 100.0%;
        left: 0.0%;
        top: 0.0%;
        }
    </style>
</head>
<body>    
    
    <div class="folium-map" id="map_f10a3817340042509413182c23bff97b" ></div>
</body>
<script>    
    
    
        var bounds = null;
    

    var map_f10a3817340042509413182c23bff97b = L.map(
        'map_f10a3817340042509413182c23bff97b', {
        center: [38.1623, -121.6911],
        zoom: 10,
        maxBounds: bounds,
        layers: [],
        worldCopyJump: false,
        crs: L.CRS.EPSG3857,
        zoomControl: true,
        });


    
    var tile_layer_c3144175babb4933a78553dcba347684 = L.tileLayer(
        'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png',
        {
        "attribution": null,
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    var tile_layer_408b8dbf6ace44b0bc7bce2a6eba29ac = L.tileLayer(
        'https://earthengine.googleapis.com/map/4763307922cba4c815c3dd3e0408b491/{z}/{x}/{y}?token=b91f11b254042497f5bb8f2fda895928',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    var tile_layer_23566ea759dd4b1f83d32d7ae5e9e94a = L.tileLayer(
        'https://earthengine.googleapis.com/map/b18eedf56e09b6f4253ec7ddfa7e9e2c/{z}/{x}/{y}?token=02b038db9268c829e28d1a8b43997b5c',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    var tile_layer_d15b2cfcc65a4fc283cc61cca3aa7d25 = L.tileLayer(
        'https://earthengine.googleapis.com/map/b07a9c4a8021ad6cca1530f06c63ec8e/{z}/{x}/{y}?token=70710487d7c17de9c58f55b84a5cce00',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    var tile_layer_ecb9d586869040b896a9adf4474c73d8 = L.tileLayer(
        'https://earthengine.googleapis.com/map/cb10b35677f5c145b4ffcab0db324b1f/{z}/{x}/{y}?token=f3854ca44459e080c3ba7b05a73ff7fc',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    var tile_layer_36adbdefe4b8437baee099dd61c08d25 = L.tileLayer(
        'https://earthengine.googleapis.com/map/c226317429fb5881afeb8597c0f789e6/{z}/{x}/{y}?token=c6af4db8a9dcfadcf8b8455c308c071f',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    var tile_layer_a000ee05e74c4ecb9dff87c4bbfe5a33 = L.tileLayer(
        'https://earthengine.googleapis.com/map/80740e38741a8fe45c4b83334c64a139/{z}/{x}/{y}?token=6cb84e6c75d3b996088955320ae3e2dd',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    var tile_layer_a228dc345f9b4f7194364ba440d00290 = L.tileLayer(
        'https://earthengine.googleapis.com/map/f741e3f3a0e4870a45f0964045600eb3/{z}/{x}/{y}?token=fbb731b3e025360e782e94e34d73aec6',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    var tile_layer_49260b0b9994408a88bf32476272e6cd = L.tileLayer(
        'https://earthengine.googleapis.com/map/8ca324b71c880c50c541c9c34a7132d2/{z}/{x}/{y}?token=c4571c4f0795ce3c49546d1df8ebbe1b',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    var tile_layer_7507032791354d289424b58293c98176 = L.tileLayer(
        'https://earthengine.googleapis.com/map/ab900e01036a04290aed5a5d7ce0c1c7/{z}/{x}/{y}?token=3b51c5d8592ee96ca25f03ac2d56f528',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_f10a3817340042509413182c23bff97b);
    
            var layer_control_8b40ea9f978148d2bfa05d8e907d6670 = {
                base_layers : { "openstreetmap" : tile_layer_c3144175babb4933a78553dcba347684, },
                overlays : { "['B4', 'B3', 'B2']" : tile_layer_408b8dbf6ace44b0bc7bce2a6eba29ac,"['B1']" : tile_layer_23566ea759dd4b1f83d32d7ae5e9e94a,"['B5']" : tile_layer_d15b2cfcc65a4fc283cc61cca3aa7d25,"['B6']" : tile_layer_ecb9d586869040b896a9adf4474c73d8,"['B7']" : tile_layer_36adbdefe4b8437baee099dd61c08d25,"['B8A']" : tile_layer_a000ee05e74c4ecb9dff87c4bbfe5a33,"['B8']" : tile_layer_a228dc345f9b4f7194364ba440d00290,"['B11']" : tile_layer_49260b0b9994408a88bf32476272e6cd,"['B12']" : tile_layer_7507032791354d289424b58293c98176, }
                };
            L.control.layers(
                layer_control_8b40ea9f978148d2bfa05d8e907d6670.base_layers,
                layer_control_8b40ea9f978148d2bfa05d8e907d6670.overlays,
                {position: 'topright',
                 collapsed: true,
                 autoZIndex: true
                }).addTo(map_f10a3817340042509413182c23bff97b);
            
        
</script>\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x11f909790>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "map = folium.Map(location=[38.1623, -121.6911])\n",
    "for params in ee_collection_specifics.vizz_params(collection):\n",
    "    mapid = image.getMapId(params)\n",
    "    folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name=str(params['bands']),\n",
    "  ).add_to(map)\n",
    "    \n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Create TFRecords for training\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "inCollection = 'Landsat8_SR'\n",
    "outCollection = 'CroplandDataLayers'\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "startDate = '2016-01-01'\n",
    "stopDate = '2016-12-31'\n",
    "scale = 30 #scale in meters\n",
    "sampleSize = 1000 # Total sample size in each polygon.\n",
    "datasetName = 'Landsat8_Cropland'\n",
    "trainPolys = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"MultiPolygon\",\n",
    "        \"coordinates\":  [\n",
    "[[[  -122.882080078125,  40.50126945841645],[  -122.1240234375,  40.50126945841645],[  -122.1240234375,  41.008920735004885],[  -122.882080078125,  41.008920735004885],[  -122.882080078125,  40.50126945841645]]],\n",
    "[[[  -122.2283935546875,  39.00637903337455],[  -121.607666015625,  39.00637903337455],[  -121.607666015625,  39.46588451142044],[  -122.2283935546875,  39.46588451142044],[  -122.2283935546875,  39.00637903337455]]],\n",
    "[[[  -120.355224609375,  38.77978137804918],[  -119.608154296875,  38.77978137804918],[  -119.608154296875,  39.342794408952365],[  -120.355224609375,  39.342794408952365],[  -120.355224609375,  38.77978137804918]]],\n",
    "[[[  -121.90979003906249,  37.70555348721583],[  -120.9814453125,  37.70555348721583],[  -120.9814453125,  38.39764411353178],[  -121.90979003906249,  38.39764411353178],[  -121.90979003906249,  37.70555348721583]]],\n",
    "[[[  -120.03662109374999,  37.45741810262938],[  -119.1851806640625,  37.45741810262938],[  -119.1851806640625,  38.08268954483802],[  -120.03662109374999,  38.08268954483802],[  -120.03662109374999,  37.45741810262938]]],\n",
    "[[[  -120.03662109374999,  37.45741810262938],[  -119.1851806640625,  37.45741810262938],[  -119.1851806640625,  38.08268954483802],[  -120.03662109374999,  38.08268954483802],[  -120.03662109374999,  37.45741810262938]]],\n",
    "[[[  -120.03662109374999,  37.45741810262938],[  -119.1851806640625,  37.45741810262938],[  -119.1851806640625,  38.08268954483802],[  -120.03662109374999,  38.08268954483802],[  -120.03662109374999,  37.45741810262938]]],\n",
    "[[[  -112.554931640625,  33.0178760185549],[  -111.588134765625,  33.0178760185549],[  -111.588134765625,  33.78827853625996],[  -112.554931640625,  33.78827853625996],[  -112.554931640625,  33.0178760185549]]],\n",
    "[[[  -112.87353515625,  40.51379915504413],[  -111.829833984375,  40.51379915504413],[  -111.829833984375,  41.28606238749825],[  -112.87353515625,  41.28606238749825],[  -112.87353515625,  40.51379915504413]]],\n",
    "[[[  -108.19335937499999,  39.095962936305476],[  -107.1826171875,  39.095962936305476],[  -107.1826171875,  39.85915479295669],[  -108.19335937499999,  39.85915479295669],[  -108.19335937499999,  39.095962936305476]]],\n",
    "[[[  -124.25537109375,  30.86451022625836],[  -124.25537109375,  30.86451022625836],[  -124.25537109375,  30.86451022625836],[  -124.25537109375,  30.86451022625836]]],\n",
    "[[[  -106.875,  37.142803443716836],[  -105.49072265625,  37.142803443716836],[  -105.49072265625,  38.18638677411551],[  -106.875,  38.18638677411551],[  -106.875,  37.142803443716836]]],\n",
    "[[[  -117.31201171875001,  43.27720532212024],[  -116.01562499999999,  43.27720532212024],[  -116.01562499999999,  44.134913443750726],[  -117.31201171875001,  44.134913443750726],[  -117.31201171875001,  43.27720532212024]]],\n",
    "[[[  -115.7080078125,  44.69989765840318],[  -114.7412109375,  44.69989765840318],[  -114.7412109375,  45.36758436884978],[  -115.7080078125,  45.36758436884978],[  -115.7080078125,  44.69989765840318]]],\n",
    "[[[  -120.65185546875,  47.517200697839414],[  -119.33349609375,  47.517200697839414],[  -119.33349609375,  48.32703913063476],[  -120.65185546875,  48.32703913063476],[  -120.65185546875,  47.517200697839414]]],\n",
    "[[[  -119.83886718750001,  45.69083283645816],[  -118.38867187500001,  45.69083283645816],[  -118.38867187500001,  46.694667307773116],[  -119.83886718750001,  46.694667307773116],[  -119.83886718750001,  45.69083283645816]]],\n",
    "[[[  -107.09472656249999,  47.45780853075031],[  -105.84228515625,  47.45780853075031],[  -105.84228515625,  48.31242790407178],[  -107.09472656249999,  48.31242790407178],[  -107.09472656249999,  47.45780853075031]]],\n",
    "[[[  -101.57958984375,  46.93526088057719],[  -100.107421875,  46.93526088057719],[  -100.107421875,  47.945786463687185],[  -101.57958984375,  47.945786463687185],[  -101.57958984375,  46.93526088057719]]],\n",
    "[[[  -101.162109375,  44.32384807250689],[  -99.7119140625,  44.32384807250689],[  -99.7119140625,  45.22848059584359],[  -101.162109375,  45.22848059584359],[  -101.162109375,  44.32384807250689]]],\n",
    "[[[  -100.5908203125,  41.261291493919884],[  -99.25048828124999,  41.261291493919884],[  -99.25048828124999,  42.114523952464246],[  -100.5908203125,  42.114523952464246],[  -100.5908203125,  41.261291493919884]]],\n",
    "[[[  -97.9541015625,  37.142803443716836],[  -96.65771484375,  37.142803443716836],[  -96.65771484375,  38.13455657705411],[  -97.9541015625,  38.13455657705411],[  -97.9541015625,  37.142803443716836]]],\n",
    "[[[  -112.78564453124999,  32.91648534731439],[  -111.357421875,  32.91648534731439],[  -111.357421875,  33.925129700072],[  -112.78564453124999,  33.925129700072],[  -112.78564453124999,  32.91648534731439]]],\n",
    "[[[  -106.435546875,  35.15584570226544],[  -105.22705078125,  35.15584570226544],[  -105.22705078125,  36.13787471840729],[  -106.435546875,  36.13787471840729],[  -106.435546875,  35.15584570226544]]],\n",
    "[[[  -97.3828125,  32.45415593941475],[  -96.2841796875,  32.45415593941475],[  -96.2841796875,  33.22949814144951],[  -97.3828125,  33.22949814144951],[  -97.3828125,  32.45415593941475]]],\n",
    "[[[  -97.97607421875,  35.04798673426734],[  -97.00927734375,  35.04798673426734],[  -97.00927734375,  35.764343479667176],[  -97.97607421875,  35.764343479667176],[  -97.97607421875,  35.04798673426734]]],\n",
    "[[[  -97.97607421875,  35.04798673426734],[  -97.00927734375,  35.04798673426734],[  -97.00927734375,  35.764343479667176],[  -97.97607421875,  35.764343479667176],[  -97.97607421875,  35.04798673426734]]],\n",
    "[[[  -95.4052734375,  47.62097541515849],[  -94.24072265625,  47.62097541515849],[  -94.24072265625,  48.28319289548349],[  -95.4052734375,  48.28319289548349],[  -95.4052734375,  47.62097541515849]]],\n",
    "[[[  -94.19677734375,  41.27780646738183],[  -93.09814453125,  41.27780646738183],[  -93.09814453125,  42.13082130188811],[  -94.19677734375,  42.13082130188811],[  -94.19677734375,  41.27780646738183]]],\n",
    "[[[  -93.71337890625,  37.75334401310656],[  -92.6806640625,  37.75334401310656],[  -92.6806640625,  38.51378825951165],[  -93.71337890625,  38.51378825951165],[  -93.71337890625,  37.75334401310656]]],\n",
    "[[[  -90.63720703125,  34.615126683462194],[  -89.47265625,  34.615126683462194],[  -89.47265625,  35.69299463209881],[  -90.63720703125,  35.69299463209881],[  -90.63720703125,  34.615126683462194]]],\n",
    "[[[  -93.05419921875,  30.44867367928756],[  -91.77978515625,  30.44867367928756],[  -91.77978515625,  31.57853542647338],[  -93.05419921875,  31.57853542647338],[  -93.05419921875,  30.44867367928756]]],\n",
    "[[[  -90.02197265625,  44.276671273775186],[  -88.59374999999999,  44.276671273775186],[  -88.59374999999999,  44.98034238084973],[  -90.02197265625,  44.98034238084973],[  -90.02197265625,  44.276671273775186]]],\n",
    "[[[  -90.63720703125,  38.41055825094609],[  -89.49462890625,  38.41055825094609],[  -89.49462890625,  39.18117526158749],[  -90.63720703125,  39.18117526158749],[  -90.63720703125,  38.41055825094609]]],\n",
    "[[[  -87.56103515625,  35.62158189955968],[  -86.28662109375,  35.62158189955968],[  -86.28662109375,  36.4566360115962],[  -87.56103515625,  36.4566360115962],[  -87.56103515625,  35.62158189955968]]],\n",
    "[[[  -90.63720703125,  31.93351676190369],[  -89.49462890625,  31.93351676190369],[  -89.49462890625,  32.731840896865684],[  -90.63720703125,  32.731840896865684],[  -90.63720703125,  31.93351676190369]]],\n",
    "[[[  -69.54345703125,  44.68427737181225],[  -68.5107421875,  44.68427737181225],[  -68.5107421875,  45.336701909968134],[  -69.54345703125,  45.336701909968134],[  -69.54345703125,  44.68427737181225]]],\n",
    "[[[  -73.212890625,  41.49212083968776],[  -72.35595703125,  41.49212083968776],[  -72.35595703125,  42.032974332441405],[  -73.212890625,  42.032974332441405],[  -73.212890625,  41.49212083968776]]],\n",
    "[[[  -77.93701171875,  38.70265930723801],[  -76.97021484375,  38.70265930723801],[  -76.97021484375,  39.26628442213066],[  -77.93701171875,  39.26628442213066],[  -77.93701171875,  38.70265930723801]]],\n",
    "[[[  -79.25537109375,  35.44277092585766],[  -78.15673828125,  35.44277092585766],[  -78.15673828125,  36.13787471840729],[  -79.25537109375,  36.13787471840729],[  -79.25537109375,  35.44277092585766]]],\n",
    "[[[  -81.4306640625,  33.55970664841198],[  -80.44189453125,  33.55970664841198],[  -80.44189453125,  34.288991865037524],[  -81.4306640625,  34.288991865037524],[  -81.4306640625,  33.55970664841198]]],\n",
    "[[[  -84.90234375,  33.394759218577995],[  -83.91357421875,  33.394759218577995],[  -83.91357421875,  34.19817309627726],[  -84.90234375,  34.19817309627726],[  -84.90234375,  33.394759218577995]]],\n",
    "[[[  -82.28759765625,  28.246327971048842],[  -81.2548828125,  28.246327971048842],[  -81.2548828125,  29.209713225868185],[  -82.28759765625,  29.209713225868185],[  -82.28759765625,  28.246327971048842]]],\n",
    "[[[  -109.88525390624999,  42.65012181368022],[  -108.56689453125,  42.65012181368022],[  -108.56689453125,  43.50075243569041],[  -109.88525390624999,  43.50075243569041],[  -109.88525390624999,  42.65012181368022]]],\n",
    "[[[  -117.61962890624999,  39.04478604850143],[  -116.65283203124999,  39.04478604850143],[  -116.65283203124999,  39.740986355883564],[  -117.61962890624999,  39.740986355883564],[  -117.61962890624999,  39.04478604850143]]],\n",
    "[[[  -102.67822265625,  31.42866311735861],[  -101.71142578125,  31.42866311735861],[  -101.71142578125,  32.26855544621476],[  -102.67822265625,  32.26855544621476],[  -102.67822265625,  31.42866311735861]]],\n",
    "[[[  -119.47631835937499,  36.03133177633187],[  -118.58642578124999,  36.03133177633187],[  -118.58642578124999,  36.55377524336089],[  -119.47631835937499,  36.55377524336089],[  -119.47631835937499,  36.03133177633187]]],\n",
    "[[[  -116.224365234375,  33.091541548655215],[  -115.56518554687499,  33.091541548655215],[  -115.56518554687499,  33.568861182555565],[  -116.224365234375,  33.568861182555565],[  -116.224365234375,  33.091541548655215]]]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "evalPolys = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"MultiPolygon\",\n",
    "        \"coordinates\":  [\n",
    "[[[-122.13208008,   41.25126946],[-121.37402344,   41.25126946],[-121.37402344,   41.75892074],[-122.13208008,   41.75892074],[-122.13208008,   41.25126946]]],\n",
    "[[[-121.15979004,   38.45555349],[-120.23144531,   38.45555349],[-120.23144531,   39.14764411],[-121.15979004,   39.14764411],[-121.15979004,   38.45555349]]],\n",
    "[[[-111.80493164,   33.76787602],[-110.83813477,   33.76787602],[-110.83813477,   34.53827854],[-111.80493164,   34.53827854],[-111.80493164,   33.76787602]]],\n",
    "[[[-106.125     ,   37.89280344],[-104.74072266,   37.89280344],[-104.74072266,   38.93638677],[-106.125     ,   38.93638677],[-106.125     ,   37.89280344]]],\n",
    "[[[-119.08886719,   46.44083284],[-117.63867188,   46.44083284],[-117.63867188,   47.44466731],[-119.08886719,   47.44466731],[-119.08886719,   46.44083284]]],\n",
    "[[[-99.84082031,  42.01129149],[-98.50048828,  42.01129149],[-98.50048828,  42.86452395],[-99.84082031,  42.86452395],[-99.84082031,  42.01129149]]],\n",
    "[[[-96.6328125 ,  33.20415594],[-95.53417969,  33.20415594],[-95.53417969,  33.97949814],[-96.6328125 ,  33.97949814],[-96.6328125 ,  33.20415594]]],\n",
    "[[[-93.44677734,  42.02780647],[-92.34814453,  42.02780647],[-92.34814453,  42.8808213 ],[-93.44677734,  42.8808213 ],[-93.44677734,  42.02780647]]],\n",
    "[[[-89.27197266,  45.02667127],[-87.84375   ,  45.02667127],[-87.84375   ,  45.73034238],[-89.27197266,  45.73034238],[-89.27197266,  45.02667127]]],\n",
    "[[[-68.79345703,  45.43427737],[-67.76074219,  45.43427737],[-67.76074219,  46.08670191],[-68.79345703,  46.08670191],[-68.79345703,  45.43427737]]],\n",
    "[[[-80.68066406,  34.30970665],[-79.69189453,  34.30970665],[-79.69189453,  35.03899187],[-80.68066406,  35.03899187],[-80.68066406,  34.30970665]]],\n",
    "[[[-116.86962891,   39.79478605],[-115.90283203,   39.79478605],[-115.90283203,   40.49098636],[-116.86962891,   40.49098636],[-116.86962891,   39.79478605]]]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Polygons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x11e878b90>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "#  Convert the GeoJSONs to feature collections\n",
    "trainFeatures = ee.FeatureCollection(trainPolys.get('features'))\n",
    "evalFeatures = ee.FeatureCollection(evalPolys.get('features'))\n",
    "\n",
    "polyImage = ee.Image(0).byte().paint(trainFeatures, 1).paint(evalFeatures, 2)\n",
    "polyImage = polyImage.updateMask(polyImage)\n",
    "\n",
    "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
    "map = folium.Map(location=[38., -100.], zoom_start=5)\n",
    "folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='training polygons',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An array of images**\n",
    "\n",
    "We have to stack the 2D images (input and output images of the Neural Network) to create a single image from which samples can be taken. Convert the image into an array image in which each pixel stores 256x256 patches of pixels for each band. This is a key step that bears emphasis: to export training patches, convert a multi-band image to [an array image](https://developers.google.com/earth-engine/arrays_array_images#array-images) using [neighborhoodToArray()](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then sample the image at points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_into_array(url, collections, bands, kernelSize, startDate, stopDate, scale):\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    \n",
    "    for i, collection in enumerate(collections):\n",
    "        payload =   {\n",
    "            \"collection\": collection,\n",
    "            \"start\": startDate,\n",
    "            \"end\": stopDate,\n",
    "            \"scale\": scale\n",
    "        }\n",
    "        \n",
    "        output = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "        \n",
    "        if i == 0:\n",
    "            image = ee.deserializer.fromJSON(output.json()['composite']).select(bands[i])\n",
    "        else:\n",
    "            featureStack = ee.Image.cat([image,\\\n",
    "                                         ee.deserializer.fromJSON(output.json()['composite']).select(bands[i])\\\n",
    "                                        ]).float()\n",
    "            \n",
    "    list = ee.List.repeat(1, kernelSize)\n",
    "    lists = ee.List.repeat(list, kernelSize)\n",
    "    kernel = ee.Kernel.fixed(kernelSize, kernelSize, lists)\n",
    "    \n",
    "    arrays = featureStack.neighborhoodToArray(kernel)\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = f'https://us-central1-skydipper-196010.cloudfunctions.net/ee_pre_processing'\n",
    "collections = [inCollection, outCollection]\n",
    "bands = [inBands, outBands]\n",
    "kernelSize = 256\n",
    "\n",
    "arrays = image_into_array(url, collections, bands, kernelSize, startDate, stopDate, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export TFRecords**\n",
    "\n",
    "The mapped data look reasonable so take a sample from each polygon and merge the results into a single export. The key step is sampling the array image at points, to get all the pixels in a 256x256 neighborhood at each point. It's worth noting that to build the training and testing data for the FCNN, you export a single TFRecord file that contains patches of pixel values in each record. You do NOT need to export each training/testing patch to a different image. Since each record potentially contains a lot of data (especially with big patches or many input bands), some manual sharding of the computation is necessary to avoid the computed value too large error. Specifically, the following code takes multiple (smaller) samples within each geometry, merging the results to get a single export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_TFRecords(arrays, scale, nShards, sampleSize, features, polysLists, baseNames, bucket, folder, selectors):\n",
    "    # Export all the training/evaluation data (in many pieces), with one task per geometry.\n",
    "    filePaths = []\n",
    "    for i, feature in enumerate(features):\n",
    "        for g in range(feature.size().getInfo()):\n",
    "            geomSample = ee.FeatureCollection([])\n",
    "            for j in range(nShards):\n",
    "                sample = arrays.sample(\n",
    "                    region = ee.Feature(polysLists[i].get(g)).geometry(), \n",
    "                    scale = scale, \n",
    "                    numPixels = sampleSize / nShards, # Size of the shard.\n",
    "                    seed = j,\n",
    "                    tileScale = 8\n",
    "                )\n",
    "                geomSample = geomSample.merge(sample)\n",
    "                \n",
    "            desc = baseNames[i] + '_g' + str(g)\n",
    "            \n",
    "            filePaths.append(bucket+ '/' + folder + '/' + desc)\n",
    "            \n",
    "            task = ee.batch.Export.table.toCloudStorage(\n",
    "                collection = geomSample,\n",
    "                description = desc, \n",
    "                bucket = bucket, \n",
    "                fileNamePrefix = folder + '/' + desc,\n",
    "                fileFormat = 'TFRecord',\n",
    "                selectors = selectors\n",
    "            )\n",
    "            task.start()\n",
    "            \n",
    "    return filePaths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeoJSONs_to_FeatureCollections(multipolygon):\n",
    "    # Make a list of Features\n",
    "    features = []\n",
    "    for i in range(len(multipolygon.get('features')[0].get('geometry').get('coordinates'))):\n",
    "        features.append(\n",
    "            ee.Feature(\n",
    "                ee.Geometry.Polygon(\n",
    "                    multipolygon.get('features')[0].get('geometry').get('coordinates')[i]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # Create a FeatureCollection from the list and print it.\n",
    "    return ee.FeatureCollection(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the GeoJSONs to feature collections\n",
    "trainFeatures = GeoJSONs_to_FeatureCollections(trainPolys)\n",
    "evalFeatures = GeoJSONs_to_FeatureCollections(evalPolys)\n",
    "\n",
    "# Convert the feature collections to lists for iteration.\n",
    "trainPolysList = trainFeatures.toList(trainFeatures.size())\n",
    "evalPolysList = evalFeatures.toList(evalFeatures.size())\n",
    "\n",
    "# These numbers determined experimentally.\n",
    "nShards  = int(sampleSize/20)#100 # Number of shards in each polygon.\n",
    "\n",
    "features = [trainFeatures, evalFeatures]\n",
    "polysLists = [trainPolysList, evalPolysList]\n",
    "baseNames = ['training_patches', 'eval_patches']\n",
    "bucket = 'skydipper_materials'\n",
    "folder = 'cnn-models/'+datasetName+'/data'\n",
    "selectors = inBands + outBands\n",
    "\n",
    "# Export all the training/evaluation data (in many pieces), with one task per geometry.\n",
    "filePaths = export_TFRecords(arrays, scale, nShards, sampleSize, features, polysLists, baseNames, bucket, folder, selectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Inspect data\n",
    "Load the data exported from Earth Engine into a tf.data.Dataset. \n",
    "\n",
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow setup.\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(proto):\n",
    "    \"\"\"The parsing function.\n",
    "    Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "    Args:\n",
    "      example_proto: a serialized Example.\n",
    "    Returns: \n",
    "      A dictionary of tensors, keyed by feature name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define your tfrecord \n",
    "    features = inBands + outBands\n",
    "    \n",
    "    # Specify the size and shape of patches expected by the model.\n",
    "    kernel_shape = [kernel_size, kernel_size]\n",
    "    columns = [\n",
    "      tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) for k in features\n",
    "    ]\n",
    "    features_dict = dict(zip(features, columns))\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.io.parse_single_example(proto, features_dict)\n",
    "\n",
    "    # Convert a dictionary of tensors to a tuple of (inputs, outputs)\n",
    "    inputsList = [parsed_features.get(key) for key in features]\n",
    "    stacked = tf.stack(inputsList, axis=0)\n",
    "    \n",
    "    # Convert the tensors into a stack in HWC shape\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    \n",
    "    return stacked[:,:,:len(inBands)], stacked[:,:,len(inBands):]\n",
    "\n",
    "def get_dataset(glob, inBands, outBands, kernel_size, buffer_size, batch_size):\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "    Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "    \"\"\"\n",
    "    glob = tf.compat.v1.io.gfile.glob(glob)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=5)\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = 'Landsat8_Cropland'\n",
    "baseNames = ['training_patches', 'eval_patches']\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "        \n",
    "bucket = env.bucket_name\n",
    "folder = 'cnn-models/'+datasetName+'/data'\n",
    "kernel_size = 256\n",
    "buffer_size = 100\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob = 'gs://' + bucket + '/' + folder + '/' + baseNames[0] + '_g0'+ '*'\n",
    "dataset = get_dataset(glob, inBands, outBands, kernel_size, buffer_size, batch_size)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the first record**\n",
    "\n",
    "Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arr = iter(dataset.take(1)).next()\n",
    "input_arr = training_arr[0].numpy()\n",
    "print(input_arr.shape)\n",
    "output_arr = training_arr[1].numpy()\n",
    "print(output_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And display the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_channels(data, nChannels, titles = False):\n",
    "    if nChannels == 1:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(data[:,:,0])\n",
    "        if titles:\n",
    "            plt.title(titles[0])\n",
    "    else:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=nChannels, figsize=(5*nChannels,5))\n",
    "        for i in range(nChannels):\n",
    "            ax = axs[i]\n",
    "            ax.imshow(data[:,:,i])\n",
    "            if titles:\n",
    "                ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(input_arr[3,:,:,:], input_arr.shape[3], titles=inBands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(output_arr[3,:,:,:], output_arr.shape[3], titles=outBands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Training the model in AI Platform\n",
    "### Training code package setup\n",
    "\n",
    "It's necessary to create a Python package to hold the training code.  Here we're going to get started with that by creating a folder for the package and adding an empty `__init__.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = 'AI_Platform/cnn_trainer'\n",
    "PACKAGE_FOLDER = '/trainer'\n",
    "\n",
    "!rm -r {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH+PACKAGE_FOLDER}\n",
    "!touch {ROOT_PATH+PACKAGE_FOLDER}/__init__.py\n",
    "!ls -l {ROOT_PATH+PACKAGE_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Files**\n",
    "\n",
    "`env.py` file into the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp env.py {ROOT_PATH+PACKAGE_FOLDER}/env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**\n",
    "\n",
    "These variables need to be stored in a place where other code can access them.  There are a variety of ways of accomplishing that, but here we'll use the `%%writefile` command to write the contents of the code cell to a file called `config.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/config.py\n",
    "\n",
    "from . import env\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your Google Cloud Storage bucket\n",
    "bucket = env.bucket_name\n",
    "\n",
    "# Specify names of output locations in Cloud Storage.\n",
    "dataset_name = 'Landsat8_Cropland'\n",
    "job_dir = 'gs://' + bucket + '/' + 'cnn-models/'+ dataset_name +'/trainer'\n",
    "model_dir = job_dir + '/model'\n",
    "logs_dir = job_dir + '/logs'\n",
    "\n",
    "# Pre-computed training and eval data.\n",
    "base_names = ['training_patches', 'eval_patches']\n",
    "folder = 'cnn-models/'+dataset_name+'/data'\n",
    "\n",
    "# Specify inputs/outputs to the model\n",
    "in_bands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "out_bands = ['cropland', 'land', 'water', 'urban']\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "kernel_size = 256\n",
    "\n",
    "# Sizes of the training and evaluation datasets.\n",
    "train_size = 1000*47\n",
    "eval_size = 1000*12\n",
    "\n",
    "# Specify model training parameters.\n",
    "batch_size = 16\n",
    "epochs = 25\n",
    "shuffle_size = 2000\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
    "loss = 'mse'\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training/evaluation data**\n",
    "\n",
    "The following is code to load training/evaluation data.  Write this into `util.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/util.py\n",
    "\"\"\"Utilities to download and preprocess the data.\"\"\"\n",
    "\n",
    "from . import config\n",
    "import tensorflow as tf\n",
    "\n",
    "def parse_function(proto):\n",
    "    \"\"\"The parsing function.\n",
    "    Read a serialized example into the structure defined by features_dict.\n",
    "    Args:\n",
    "      example_proto: a serialized Example.\n",
    "    Returns: \n",
    "      A dictionary of tensors, keyed by feature name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define your tfrecord \n",
    "    features = config.in_bands + config.out_bands\n",
    "    \n",
    "    # Specify the size and shape of patches expected by the model.\n",
    "    kernel_shape = [config.kernel_size, config.kernel_size]\n",
    "    columns = [\n",
    "      tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) for k in features\n",
    "    ]\n",
    "    features_dict = dict(zip(features, columns))\n",
    "    \n",
    "    # Load one example\n",
    "    parsed_features = tf.io.parse_single_example(proto, features_dict)\n",
    "\n",
    "    # Convert a dictionary of tensors to a tuple of (inputs, outputs)\n",
    "    inputs_list = [parsed_features.get(key) for key in features]\n",
    "    stacked = tf.stack(inputs_list, axis=0)\n",
    "    \n",
    "    # Convert the tensors into a stack in HWC shape\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    \n",
    "    return stacked[:,:,:len(config.in_bands)], stacked[:,:,len(config.in_bands):]\n",
    "\n",
    "def get_dataset(glob):\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "    Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "    \"\"\"\n",
    "    glob = tf.compat.v1.io.gfile.glob(glob)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=5)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_training_dataset():\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "    Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "    \"\"\"\n",
    "    glob = 'gs://' + config.bucket + '/' + config.folder + '/' + config.base_names[0] + '*'\n",
    "    dataset = get_dataset(glob)\n",
    "    dataset = dataset.shuffle(config.shuffle_size).batch(config.batch_size).repeat()\n",
    "    return dataset\n",
    "\n",
    "def get_evaluation_dataset():\n",
    "    \"\"\"Get the preprocessed evaluation dataset\n",
    "    Returns: \n",
    "      A tf.data.Dataset of evaluation data.\n",
    "    \"\"\"\n",
    "    glob = 'gs://' + config.bucket + '/' + config.folder + '/' + config.base_names[1] + '*'\n",
    "    dataset = get_dataset(glob)\n",
    "    dataset = dataset.batch(1).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that `util.py` is functioning as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_Platform.cnn_trainer.trainer import config\n",
    "from AI_Platform.cnn_trainer.trainer import util\n",
    "\n",
    "training_dataset = util.get_training_dataset()\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**\n",
    "\n",
    "Here we use the Keras implementation of the SegNet or DeepVel models. We write this into `model.py`.\n",
    "\n",
    "**`DeepVel`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/model.py\n",
    "\n",
    "\"\"\"DeepVel model.\n",
    "\n",
    "DeepVel: Deep learning for the estimation of horizontal\n",
    "velocities at the solar surface\n",
    "https://www.aanda.org/articles/aa/pdf/2017/08/aa30783-17.pdf\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Add\n",
    "from tensorflow.python.keras.layers.core import Layer, Activation, Reshape\n",
    "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "\n",
    "\n",
    "def residual(inputs, filter_size, kernel):\n",
    "    x = Conv2D(filter_size, kernel, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filter_size, kernel, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, inputs])\n",
    "\n",
    "    return x\n",
    "\n",
    "def create_keras_model(inputShape, nClasses):\n",
    "    \"\"\"\n",
    "    DeepVel model\n",
    "    ----------\n",
    "    inputShape : tuple\n",
    "        Tuple with the dimensions of the input data (ny, nx, nBands). \n",
    "    nClasses : int\n",
    "            Number of classes.\n",
    "    \"\"\"\n",
    "        \n",
    "    filter_size = 64\n",
    "    kernel = (3, 3)        \n",
    "    n_residual_layers = 5  \n",
    "\n",
    "            \n",
    "    inputs = Input(shape=inputShape, name='image')\n",
    "\n",
    "    conv = Conv2D(filter_size, kernel, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = residual(conv, filter_size, kernel)\n",
    "    for i in range(n_residual_layers):\n",
    "        x = residual(x, filter_size, kernel)\n",
    "\n",
    "    x = Conv2D(filter_size, kernel, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, conv])\n",
    "\n",
    "    outputs = Conv2D(nClasses, (1, 1), activation='softmax', padding='same', kernel_initializer='he_normal', name= 'output')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='deepvel')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`SegNet`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/model.py\n",
    "\n",
    "\"\"\"SegNet model.\n",
    "\n",
    "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation\n",
    "https://arxiv.org/pdf/1511.00561.pdf\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.python.keras import Model\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers.core import Layer, Activation, Reshape, Permute\n",
    "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "\n",
    "def create_keras_model(inputShape, nClasses):\n",
    "    \"\"\"\n",
    "    SegNet model\n",
    "    ----------\n",
    "    inputShape : tuple\n",
    "        Tuple with the dimensions of the input data (ny, nx, nBands). \n",
    "    nClasses : int\n",
    "         Number of classes.\n",
    "    \"\"\"\n",
    "\n",
    "    filter_size = 64\n",
    "    kernel = (3, 3)        \n",
    "    pad = (1, 1)\n",
    "    pool_size = (2, 2)\n",
    "        \n",
    "\n",
    "    inputs = Input(shape=inputShape, name= 'image')\n",
    "        \n",
    "    # Encoder\n",
    "    x = Conv2D(64, kernel, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=pool_size)(x)\n",
    "            \n",
    "    x = Conv2D(128, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=pool_size)(x)\n",
    "            \n",
    "    x = Conv2D(256, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=pool_size)(x)\n",
    "            \n",
    "    x = Conv2D(512, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "            \n",
    "            \n",
    "    # Decoder\n",
    "    x = Conv2D(512, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D(size=pool_size)(x)\n",
    "            \n",
    "    x = Conv2D(256, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D(size=pool_size)(x)\n",
    "            \n",
    "    x = Conv2D(128, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D(size=pool_size)(x)\n",
    "            \n",
    "    x = Conv2D(64, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "            \n",
    "    x = Conv2D(nClasses, (1, 1), padding='valid')(x)\n",
    "    \n",
    "    outputs = Activation('softmax', name= 'output')(x)\n",
    "        \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='segnet')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that `model.py` is functioning as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_Platform.cnn_trainer.trainer import config\n",
    "from AI_Platform.cnn_trainer.trainer import model\n",
    "\n",
    "model = model.create_keras_model(inputShape = (None, None, len(config.in_bands)), nClasses = len(config.out_bands))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training task**\n",
    "\n",
    "The following will create `task.py`, which will get the training and evaluation data, train the model and save it when it's done in a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/task.py\n",
    "\"\"\"Trains a Keras model\"\"\"\n",
    "\n",
    "from . import config\n",
    "from . import model\n",
    "from . import util\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def train_and_evaluate():\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the Keras Model\n",
    "    keras_model = model.create_keras_model(inputShape = (None, None, len(config.in_bands)), nClasses = len(config.out_bands))\n",
    "\n",
    "    # Compile Keras model\n",
    "    keras_model.compile(loss=config.loss, optimizer=config.optimizer, metrics=config.metrics)\n",
    "\n",
    "\n",
    "    # Pass a tfrecord\n",
    "    training_dataset = util.get_training_dataset()\n",
    "    evaluation_dataset = util.get_evaluation_dataset()\n",
    "\n",
    "    # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(config.logs_dir)\n",
    "\n",
    "    # Train model\n",
    "    keras_model.fit(\n",
    "        x=training_dataset,\n",
    "        steps_per_epoch=int(config.train_size / config.batch_size),\n",
    "        epochs=config.epochs,\n",
    "        validation_data=evaluation_dataset,\n",
    "        validation_steps=int(config.eval_size / config.batch_size),\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard_cb])\n",
    "\n",
    "    tf.contrib.saved_model.save_keras_model(keras_model, os.path.join(config.model_dir, str(int(time.time()))))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity('INFO')\n",
    "    train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using GPUs**\n",
    "\n",
    "AI Platform lets you run any TensorFlow training application on a GPU-enabled machine. Learn more about [using GPUs for training models in the cloud](https://cloud.google.com/ml-engine/docs/tensorflow/using-gpus#submit-job).\n",
    "We define a `config.yaml` file that describes the GPU options we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {ROOT_PATH}/config.yaml\n",
    "\n",
    "trainingInput:\n",
    "    scaleTier: CUSTOM\n",
    "    # A single NVIDIA Tesla V100 GPU\n",
    "    masterType: large_model_v100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the package to AI Platform for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# INSERT YOUR PROJECT HERE!\n",
    "PROJECT_ID = env.project_id\n",
    "REGION = \"europe-west4\"\n",
    "\n",
    "JOB_NAME = 'job_v' + str(int(time.time()))\n",
    "TRAINER_PACKAGE_PATH = 'AI_Platform/cnn_trainer/trainer'\n",
    "MAIN_TRAINER_MODULE = 'trainer.task'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up your GCP project**\n",
    "\n",
    "Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authenticate your GCP account**\n",
    "\n",
    "Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS 'privatekey.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit a training job to AI Platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
    "    --job-dir {config.job_dir} \\\n",
    "    --package-path {TRAINER_PACKAGE_PATH} \\\n",
    "    --module-name {MAIN_TRAINER_MODULE} \\\n",
    "    --region {REGION} \\\n",
    "    --config {ROOT_PATH}/config.yaml \\\n",
    "    --runtime-version 1.14 \\\n",
    "    --python-version 3.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monitor the training job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = !gcloud ai-platform jobs describe {JOB_NAME} --project {PROJECT_ID}\n",
    "state = desc.grep('state:')[0].split(':')[1].strip()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Prepare the model for making predictions in Earth Engine\n",
    "\n",
    "Before we can use the model in Earth Engine, it needs to be hosted by AI Platform.  But before we can host the model on AI Platform we need to *EEify* (a new word!) it.  The EEification process merely appends some extra operations to the input and outputs of the model in order to accomdate the interchange format between pixels from Earth Engine (float32) and inputs to AI Platform (base64).  (See [this doc](https://cloud.google.com/ml-engine/docs/online-predict#binary_data_in_prediction_input) for details.)  \n",
    "\n",
    "**`earthengine model prepare`**\n",
    "\n",
    "The EEification process is handled for you using the Earth Engine command `earthengine model prepare`.  To use that command, we need to specify the input and output model directories and the name of the input and output nodes in the TensorFlow computation graph.  We can do all that programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "\n",
    "# Specify names of input locations in Cloud Storage.\n",
    "dataset_name = 'Landsat8_Cropland'\n",
    "job_dir = 'gs://' + env.bucket_name + '/' + 'cnn-models/' + dataset_name + '/trainer'\n",
    "model_dir = job_dir + '/model'\n",
    "PROJECT_ID = env.project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the directory with the latest timestamp, in case you've trained multiple times\n",
    "exported_model_dirs = ! gsutil ls {model_dir}\n",
    "saved_model_path = exported_model_dirs[-1]\n",
    "\n",
    "folder_name = saved_model_path.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ikersanchez/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_path, 'serve')\n",
    "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
    "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
    "\n",
    "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
    "# model only has a single input and a single output.\n",
    "input_name = None\n",
    "for k,v in inputs.items():\n",
    "    input_name = v.name\n",
    "    break\n",
    "\n",
    "output_name = None\n",
    "for k,v in outputs.items():\n",
    "    output_name = v.name\n",
    "    break\n",
    "\n",
    "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
    "# AI Platform inputs and outputs, respectively.\n",
    "import json\n",
    "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
    "output_dict = \"'\" + json.dumps({output_name: \"prediction\"}) + \"'\"\n",
    "\n",
    "# Put the EEified model next to the trained model directory.\n",
    "EEIFIED_DIR = job_dir + '/eeified/' + folder_name\n",
    "\n",
    "# You need to set the project before using the model prepare command.\n",
    "#!earthengine set_project {PROJECT_ID}\n",
    "#!earthengine model prepare --source_dir {saved_model_path} --dest_dir {EEIFIED_DIR} --input {input_dict} --output {output_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployed the model to AI Platform**\n",
    "\n",
    "Before it's possible to get predictions from the trained and EEified model, it needs to be deployed on AI Platform.  The first step is to create the model.  The second step is to create a version.  See [this guide](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models) for details.  Note that models and versions can be monitored from the [AI Platform models page](http://console.cloud.google.com/ai-platform/models) of the Cloud Console. \n",
    "\n",
    "To ensure that the model is ready for predictions without having to warm up nodes, you can use a configuration yaml file to set the scaling type of this version to autoScaling, and, set a minimum number of nodes for the version. This will ensure there are always nodes on stand-by, however, you will be charged as long as they are running. For this example, we'll set the minNodes to 10. That means that at a minimum, 10 nodes are always up and running and waiting for predictions. The number of nodes will also scale up automatically if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "autoScaling:\n",
    "    minNodes: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "REGION = \"us-west1\"\n",
    "MODEL_NAME = 'deepvel_'+dataset_name\n",
    "VERSION_NAME = 'v' + folder_name\n",
    "print('Creating version: ' + VERSION_NAME)\n",
    "\n",
    "!gcloud ai-platform models create {MODEL_NAME} \n",
    "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --model {MODEL_NAME} \\\n",
    "  --origin {EEIFIED_DIR} \\\n",
    "  --runtime-version=1.14 \\\n",
    "  --framework \"TENSORFLOW\" \\\n",
    "  --python-version=3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Predict in Earth Engine\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inCollection = 'Landsat8_SR'\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "startDate = '2016-01-01'\n",
    "stopDate = '2016-12-31'\n",
    "scale = 30 #scale in meters\n",
    "\n",
    "# Model variables\n",
    "PROJECT_ID = env.project_id\n",
    "MODEL_NAME = 'deepvel_Landsat8_Cropland'\n",
    "VERSION_NAME = 'v' + folder_name\n",
    "modelType = 'segmentation'\n",
    "\n",
    "# polygon where we want to display de predictions\n",
    "geometry = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [\n",
    "              -121.59221649169923,\n",
    "              38.53796141693008\n",
    "            ],\n",
    "            [\n",
    "              -121.44081115722655,\n",
    "              38.53796141693008\n",
    "            ],\n",
    "            [\n",
    "              -121.44081115722655,\n",
    "              38.626526838378076\n",
    "            ],\n",
    "            [\n",
    "              -121.59221649169923,\n",
    "              38.626526838378076\n",
    "            ],\n",
    "            [\n",
    "              -121.59221649169923,\n",
    "              38.53796141693008\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ee.Model.fromAiPlatformPredictor`**\n",
    "\n",
    "There is now a trained model, prepared for serving to Earth Engine, hosted and versioned on AI Platform.  \n",
    "We can now connect Earth Engine directly to the trained model for inference.  You do that with the `ee.Model.fromAiPlatformPredictor` command.\n",
    "For this command to work, we need to know a lot about the model.  To connect to the model, you need to know the name and version.\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "You need to be able to recreate the imagery on which it was trained in order to perform inference.  Specifically, you need to create an array-valued input from the scaled data and use that for input.  (Recall that the new input node is named `array`, which is convenient because the array image has one band, named `array` by default.)  The inputs will be provided as 144x144 patches (`inputTileSize`), at 30-meter resolution (`proj`), but 8 pixels will be thrown out (`inputOverlapSize`) to minimize boundary effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "payload =   {\n",
    "    \"collection\": inCollection,\n",
    "    \"start\": startDate,\n",
    "    \"end\": stopDate,\n",
    "    \"scale\": scale\n",
    "}\n",
    "\n",
    "url = f'https://us-central1-skydipper-196010.cloudfunctions.net/ee_pre_processing'\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "output_pre_processing = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "output_pre_processing.json()\n",
    "\n",
    "image = ee.deserializer.fromJSON(output_pre_processing.json()['composite'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select bands and convert them into float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bands': [{'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'B1'},\n",
       "  {'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'B2'},\n",
       "  {'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'B3'},\n",
       "  {'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'B4'},\n",
       "  {'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'B5'},\n",
       "  {'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'B6'},\n",
       "  {'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'B7'}],\n",
       " 'type': 'Image'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = image.select(inBands).float()\n",
    "image.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outputs**\n",
    "\n",
    "The output (which you also need to know)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bands': [{'crs': 'EPSG:4326',\n",
       "   'crs_transform': [0.00026949458523585647,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.00026949458523585647,\n",
       "    0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'cropland'},\n",
       "  {'crs': 'EPSG:4326',\n",
       "   'crs_transform': [0.00026949458523585647,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.00026949458523585647,\n",
       "    0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'land'},\n",
       "  {'crs': 'EPSG:4326',\n",
       "   'crs_transform': [0.00026949458523585647,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.00026949458523585647,\n",
       "    0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'water'},\n",
       "  {'crs': 'EPSG:4326',\n",
       "   'crs_transform': [0.00026949458523585647,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.00026949458523585647,\n",
       "    0.0],\n",
       "   'data_type': {'precision': 'float', 'type': 'PixelType'},\n",
       "   'id': 'urban'}],\n",
       " 'type': 'Image'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model and use it for prediction.\n",
    "model = ee.Model.fromAiPlatformPredictor(\n",
    "    projectName = PROJECT_ID,\n",
    "    modelName = MODEL_NAME,\n",
    "    version = VERSION_NAME,\n",
    "    inputTileSize = [144, 144],\n",
    "    inputOverlapSize = [8, 8],\n",
    "    proj = ee.Projection('EPSG:4326').atScale(scale),\n",
    "    fixInputProj = True,\n",
    "    outputBands = {'prediction': {\n",
    "        'type': ee.PixelType.float(),\n",
    "        'dimensions': 1,\n",
    "      }                  \n",
    "    }\n",
    ")\n",
    "predictions = model.predictImage(image.toArray()).arrayFlatten([outBands])\n",
    "predictions.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the prediction area with the polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip the prediction area with the polygon\n",
    "polygon = ee.Geometry.Polygon(geometry.get('features')[0].get('geometry').get('coordinates'))\n",
    "predictions = predictions.clip(polygon)\n",
    "\n",
    "# Get centroid\n",
    "centroid = polygon.centroid().getInfo().get('coordinates')[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentate image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelType == 'segmentation':\n",
    "    maxValues = predictions.reduce(ee.Reducer.max())\n",
    "\n",
    "    predictions = predictions.addBands(maxValues)\n",
    "\n",
    "    expression = \"\"\n",
    "    for n, band in enumerate(outBands):\n",
    "        expression = expression + f\"(b('{band}') == b('max')) ? {str(n+1)} : \"\n",
    "\n",
    "    expression = expression + f\"0\"\n",
    "\n",
    "    segmentation = predictions.expression(expression)\n",
    "    predictions = predictions.addBands(segmentation.mask(segmentation).select(['constant'], ['categories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display**\n",
    "\n",
    "Use folium to visualize the input imagery and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <script>L_PREFER_CANVAS=false; L_NO_TOUCH=false; L_DISABLE_3D=false;</script>
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.js"></script>
    <script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.4.0/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
    <meta name="viewport" content="width=device-width,
        initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <style>#map_cfc4a7d140484bf1b4874f4d7f8ad9ce {
        position: relative;
        width: 100.0%;
        height: 100.0%;
        left: 0.0%;
        top: 0.0%;
        }
    </style>
</head>
<body>    
    
    <div class="folium-map" id="map_cfc4a7d140484bf1b4874f4d7f8ad9ce" ></div>
</body>
<script>    
    
    
        var bounds = null;
    

    var map_cfc4a7d140484bf1b4874f4d7f8ad9ce = L.map(
        'map_cfc4a7d140484bf1b4874f4d7f8ad9ce', {
        center: [38.58225940711006, -121.51651382446346],
        zoom: 12,
        maxBounds: bounds,
        layers: [],
        worldCopyJump: false,
        crs: L.CRS.EPSG3857,
        zoomControl: true,
        });


    
    var tile_layer_4022f45ea062403ab6391f7e6691cf71 = L.tileLayer(
        'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png',
        {
        "attribution": null,
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_cfc4a7d140484bf1b4874f4d7f8ad9ce);
    var tile_layer_d809efc30b2d461786d530c66ff2322e = L.tileLayer(
        'https://earthengine.googleapis.com/map/c5d3969e6c0185a73d2dc77a88875635/{z}/{x}/{y}?token=552da3687ad081337b7808a6e8082109',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_cfc4a7d140484bf1b4874f4d7f8ad9ce);
    var tile_layer_1763675408f14cb292b23ec5230ea077 = L.tileLayer(
        'https://earthengine.googleapis.com/map/545ef90b7c65d55e462d16cf1f21b029/{z}/{x}/{y}?token=f394282eb3881c097c15d066d10cbda4',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_cfc4a7d140484bf1b4874f4d7f8ad9ce);
    var tile_layer_d87dae85021447f9afd74de117884096 = L.tileLayer(
        'https://earthengine.googleapis.com/map/aed58d8c65241d6ca9c41d30ce4cd282/{z}/{x}/{y}?token=c26c249cc53b59ef480376b91fae0bf8',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_cfc4a7d140484bf1b4874f4d7f8ad9ce);
    var tile_layer_d267a52f51c44e27a5895aa2c94c289b = L.tileLayer(
        'https://earthengine.googleapis.com/map/a1837a0539240ad937040ef7982486fd/{z}/{x}/{y}?token=74a93f0c0495835de68f9d106f9c3319',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_cfc4a7d140484bf1b4874f4d7f8ad9ce);
    var tile_layer_3339a171ea7745719e242ec01b25636d = L.tileLayer(
        'https://earthengine.googleapis.com/map/0b60934937a79ec303bbacf72d8a1390/{z}/{x}/{y}?token=3cc86638e2fd6d68e886e45841c1ba2c',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_cfc4a7d140484bf1b4874f4d7f8ad9ce);
    var tile_layer_b88bece7d877416f873f99ddb1cf094d = L.tileLayer(
        'https://earthengine.googleapis.com/map/c3a707e1d87fd85a5e5ff65967aec547/{z}/{x}/{y}?token=394fd917094fd83240743fd7cd72c8a6',
        {
        "attribution": "Google Earth Engine",
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_cfc4a7d140484bf1b4874f4d7f8ad9ce);
    
            var layer_control_b3ad537c91bf435e9810b8be05329272 = {
                base_layers : { "openstreetmap" : tile_layer_4022f45ea062403ab6391f7e6691cf71, },
                overlays : { "median composite" : tile_layer_d809efc30b2d461786d530c66ff2322e,"cropland" : tile_layer_1763675408f14cb292b23ec5230ea077,"land" : tile_layer_d87dae85021447f9afd74de117884096,"water" : tile_layer_d267a52f51c44e27a5895aa2c94c289b,"urban" : tile_layer_3339a171ea7745719e242ec01b25636d,"categories" : tile_layer_b88bece7d877416f873f99ddb1cf094d, }
                };
            L.control.layers(
                layer_control_b3ad537c91bf435e9810b8be05329272.base_layers,
                layer_control_b3ad537c91bf435e9810b8be05329272.overlays,
                {position: 'topright',
                 collapsed: true,
                 autoZIndex: true
                }).addTo(map_cfc4a7d140484bf1b4874f4d7f8ad9ce);
            
        
</script>\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x11ded9610>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 1})\n",
    "map = folium.Map(location=centroid, zoom_start=12)\n",
    "folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(map)\n",
    "\n",
    "for band in outBands:\n",
    "    mapid = predictions.getMapId({'bands': [band], 'min': 0, 'max': 1})\n",
    "    \n",
    "    folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name=band,\n",
    "      ).add_to(map)\n",
    "\n",
    "\n",
    "if modelType == 'segmentation':\n",
    "    mapid = predictions.getMapId({'bands': ['categories'], 'min': 1, 'max': len(outBands)})\n",
    "    \n",
    "    folium.TileLayer(\n",
    "        tiles=EE_TILES.format(**mapid),\n",
    "        attr='Google Earth Engine',\n",
    "        overlay=True,\n",
    "        name='categories',\n",
    "      ).add_to(map)\n",
    "    \n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Predict in AI Platform\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inCollection = 'Landsat8_SR'\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "startDate = '2016-01-01'\n",
    "stopDate = '2016-12-31'\n",
    "scale = 30 #scale in meters\n",
    "\n",
    "# Model variables\n",
    "PROJECT_ID = env.project_id\n",
    "MODEL_NAME = 'segnet_Landsat8_Cropland'\n",
    "VERSION_NAME = VERSION_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify names of input locations in Cloud Storage.\n",
    "dataset_name = 'Landsat8_Cropland'\n",
    "job_dir = 'gs://' + bucket + '/' + 'cnn-models/' + dataset_name + '/trainer'\n",
    "model_dir = job_dir + '/model' \n",
    "\n",
    "exported_model_dirs = ! gsutil ls {model_dir}\n",
    "\n",
    "# Pick the directory with the latest timestamp, in case you've trained multiple times\n",
    "saved_model_path = exported_model_dirs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = 'segnet_Landsat8_Cropland'\n",
    "VERSION_NAME = VERSION_NAME\n",
    "print('Creating version: ' + VERSION_NAME)\n",
    "\n",
    "!gcloud ai-platform models create {MODEL_NAME} \n",
    "\n",
    "# Create model version based on that SavedModel directory\n",
    "! gcloud ai-platform versions create {VERSION_NAME} \\\n",
    "  --model {MODEL_NAME} \\\n",
    "  --runtime-version 1.13 \\\n",
    "  --python-version 3.5 \\\n",
    "  --framework tensorflow \\\n",
    "  --origin {saved_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = 'Landsat8_Cropland'\n",
    "baseNames = ['training_patches', 'eval_patches']\n",
    "inBands = ['B1','B2','B3','B4','B5','B6','B7']\n",
    "outBands = ['cropland', 'land', 'water', 'urban']\n",
    "        \n",
    "folder = 'cnn-models/'+datasetName+'/data'\n",
    "kernel_size = 256\n",
    "buffer_size = 100\n",
    "batch_size = 4\n",
    "\n",
    "glob = 'gs://' + env.bucket_name + '/' + folder + '/' + baseNames[0] + '_g0'+ '*'\n",
    "dataset = get_dataset(glob, inBands, outBands, kernel_size, buffer_size, batch_size)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(input_arr[2,:80,:80,:], input_arr.shape[3], titles=inBands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formatting input data for online prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = input_arr[2,:80,:80,:]\n",
    "data = np.around(data,2).tolist()\n",
    "instance = {\"image\" : data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requesting predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import googleapiclient\n",
    "\n",
    "def predict_json(project, model, instances, privatekey_path, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "\n",
    "    Args:\n",
    "        project (str): project where the AI Platform Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"    \n",
    "    # To authenticate set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "    credentials = service_account.Credentials.from_service_account_file(privatekey_path)\n",
    "    \n",
    "    # Create the AI Platform service object.\n",
    "    service = googleapiclient.discovery.build('ml', 'v1', credentials=credentials)\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit the online prediction request**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privatekey_path = 'privatekey.json'\n",
    "response = predict_json(project=PROJECT_ID, model=MODEL_NAME, instances=instance, privatekey_path=privatekey_path, version=VERSION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.array(response[0].get('output'))\n",
    "output.shape\n",
    "display_channels(output, output.shape[2], titles=['cropland', 'land', 'water', 'urban'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data post-processing\n",
    "\n",
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
