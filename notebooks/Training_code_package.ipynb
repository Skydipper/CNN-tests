{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code package setup for AI Platform\n",
    "\n",
    "It's necessary to create a Python package to hold the training code.  Here we're going to get started with that by creating a folder for the package and adding an empty `__init__.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from google.cloud import storage\n",
    "import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "-rw-r--r--  1 ikersanchez  staff  0 Feb 13 16:24 __init__.py\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = 'AI_Platform/trainig_package'\n",
    "PACKAGE_FOLDER = '/trainer'\n",
    "\n",
    "!rm -r {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH}\n",
    "!mkdir {ROOT_PATH+PACKAGE_FOLDER}\n",
    "!touch {ROOT_PATH+PACKAGE_FOLDER}/__init__.py\n",
    "!ls -l {ROOT_PATH+PACKAGE_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setuptools file named `setup.py`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AI_Platform/trainig_package/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ROOT_PATH}/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['keras==2.2.4']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.2',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='My training application package.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation data**\n",
    "\n",
    "The following is code to load training/evaluation data.  Write this into `util.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AI_Platform/trainig_package/trainer/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/util.py\n",
    "\"\"\"Utilities to download and preprocess the data.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "\n",
    "class Util():\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "        self.client = storage.Client(project='skydipper-196010')\n",
    "        self.bucket = self.client.get_bucket('geo-ai')\n",
    "        self.blob = self.bucket.blob(self.path)\n",
    "        self.config = json.loads(self.blob.download_as_string(client=self.client).decode('utf-8'))\n",
    "        \n",
    "    def parse_function(self, proto):\n",
    "        \"\"\"The parsing function.\n",
    "        Read a serialized example into the structure defined by features_dict.\n",
    "        Args:\n",
    "          example_proto: a serialized Example.\n",
    "        Returns: \n",
    "          A dictionary of tensors, keyed by feature name.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define your tfrecord \n",
    "        features = self.config.get('in_bands') + self.config.get('out_bands')\n",
    "        \n",
    "        # Specify the size and shape of patches expected by the model.\n",
    "        kernel_shape = [self.config.get('kernel_size'), self.config.get('kernel_size')]\n",
    "        columns = [\n",
    "          tf.io.FixedLenFeature(shape=kernel_shape, dtype=tf.float32) for k in features\n",
    "        ]\n",
    "        features_dict = dict(zip(features, columns))\n",
    "        \n",
    "        # Load one example\n",
    "        parsed_features = tf.io.parse_single_example(proto, features_dict)\n",
    "    \n",
    "        # Convert a dictionary of tensors to a tuple of (inputs, outputs)\n",
    "        inputs_list = [parsed_features.get(key) for key in features]\n",
    "        stacked = tf.stack(inputs_list, axis=0)\n",
    "        \n",
    "        # Convert the tensors into a stack in HWC shape\n",
    "        stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "        \n",
    "        return stacked[:,:,:len(self.config.get('in_bands'))], stacked[:,:,len(self.config.get('in_bands')):]\n",
    "    \n",
    "    def get_dataset(self, glob):\n",
    "        \"\"\"Get the preprocessed training dataset\n",
    "        Returns: \n",
    "        A tf.data.Dataset of training data.\n",
    "        \"\"\"\n",
    "        glob = tf.compat.v1.io.gfile.glob(glob)\n",
    "        \n",
    "        dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "        dataset = dataset.map(self.parse_function, num_parallel_calls=5)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    def get_training_dataset(self):\n",
    "        \"\"\"Get the preprocessed training dataset\n",
    "        Returns: \n",
    "        A tf.data.Dataset of training data.\n",
    "        \"\"\"\n",
    "        glob = self.config.get('data_dir') + '/' + self.config.get('base_names')[0] + '*'\n",
    "        dataset = self.get_dataset(glob)\n",
    "        dataset = dataset.shuffle(self.config.get('shuffle_size')).batch(self.config.get('batch_size')).repeat()\n",
    "        return dataset\n",
    "    \n",
    "    def get_validation_dataset(self):\n",
    "        \"\"\"Get the preprocessed validation dataset\n",
    "        Returns: \n",
    "          A tf.data.Dataset of validation data.\n",
    "        \"\"\"\n",
    "        glob = self.config.get('data_dir') + '/' + self.config.get('base_names')[1] + '*'\n",
    "        dataset = self.get_dataset(glob)\n",
    "        dataset = dataset.batch(1).repeat()\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {ROOT_PATH+PACKAGE_FOLDER+'/models'}\n",
    "!touch {ROOT_PATH+PACKAGE_FOLDER+'/models'}/__init__.py\n",
    "!cp -r ../models/CNN {ROOT_PATH+PACKAGE_FOLDER+'/models'}/CNN\n",
    "!cp -r ../models/MLP {ROOT_PATH+PACKAGE_FOLDER+'/models'}/MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AI_Platform/trainig_package/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/model.py\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "import json\n",
    "\n",
    "from .models.CNN.regression import deepvel as CNNregDeepVel, segnet as  CNNregSegNet, unet as CNNregUNet\n",
    "from .models.CNN.segmentation import deepvel as CNNsegDeepVel, segnet as  CNNsegSegNet, unet as CNNsegUNet\n",
    "from .models.MLP.regression import sequential1 as MLPregSequential1\n",
    "\n",
    "def select_model(path):\n",
    "    # Read training parameters from GCS\n",
    "    client = storage.Client(project='skydipper-196010')\n",
    "    bucket = client.get_bucket('geo-ai')\n",
    "    blob = bucket.blob(path)\n",
    "    config = json.loads(blob.download_as_string(client=client).decode('utf-8'))\n",
    "    \n",
    "    # Model's dictionary\n",
    "    models = {'CNN':\n",
    "              {\n",
    "                  'regression': \n",
    "                  {\n",
    "                      'deepvel': CNNregDeepVel.create_keras_model,\n",
    "                      'segnet': CNNregSegNet.create_keras_model,\n",
    "                      'unet': CNNregUNet.create_keras_model,\n",
    "                  },\n",
    "                  'segmentation': \n",
    "                  {\n",
    "                      'deepvel': CNNsegDeepVel.create_keras_model,\n",
    "                      'segnet': CNNsegSegNet.create_keras_model,\n",
    "                      'unet': CNNsegUNet.create_keras_model,\n",
    "                  }\n",
    "              }, \n",
    "              'MLP': \n",
    "              {\n",
    "                  'regression': \n",
    "                  {\n",
    "                      'sequential1': MLPregSequential1.create_keras_model,\n",
    "                  }\n",
    "              }\n",
    "             }\n",
    "    \n",
    "    return models.get(config.get('model_type')).get(config.get('model_output')).get(config.get('model_architecture'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training task**\n",
    "\n",
    "The following will create `task.py`, which will get the training and evaluation data, train the model and save it when it's done in a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing AI_Platform/trainig_package/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {ROOT_PATH+PACKAGE_FOLDER}/task.py\n",
    "\"\"\"Trains a Keras model\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "\n",
    "from .util import Util\n",
    "from . import model\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--params-file',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='GCS location where we have saved the training_params.json file')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='INFO')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def train_and_validation(args):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read training parameters from GCS\n",
    "    client = storage.Client(project='skydipper-196010')\n",
    "    bucket = client.get_bucket('geo-ai')\n",
    "    blob = bucket.blob(args.params_file)\n",
    "    config = json.loads(blob.download_as_string(client=client).decode('utf-8'))\n",
    "\n",
    "    # Create the Keras Model\n",
    "    selected_model = model.select_model(args.params_file)\n",
    "\n",
    "    if not config.get('output_activation'):\n",
    "        keras_model = selected_model(inputShape = (None, None, len(config.get('in_bands'))), nClasses = len(config.get('out_bands')))\n",
    "    else:\n",
    "        keras_model = selected_model(inputShape = (None, None, len(config.get('in_bands'))), nClasses = len(config.get('out_bands')), output_activation = config.get('output_activation'))\n",
    "\n",
    "    # Compile Keras model\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=config.get('learning_rate'))\n",
    "    keras_model.compile(loss=config.get('loss'), optimizer=optimizer, metrics=config.get('metrics'))\n",
    "\n",
    "\n",
    "    # Pass a tfrecord\n",
    "    util = Util(path = args.params_file) \n",
    "    training_dataset = util.get_training_dataset()\n",
    "    validation_dataset = util.get_validation_dataset()\n",
    "\n",
    "    # Setup TensorBoard callback.\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(os.path.join(config.get('job_dir'), 'logs'))\n",
    "\n",
    "    # Train model\n",
    "    keras_model.fit(\n",
    "        x=training_dataset,\n",
    "        steps_per_epoch=int(config.get('training_size') / config.get('batch_size')),\n",
    "        epochs=config.get('epochs'),\n",
    "        validation_data=validation_dataset,\n",
    "        validation_steps=int(config.get('validation_size') / config.get('batch_size')),\n",
    "        verbose=1,\n",
    "        callbacks=[tensorboard_cb])\n",
    "\n",
    "    tf.contrib.saved_model.save_keras_model(keras_model, os.path.join(config.get('job_dir'), 'model'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    tf.logging.set_verbosity('INFO')\n",
    "    train_and_validation(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a .tar.gz distribution package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = './AI_Platform/trainer-0.2.tar.gz'\n",
    "source_dir = './AI_Platform/trainig_package/'\n",
    "\n",
    "make_tarfile(output_filename, source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=/Users/ikersanchez/Vizzuality/Keys/Skydipper/skydipper-196010-a4ce18e66917.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS {env.privatekey_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://storage.googleapis.com/geo-ai/Train/trainer-0.2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "client = storage.Client().from_service_account_json(env.privatekey_path)\n",
    "bucket = client.get_bucket('geo-ai')\n",
    "blob = bucket.blob('Train/trainer-0.2.tar.gz')\n",
    "                     \n",
    "blob.upload_from_filename(\n",
    "    filename = output_filename, \n",
    "    content_type = 'text/plain',\n",
    "    client=client\n",
    ")\n",
    "print(blob.public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
