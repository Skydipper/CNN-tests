{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2 cropland mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the paper by [Belgiu & Csillik (2018)] (see also [Hao et al. 2018](https://peerj.com/articles/5431/?utm_source=TrendMD&utm_campaign=PeerJ_TrendMD_0&utm_medium=TrendMD))(https://www.sciencedirect.com/science/article/pii/S0034425717304686) we can train a CNN for the segmentation of the croplands. As an input we can use [Sentinel-2 MSI](https://sentinel.esa.int/web/sentinel/missions/sentinel-2) multispectral data, and as an output crop types data classified by experts from the European Land Use and Coverage Area Frame Survey ([LUCAS](https://ec.europa.eu/eurostat/statistics-explained/index.php/LUCAS_-_Land_use_and_land_cover_survey)) and  CropScape â€“ Cropland Data Layer ([CDL](https://nassgeodata.gmu.edu/CropScape/)), respectively.\n",
    "\n",
    "Datasets in Google Earth Engine:\n",
    "\n",
    "- [Sentinel-2 MSI: MultiSpectral Instrument, Level-1C](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2)\n",
    "- [USDA NASS Cropland Data Layers](https://developers.google.com/earth-engine/datasets/catalog/USDA_NASS_CDL)\n",
    "- [Canada AAFC Annual Crop Inventory](https://developers.google.com/earth-engine/datasets/catalog/AAFC_ACI) \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Environment\n",
    "\n",
    "We begin by importing a number of useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from functools import reduce\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Earth Engine client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earth Engine data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central position of (AOIs)\n",
    "points = [[-120.7224, 37.3872], [-112.6799, 42.9816], [-89.7649, 35.8764], \n",
    "          [-96.0181, 41.2412], [-115.473, 46.861], [-103.9803, 47.9713], \n",
    "          [-96.9217, 32.8958], [-82.986, 40.019], [-90.347, 38.668], \n",
    "          [-110.6947, 37.4568], [-101.8889, 33.5527], [-92.621, 33.417],\n",
    "          [-80.352, 38.628], [-104.752, 43.972], [-110.92, 37.18]]\n",
    "\n",
    "# Start and stop of time series\n",
    "startDate = ee.Date('2016-01-01')\n",
    "stopDate  = ee.Date('2016-12-31')\n",
    "# Scale in meters\n",
    "scale = 10\n",
    "# Buffer\n",
    "buffer = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.ee_dataset_acquisition import ee_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export `.tif` files to Google Cloud Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel = ee_dataset(points = points, buffer = buffer, startDate = startDate, stopDate = stopDate, scale = scale, file_name='S2_AOI', dataset_name = 'data_x', chunk_size = (128,128), collection = 'Sentinel2')\n",
    "#sentinel.export_toCloudStorage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropland = ee_dataset(points = points, buffer = buffer, startDate = startDate, stopDate = stopDate, scale = scale, file_name='cropland_AOI', dataset_name = 'data_y', chunk_size = (128,128), collection = 'CroplandDataLayers')\n",
    "#cropland.export_toCloudStorage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read `.tif` files from Google Cloud Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel.read_fromCloudStorage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropland.read_fromCloudStorage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resize the datasets into small chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel.resize_inChunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropland.resize_inChunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge different AOIs into a single dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel.merge_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropland.merge_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from preprocess.preprocess_datasets import preprocess_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get normalization values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = preprocess_datasets(dataset_names=['data_x','data_y'], collections=['Sentinel2', 'CroplandDataLayers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.normalization_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change class labels**\n",
    "\n",
    "Each class is encoded as a value in the range between 0 to 254. For training a Neural Network in Keras we have to convert the 1-dimensional class arrays to N classes-dimensional matrices. To simplify the problem here we regroup all the classes into 4 categories, namely, land, water, urban, and cropland areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.change_class_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomize the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.randomize_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.train_validation_split(val_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train SegNet from the command line type:\n",
    "\n",
    "```python\n",
    "python train.py -i samples/ -o networks/ -c 4 -e 20 -m segnet -a start\n",
    "```\n",
    "\n",
    "The parameters are:\n",
    "\n",
    "```\n",
    "-i = samples/\n",
    "    Path of the input files.\n",
    "-o = networks/\n",
    "    Path of the output files that will contain the network weights.\n",
    "-e = 20\n",
    "    Number of epochs to use during training.\n",
    "-m = segnet\n",
    "    Model name.\n",
    "-a = {start,continue}\n",
    "     `start`: start a new calculation\n",
    "     `continue`: continue a previous calculation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central position of (AOIs)\n",
    "point = [-112.0087, 43.4952]\n",
    "# Start and stop of time series\n",
    "startDate = ee.Date('2016-01-01')\n",
    "stopDate  = ee.Date('2016-12-31')\n",
    "# Scale in meters\n",
    "scale = 10\n",
    "# Buffer\n",
    "buffer = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ee_datasets import ee_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel = ee_datasets(point = point, buffer = buffer, startDate = startDate, stopDate = stopDate, scale = scale, collection = 'Sentinel2')\n",
    "cropland = ee_datasets(point = point, buffer = buffer, startDate = startDate, stopDate = stopDate, scale = scale, collection = 'CroplandDataLayers')\n",
    "data_x = sentinel.read_datasets()\n",
    "data_y = cropland.read_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess class labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_new = np.zeros((1, 2005, 2745, 1), dtype=np.float32)\n",
    "data_y_new[0,:,:,:] = data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_data import  categorical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = categorical_data(data_y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_data import normalize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_new = np.zeros((1, 2005, 2745, 6), dtype=np.float32)\n",
    "data_x_new[0,:,:,:] = data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = normalize_data(data_x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples\n",
    "write_data(\"../SegNet/Samples/data_x.h5\", 'data_x', data_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SegNet from the command line type:\n",
    "\n",
    "```python\n",
    "python predict.py -i Samples/ -o output/output.hdf5 -c 4 -m segnet\n",
    "```\n",
    "\n",
    "The parameters are:\n",
    "\n",
    "```\n",
    "-i = Samples/file \n",
    "    Path of the input file.\n",
    "-o = Predictions/file\n",
    "    Path of the output file.\n",
    "-c = 4\n",
    "    Number of classes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"/Users/ikersanchez/Vizzuality/GitHub/Skydipper/cnn-models/SegNet/Samples/data_x.h5\", 'r')\n",
    "data_x = f.get(\"data_x\")[:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"../SegNet/Predictions/output.h5\", 'r')\n",
    "output = f.get(\"out\")[:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"../SegNet/Predictions/output_0.h5\", 'r')\n",
    "output = f.get(\"out\")[:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_channels(data, nChannels, titles = False):\n",
    "    if nChannels == 1:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(data[:,:,0])\n",
    "        if titles:\n",
    "            plt.title(titles[0])\n",
    "    else:\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=nChannels, figsize=(5*nChannels,5))\n",
    "        for i in range(nChannels):\n",
    "            ax = axs[i]\n",
    "            ax.imshow(data[:,:,i])\n",
    "            if titles:\n",
    "                ax.set_title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel 2 composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_x[0,:,:,:], data_x.shape[3], titles=['Blue', 'Green', 'Red', 'NIR', 'NDVI', 'NDWI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth land cover classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(data_y[0,:,:,:], data_y.shape[3], titles=['Land', 'Water', 'Urban', 'Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(output[0,:,:,:], output.shape[3], titles=['Land', 'Water', 'Urbar', 'Cropland'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_channels(output_max, output_max.shape[2], titles=['Land', 'Water', 'Urbar', 'Cropland'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We binarize the output taking the highest pixel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_data import max_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "def max_pixels(x):\n",
    "    x_new = x*0\n",
    "    max_val = np.amax(x, axis=2)\n",
    "    size = x.shape\n",
    "    for i in range(size[-1]):\n",
    "        ima = x[:,:,i]*0\n",
    "        ima[np.where(x[:,:,i] == max_val)] = 1\n",
    "        x_new[:,:,i]= ima\n",
    "\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = max_pixels(output[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From GEE to Numpy\n",
    "https://mygeoblog.com/2017/10/06/from-gee-to-numpy-to-geotiff/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ee_np_array import ee_np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central position of (AOIs)\n",
    "point = [-122.938, 46.151]\n",
    "# Start and stop of time series\n",
    "startDate = ee.Date('2016-01-01')\n",
    "stopDate  = ee.Date('2016-12-31')\n",
    "# Scale in meters\n",
    "scale = 10\n",
    "# Buffer\n",
    "buffer = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel = ee_np_array(point = point, buffer = buffer, startDate = startDate, stopDate = stopDate, scale = scale, collection = 'Sentinel2')\n",
    "cropland = ee_np_array(point = point, buffer = buffer, startDate = startDate, stopDate = stopDate, scale = scale, collection = 'CroplandDataLayers')\n",
    "dataset_x = sentinel.read_datasets()\n",
    "dataset_y = cropland.read_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download datasets\n",
    "We download and append datasets from different Areas of Interest (AOIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ee_datasets import ee_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel = ee_datasets(point = point, buffer = buffer, startDate = startDate, stopDate = stopDate, scale = scale, collection = 'Sentinel2')\n",
    "dataset_x = sentinel.read_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central position of (AOIs)\n",
    "#points = [[-120.7224, 37.3872], [-112.6799, 42.9816], [-89.7649, 35.8764], \n",
    "#          [-96.0181, 41.2412], [-115.473, 46.861], [-103.9803, 47.9713], [-96.9217, 32.8958]]\n",
    "\n",
    "# lat < -115\n",
    "points = [[-123.488, 47.712], [-122.191, 47.608], [-122.96, 46.802], [-121.752, 46.847], [-121.95, 46.531], \n",
    "          [-122.938, 46.151], [-122.015, 45.892], [-122.653, 45.539], [-122.894, 45.231], [-123.136, 44.452],\n",
    "         [-123.312, 43.279], [-122.872, 42.361], [-121.95, 42.442], [-122.411, 41.577], [-121.796, 41.28],\n",
    "         [-122.169, 40.131], [-121.928, 39.354], [-121.686, 38.876], [-121.4, 38.619], [-121.356, 38.137],\n",
    "         [-120.983, 37.651], [-120.543, 37.022], [-119.269, 37.163], [-118.676, 37.093],[-119.686, 36.06],\n",
    "         [-117.973, 34], [-120.09, 47.862], [-118.991, 47.729], [-119.782, 47.06], [-117.277, 47.313], \n",
    "         [-117.299, 46.789], [-117.607, 46.533], [-115.937, 47.179], [-119.606, 45.91], [-119.804, 45.572],\n",
    "         [-119.189, 45.403], [-118.442, 44.876], [-117.277, 45.14], [-116.293, 43.609], [-117.019, 43.974],\n",
    "         [-118.93, 43.529], [-118.754, 43.306], [-117.941, 42.404], [-120.424, 41.948], [-118.776, 41.125],\n",
    "         [-118.557, 40.041], [-119.128, 38.973], [-118.71, 38.682], [-117.178, 37.294], [-116.299, 33.736],\n",
    "         [-115.859, 33.223], [-115.508, 32.983], [-115.134, 36.149], [-116.255, 48.159], [-121.914, 37.286]]\n",
    "\n",
    "# Start and stop of time series\n",
    "startDate = ee.Date('2016-01-01')\n",
    "stopDate  = ee.Date('2016-12-31')\n",
    "# Scale in meters\n",
    "scale = 10\n",
    "# Buffer\n",
    "buffer = 11000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, point in enumerate(points):\n",
    "    print('Downloading AOI number:', n)\n",
    "    sentinel = ee_datasets(point = point, buffer = buffer, startDate = startDate, stopDate = stopDate, scale = scale, collection = 'Sentinel2')\n",
    "    cropland = ee_datasets(point = point, buffer = buffer, startDate = startDate, stopDate = stopDate, scale = scale, collection = 'CroplandDataLayers')\n",
    "    dataset_x = sentinel.read_datasets()\n",
    "    dataset_y = cropland.read_datasets()\n",
    "    \n",
    "    if n == 0:\n",
    "        dim_x = dataset_x.shape\n",
    "        dim_x = list(dim_x)\n",
    "        dim_x = [1] + dim_x\n",
    "        dim_x[1] = 2200; dim_x[2] = 2200\n",
    "        \n",
    "        dim_y = dataset_y.shape\n",
    "        dim_y = list(dim_y)\n",
    "        dim_y = [1] + dim_y\n",
    "        dim_y[1] = 2200; dim_y[2] = 2200\n",
    "        \n",
    "        data_x = np.zeros(dim_x, dtype=np.float32)\n",
    "        data_y = np.zeros(dim_y, dtype=np.float32)\n",
    "        \n",
    "        data_x[0,:] = dataset_x[:dim_x[1],:dim_x[2],:dim_x[3]]\n",
    "        data_y[0,:] = dataset_y[:dim_y[1],:dim_y[2],:dim_y[3]]\n",
    "    else:\n",
    "        data_x2 = np.zeros(dim_x, dtype=np.float32)\n",
    "        data_y2 = np.zeros(dim_y, dtype=np.float32)\n",
    "        \n",
    "        data_x2[0,:] = dataset_x[:dim_x[1],:dim_x[2],:dim_x[3]]\n",
    "        data_y2[0,:] = dataset_y[:dim_y[1],:dim_y[2],:dim_y[3]]\n",
    "        \n",
    "        data_x = np.append(data_x, data_x2, axis=0)\n",
    "        data_y = np.append(data_y, data_y2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download `.tif` files to a local directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def download_fromCloudStorage(project_name, bucket_name, file_path_cloud, file_path_local):\n",
    "    # Initialise a client\n",
    "    storage_client = storage.Client(project_name)\n",
    "    # Create a bucket object for our bucket\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    # Create a blob object from the filepath\n",
    "    blob = bucket.blob(file_path_cloud)\n",
    "    # Download the file to a destination\n",
    "    blob.download_to_filename(file_path_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'skydipper'\n",
    "bucket_name = \"skydipper_materials\"\n",
    "path_cloud = \"gee_data/\"\n",
    "path_local = \"./Data/\"\n",
    "for n, point in enumerate(points):\n",
    "    print('Downloading AOI number:', n)\n",
    "    file_name = 'sentinel_AOI_'+'{:02d}'.format(n)+'.tif'\n",
    "    download_fromCloudStorage(project_name, bucket_name, file_path_cloud = path_cloud + file_name, file_path_local = path_local + file_name)\n",
    "    \n",
    "    file_name = 'cropland_AOI_'+'{:02d}'.format(n)+'.tif'\n",
    "    download_fromCloudStorage(project_name, bucket_name, file_path_cloud = path_cloud + file_name, file_path_local = path_local + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "bucket_name = \"skydipper_materials\"\n",
    "path_cloud = \"gee_data/\"\n",
    "path_local = \"./Data/test/\"\n",
    "file_name = 'sentinel_AOI_00.tif'\n",
    "\n",
    "subprocess.call(f\"gsutil -m cp gs://{bucket_name}/{path_cloud}{file_name} {path_local}{file_name}\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a client\n",
    "storage_client = storage.Client()\n",
    "# Create a bucket object for our bucket\n",
    "bucket = storage_client.get_bucket(\"skydipper_materials\")\n",
    "# Create a blob object from the filepath\n",
    "blob = bucket.blob(\"gee_data/sentinel_AOI_00.tif\")\n",
    "# Download the file to a destination\n",
    "blob.download_to_filename('./Data/test/sentinel_AOI_00.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read datasets from `.tif`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os, urllib\n",
    "\n",
    "def load_tif_bands(file_path):\n",
    "    data = np.array([]) \n",
    "\n",
    "    image = rasterio.open(file_path)\n",
    "    nBands = image.count\n",
    "    szy = image.height\n",
    "    szx = image.width\n",
    "    \n",
    "    dataset = np.zeros((szy,szx,0), dtype=np.float32)\n",
    "    data = np.zeros((szy,szx,0), dtype=np.float32)\n",
    "    \n",
    "    for n in range(nBands):\n",
    "        data = np.zeros((szy,szx,1), dtype=np.float32)\n",
    "        data[:,:,0] = image.read(1+n)\n",
    "        \n",
    "        dataset = np.append(dataset, data, axis=2)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_local = \"./Data/\"\n",
    "nAOI = len(points)\n",
    "for n, point in enumerate(points):\n",
    "    print('Reading AOI number:', n)\n",
    "    \n",
    "    file_name = 'sentinel_AOI_'+'{:02d}'.format(n)+'.tif'\n",
    "    dataset_x = load_tif_bands(path_local+file_name)\n",
    "    \n",
    "    file_name = 'cropland_AOI_'+'{:02d}'.format(n)+'.tif'\n",
    "    dataset_y = load_tif_bands(path_local+file_name)\n",
    "    \n",
    "    if n == 0:\n",
    "        dim_x = dataset_x.shape\n",
    "        dim_x = list(dim_x)\n",
    "        dim_x = [1] + dim_x\n",
    "        dim_x[1] = dim_x[1]-4\n",
    "        dim_x[2] = dim_x[1]\n",
    "        \n",
    "        dim_y = dataset_y.shape\n",
    "        dim_y = list(dim_y)\n",
    "        dim_y = [1] + dim_y\n",
    "        dim_y[1] = dim_y[1]-4\n",
    "        dim_y[2] = dim_y[1]\n",
    "        \n",
    "        with f = h5py.File('data_x.hdf5', 'w') as f:\n",
    "            data_x = f.create_dataset(\"data_x\", dim_x, chunks=True, dtype=np.float32)\n",
    "        \n",
    "        with f = h5py.File('data_y.hdf5', 'w') as f:\n",
    "            data_y = f.create_dataset(\"data_y\", dim_y, chunks=True, dtype=np.float32)\n",
    "        \n",
    "        data_x[0,:] = dataset_x[:dim_x[1],:dim_x[2],:dim_x[3]]\n",
    "        data_y[0,:] = dataset_y[:dim_y[1],:dim_y[2],:dim_y[3]]\n",
    "    else:\n",
    "        with f = h5py.File('data_x2.hdf5', 'w') as f:\n",
    "            data_x2 = f.create_dataset(\"data_x2\", dim_y, chunks=True, dtype=np.float32)\n",
    "        \n",
    "        with f = h5py.File('data_y2.hdf5', 'w') as f:\n",
    "            data_y2 = f.create_dataset(\"data_y2\", dim_y, chunks=True, dtype=np.float32)\n",
    "        \n",
    "        data_x2[0,:] = dataset_x[:dim_x[1],:dim_x[2],:dim_x[3]]\n",
    "        data_y2[0,:] = dataset_y[:dim_y[1],:dim_y[2],:dim_y[3]]\n",
    "        \n",
    "        \n",
    "        data_x = np.append(data_x, data_x2, axis=0)\n",
    "        data_y = np.append(data_y, data_y2, axis=0)\n",
    "        \n",
    "        os.remove('data_x2.hdf5') \n",
    "        os.remove('data_y2.hdf5') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
